{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQXckIA1Z21a"
   },
   "source": [
    "# Elastic weight consolidation- concept drift\n",
    "\n",
    "In the past notebook *Triplet learning for bearing fault classification* we trained a classification model for bearing faults of rotating equipment. Such model was robust to different operating conditions such as different machine loads or rotation velocity, and did not use characteristic information about the machine, such as the rotation velocity or the characteristic coefficients. We used an approach based on triplet learning.\n",
    "\n",
    "In this notebook, we want to illustrate how the retraining process would be done, when new data (from the same machine) arribes. Such new data may be slightly different from the data previously seen by the model, either because of concept drift or simply because we are seeing a common behaviour which had not been seen before. Therefore, we want the model to learn useful information of this new data without forgetting about the previously acquired knowledge.\n",
    "\n",
    "In this notebook we will simulate concept drift, and see how the model adapts to this new situation. In the notebook *Elastic weight consolidation - new behaviour* we simulate the other situation, when the data changes only slightly. In order to simulate concept drift, we will use two different datasets: \n",
    "\n",
    "+ **Paderborn Data Center**: This dataset also contains vibration data (waveforms) of different bearings, each suffering from a fault (inner ring, outer ring) or being healthy. The fault is completely developed.This dataset will be considered as the *old* dataset.\n",
    "\n",
    "+ **IMS Nasa Bearing Data Center:** This dataset contains the time evolution of different bearings. At the end of the tests, some of these bearings suffer from faults. The goal of using this dataset is to be able to detect failures at early stages, that is, before they have been fully developed. For this reason, the waveform time series will be labeled using the Autoencoder Model, so that waveforms with early development of the failure will be labeled as being unhealthy. We will use this data as the *new* data, when the concept drift has already happened. \n",
    "\n",
    "The inputs of the models will be the same as in the notebook *Triplet learning for bearing fault classification*.\n",
    "\n",
    "More details about the training process are provided in the following sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMvDLLmTgaDf"
   },
   "source": [
    "The following lines of code are used to mount to drive account and import the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9bSImtZ6oENC"
   },
   "outputs": [],
   "source": [
    "#Import needed packages\n",
    "\n",
    "import scipy.io\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import pandas as pd\n",
    "import umap\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "sys.path.append('./Training')\n",
    "sys.path.append('./Data')\n",
    "sys.path.append('./Models')\n",
    "sys.path.append('./Load_Data')\n",
    "\n",
    "from IMS_data import Import_Data_IMS, Preprocessing_IMS\n",
    "from Paderborn_data import Import_Data_Pad, Preprocessing_Pad, Load_Pad\n",
    "from CNN_model import Embedding, Classification\n",
    "from train_classifier import Training_classifier\n",
    "from train_embeddings import Train_Embeddings\n",
    "from pretraining import Pretraining\n",
    "from EWC import Train_Classifier_EWC, Train_Embeddings_EWC,Fisher_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6253,
     "status": "ok",
     "timestamp": 1586446474091,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "Up41WXICo-6W",
    "outputId": "79e9dcb7-4780-4a84-98a6-28c46596cf6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAO3BBOowwTh"
   },
   "source": [
    "## Load Paderborn data\n",
    "\n",
    "Paderborn data contains time series of the vibration of rotating equipment (bearings). It contains two different rotation speeds (900 rpm and 1500 rpm). The dataset contains data from healthy bearings, and from two different types of faults: inner ring fault and outer ring fault. The faults are completely developed at the beginning of the time series.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7lY46PFZytQ_"
   },
   "outputs": [],
   "source": [
    "data_Pad = Import_Data_Pad('Data/Paderborn/K001/N15/', 'Data/Paderborn/K001/N09/', 'Data/Paderborn/KI01/N15/', \n",
    "                       'Data/Paderborn/KI01/N09/', 'Data/Paderborn/KA01/N15/', 'Data/Paderborn/KA01/N09/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nLEXpnk1un2e"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "In order to preprocess data we performed the following steps:\n",
    "\n",
    "+ Downsampling from 20000 Hz to 5000 Hz in order to have data more similiar to the real productio data.\n",
    "\n",
    "+ Rolling window for each waveform, to reduce the size of the wavevorm to 1000 timestamps (instead of 5000) and to obtain more samples from the same waveform. There is no overlap.\n",
    "\n",
    "+ Adaptative normalization: minmax scaling.\n",
    "\n",
    "+ Adding noise \n",
    "\n",
    "+ Shuffle samples.\n",
    "\n",
    "+ Split of training and test sets.\n",
    "\n",
    "The noise is introduced in two different ways:\n",
    "\n",
    "+ Amplify the amplitude of the 4rth first harmonics.\n",
    "\n",
    "+ Introduce white noise (random frequencies and amplitudes) within a frequency band of 750-1000Hz.\n",
    "\n",
    "To visualize such noise, refer to the notebook *Elastic weight consolidation - new behaviour*.\n",
    "We add noise so that the data resembles more to real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ooc2P8PrKzmH"
   },
   "source": [
    "### Preprocess Paderborn University Data\n",
    "\n",
    "Given the file names of the data, the following class imports and processes the data in order to make it suitable for the CNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13951,
     "status": "ok",
     "timestamp": 1586446481822,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "HuqxCPLwQ16A",
    "outputId": "76f94b20-a26d-497f-e97d-b34ad7276ffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_1.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./Load_Data\\Paderborn_data.py:253: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  data_train = pd.concat([data_train, aux])\n",
      "./Load_Data\\Paderborn_data.py:255: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  data_test = pd.concat([data_test, aux])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_10.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_11.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_12.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_13.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_14.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_15.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_16.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_17.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_18.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_19.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_2.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_20.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_3.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_4.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_5.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_6.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_7.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_8.mat\n",
      "Data/Paderborn/K001/N09/N09_M07_F10_K001_9.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_1.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_10.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_11.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_12.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_13.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_14.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_15.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_16.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_17.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_18.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_19.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_2.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_20.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_3.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_4.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_5.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_6.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_7.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_8.mat\n",
      "Data/Paderborn/K001/N15/N15_M07_F10_K001_9.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_1.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_10.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_11.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_12.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_13.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_14.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_15.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_16.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_17.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_18.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_19.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_2.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_20.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_3.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_4.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_5.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_6.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_7.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_8.mat\n",
      "Data/Paderborn/KI01/N09/N09_M07_F10_KI01_9.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_1.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_10.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_11.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_12.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_13.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_14.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_15.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_16.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_17.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_18.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_19.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_2.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_20.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_3.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_4.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_5.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_6.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_7.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_8.mat\n",
      "Data/Paderborn/KI01/N15/N15_M07_F10_KI01_9.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_1.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_10.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_11.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_12.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_13.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_14.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_15.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_16.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_17.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_18.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_19.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_2.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_20.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_3.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_4.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_5.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_6.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_7.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_8.mat\n",
      "Data/Paderborn/KA01/N09/N09_M07_F10_KA01_9.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_1.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_10.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_11.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_12.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_13.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_14.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_15.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_16.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_17.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_18.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_19.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_2.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_20.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_3.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_4.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_5.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_6.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_7.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_8.mat\n",
      "Data/Paderborn/KA01/N15/N15_M07_F10_KA01_9.mat\n"
     ]
    }
   ],
   "source": [
    "#Create preprocessing object\n",
    "prep = Preprocessing_Pad()\n",
    "\n",
    "#Load Paderborn data\n",
    "load_Pad = Load_Pad(prep, data_Pad.healthy_files09, data_Pad.healthy_files15,data_Pad.inner_files09,\n",
    "                     data_Pad.inner_files15,data_Pad.outer_files09, data_Pad.outer_files15)\n",
    "\n",
    "X_train_Pad, X_test_Pad, y_train_Pad, y_test_Pad, load_train_Pad, load_test_Pad = load_Pad.get_X_y()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmY-JhnlYld2"
   },
   "source": [
    "## Import IMS Data\n",
    "\n",
    "We import the two files from the IMS Nasa Bearing Data Center. \n",
    "The datasets consist of two tests (Test1 and Test2), which contain the waveform time evolution of four bearings. Vibration data is collected with accelerometers. The bearings are of type Rexnord ZA-2115â€‹.\n",
    "\n",
    "The final state of each of the bearings is the following:\n",
    "\n",
    "**Test1:**\n",
    "+ Bearing 1 has an outer race defect which starts at around timestep 800\n",
    "+ Bearing 2 is healthy\n",
    "+ Bearing 3 is healthy\n",
    "+ Bearing 4 has a rolling defect which starts at around timestamp 1800 \n",
    "\n",
    "**Test2:**\n",
    "+ Bearing 1 is healthy \n",
    "+ Bearing 2 is healthy\n",
    "+ Bearing 3 has an inner race defect which starts at around timestep 1800\n",
    "+ Bearing 4 is healthy\n",
    "\n",
    "\n",
    "In the following class we will import the data from csv files and split it into four dataframes, one for each bearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B10UCI2QYoex"
   },
   "outputs": [],
   "source": [
    "test1 = Import_Data_IMS('Data/IMS/Test_1.csv')\n",
    "test2 = Import_Data_IMS('Data/IMS/Test_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GU-7OM_VUJ-p"
   },
   "source": [
    "The following lines of code put split the data into being healthy or having a fault (inner ring fault, outer ring fault and rolling element fault). In order to do wo, we use the labels obtained from the Autoencoder model, which tell us approximately in which timestamp the errors start their development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TWmu5lJ_UJSZ"
   },
   "outputs": [],
   "source": [
    "#Define unhealthy data using the labels from the autoencoder model\n",
    "inner = test1.t_2[1800:,:]\n",
    "outer = test2.t_0[800:,:]\n",
    "rolling = test1.t_3[1800:,:]\n",
    "\n",
    "#For the healthy data, use waveforms from all tests\n",
    "#We use a balanced dataset for healthy/unealthy data\n",
    "idx = np.random.choice(np.arange(1300),70)\n",
    "healthy1 = test1.t_2[idx,:]\n",
    "idx = np.random.choice(np.arange(1300),70)\n",
    "healthy2 = test1.t_3[idx,:]\n",
    "idx = np.random.choice(np.arange(300),70)\n",
    "healthy3 = test2.t_0[idx,:]\n",
    "\n",
    "#Finally, concatenate all the healthy data\n",
    "healthy  = np.concatenate([healthy1,healthy2,healthy3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9-vXWMEUOfZ"
   },
   "source": [
    "## Preprocessing IMS Data\n",
    "\n",
    "In order to preprocess data we performed the following steps:\n",
    "\n",
    "+ Downsampling from 20000 Hz to 5000 Hz in order to have data more similiar to the real productio data.\n",
    "\n",
    "+ Rolling window for each waveform, to reduce the size of the wavevorm to 1000 timestamps (instead of 5000) and to obtain more samples from the same waveform. There is no overlap.\n",
    "\n",
    "+ Adaptative normalization: minmax scaling.\n",
    "\n",
    "+ Shuffle samples.\n",
    "\n",
    "+ Split of training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rueot41bYsQ3"
   },
   "outputs": [],
   "source": [
    "prep_IMS = Preprocessing_IMS(inner, outer, rolling, healthy)\n",
    "\n",
    "X_train_IMS, X_test_IMS, y_train_IMS, y_test_IMS = prep_IMS.get_X_y()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code to load the embedding and classification models from the path where the weights are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filepath, learning_rate = 0.001, net = 'classification'):\n",
    "    \"\"\"\n",
    "    Loads the embedding and classification models.\n",
    "    Args:\n",
    "        filepath (str): Path of the saved weights\n",
    "        learning_rate (float): Learning rate of the Adam optimizer\n",
    "    Returns:\n",
    "        embedding (Model): Embedding model\n",
    "        classification (Model): Classification model\n",
    "    \"\"\"\n",
    "    #Redefine model\n",
    "    embedding = Embedding()\n",
    "    classification = Classification(embedding)\n",
    "\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)\n",
    "    if net=='classification':\n",
    "        ckpt = tf.train.Checkpoint(optimizer=optimizer, net=classification)\n",
    "        manager = tf.train.CheckpointManager(ckpt, directory = filepath , max_to_keep=3)\n",
    "        ckpt.restore(manager.latest_checkpoint)\n",
    "        embedding = classification.embedding\n",
    "    else:\n",
    "        ckpt = tf.train.Checkpoint(optimizer=optimizer, net=embedding)\n",
    "        manager = tf.train.CheckpointManager(ckpt, directory = filepath , max_to_keep=3)\n",
    "        ckpt.restore(manager.latest_checkpoint)\n",
    "        \n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "    return embedding, classification\n",
    "\n",
    "# Example of use:\n",
    "# embedding, classification = load_model(\"Weights/TL/classifier/training/\", learning_rate = 0.0001 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdqR0rG9DX6q"
   },
   "source": [
    "## Pretraining: Simple CNN\n",
    "\n",
    "We start by pre training the CNN with cross entropy loss, because it is a very stable loss function. This pre trained model will be used later to train the embeddings and the final classification model. \n",
    "\n",
    "We first define the hyperparameters: learning rate, training iterations, batch size, input size of the CNN, number of classification outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPKcGP2n6xZB"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "training_iters = 150\n",
    "batch_size = 16\n",
    "display_step = 10\n",
    "filepath = \"Weights/EWC/Pad-IMS/classifier/pretraining/\"\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 1000 # Length of input data\n",
    "num_classes = 4  # number of classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbRV0wXp7UrJ"
   },
   "source": [
    "### Embeddings\n",
    "\n",
    "Next, we define the structure of the embedding network:\n",
    "\n",
    "It consists of a 1D neural network, with the input the temporal series (waveform). \n",
    "\n",
    "The CNN has 3 1D Convolutional layers with 32,32 and 64 filters respectively, height=10, strides=4 and padding='same'. We have 3 MaxPooling subsampling layers with pool size = 10, strides = 4. Then we have 2 FC with 32 nodes, and another with 20 output nodes. This last layer is the output of the embedding- All layers have Relu activation function. This embedding will be used later on to classify the faults into different faults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qj2e45pf7WWZ"
   },
   "source": [
    "### Classification model\n",
    "\n",
    "The classification model takes as input the output of the embedding model, and adds 3 Fc layers, with 64, 64 and 32 filters each. The output is the number of types of defects (4). During the pretraining process, all layers will be trained using cross entropy loss.\n",
    "\n",
    "You can omit next cell and load the save weights directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 229729,
     "status": "ok",
     "timestamp": 1586446697715,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "FqW2peNWEN_s",
    "outputId": "c55eddff-b63e-4605-992f-f35a46223d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer classification_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "step: 10, loss: 1.361948, accuracy: 0.437500\n",
      "step: 20, loss: 1.339885, accuracy: 0.500000\n",
      "step: 30, loss: 1.277514, accuracy: 0.750000\n",
      "step: 40, loss: 1.272324, accuracy: 0.437500\n",
      "step: 50, loss: 1.208798, accuracy: 0.500000\n",
      "step: 60, loss: 1.182407, accuracy: 0.625000\n",
      "step: 70, loss: 1.146564, accuracy: 0.625000\n",
      "step: 80, loss: 1.286399, accuracy: 0.312500\n",
      "step: 90, loss: 1.134626, accuracy: 0.562500\n",
      "step: 100, loss: 1.168085, accuracy: 0.625000\n",
      "step: 110, loss: 1.102629, accuracy: 0.812500\n",
      "step: 120, loss: 1.149158, accuracy: 0.500000\n",
      "step: 130, loss: 1.058008, accuracy: 0.750000\n",
      "step: 140, loss: 1.028020, accuracy: 0.812500\n",
      "step: 150, loss: 1.046238, accuracy: 0.750000\n",
      "Saved checkpoint for step Weights/EWC/Pad-IMS/classifier/pretraining/ckpt-1\n"
     ]
    }
   ],
   "source": [
    "# Build neural network model.\n",
    "embedding = Embedding()\n",
    "conv_net = Classification(embedding)\n",
    "\n",
    "pretrain = Pretraining(conv_net, learning_rate, training_iters, batch_size, display_step,filepath, restore=False)\n",
    "\n",
    "pretrain.fit(X_train_Pad, y_train_Pad, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 230011,
     "status": "ok",
     "timestamp": 1586446698023,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "gn3Zvtn8Og_0",
    "outputId": "a439f4c5-f805-40e9-9a3e-f7d0a60b99ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from Weights/EWC/Pad-IMS/classifier/pretraining/ckpt-1\n",
      "WARNING:tensorflow:Layer classification is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.out.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.out.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.conv1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.conv1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.conv2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.conv2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.conv3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.conv3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.conv4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.conv4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.embeddings.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embedding.embeddings.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.out.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.out.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.conv1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.conv1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.conv2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.conv2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.conv3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.conv3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.conv4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.conv4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.embeddings.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embedding.embeddings.beta\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Layer embedding is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test Accuracy: 0.781667\n"
     ]
    }
   ],
   "source": [
    "# Comment this line to use the model you just trained, and not the loaded weights\n",
    "embedding, conv_net = load_model(\"Weights/EWC/Pad-IMS/classifier/pretraining/\")\n",
    "\n",
    "# Test model on validation set.\n",
    "pred= conv_net(X_test_Pad)\n",
    "emb = embedding(X_test_Pad)\n",
    "y_pred = np.argmax(pred.numpy(), axis=1)\n",
    "print(\"Test Accuracy: %f\" % accuracy_score(y_pred, y_test_Pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQTVODnCgzIG"
   },
   "source": [
    "This ends the pretraining phase. We observe that we have obtained an accuracy of 0.90, which will be improved by training the embeddings with triplet loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6bPSNeytfHq"
   },
   "source": [
    "## Triplet loss\n",
    "\n",
    "Here we define the functions that will be used to compute the triplet loss of the embeddings. We use online triplet mining, with both 'batch all' and 'batch hard' strategies. \n",
    "\n",
    "For more information about triplet loss, refer to the notebook *Triplet learning for bearing fault classification*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GB-a0fuqDqew"
   },
   "source": [
    "## Training Triplet loss\n",
    "\n",
    "Now that we have a good initialization for the Weights from the pretrained model, we perform triplet loss optimization of the embeddings. \n",
    "\n",
    "We first define the hyperparameters for this part: learning rate, training iterations, batch size, margin, triplet strategy, embedding size and input size.  \n",
    "\n",
    "You can skip these three cells and load directly the saved weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RqF2ipvYO6h9"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "training_iters = 50\n",
    "batch_size = 64\n",
    "\n",
    "#Triplet parameters\n",
    "margin = 1.0\n",
    "squared=False\n",
    "triplet_strategy= 'batch_all'\n",
    "filepath = \"Weights/EWC/Pad-IMS/embedding/\"\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 1000 # Length of input data\n",
    "embedding_size = 20 # length of embedding features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMtbH3Dj3cZt"
   },
   "outputs": [],
   "source": [
    "#Function to get batches\n",
    "def get_all_batches(data,labels,batch_size):\n",
    "  '''\n",
    "  Gets all the batches from existing dataset.\n",
    "\n",
    "  Args:\n",
    "    data (numpy array): X data\n",
    "    labels (numpy array): y data\n",
    "    batch_size (int): Batch size\n",
    "\n",
    "  Returns:\n",
    "    batches_x (list of numpy array): list of batches for X data\n",
    "    batches_y (list of numpy array): list of batches for y data\n",
    "    num_batches (int): Number of batches\n",
    "  '''\n",
    "  idx = np.arange(0,data.shape[0])\n",
    "  np.random.shuffle(idx)\n",
    "\n",
    "  data_shuffle = data[idx,:]\n",
    "  labels_shuffle = labels[idx]\n",
    "\n",
    "  labels_shuffle = labels_shuffle[0:labels.shape[0] - labels.shape[0]%batch_size]\n",
    "  data_shuffle = data_shuffle[0:labels.shape[0] - labels.shape[0]%batch_size,:]\n",
    "  \n",
    "  num_batches = data_shuffle.shape[0]/batch_size \n",
    "  batches_x =np.split(data_shuffle,num_batches)\n",
    "  batches_y =np.split(labels_shuffle,num_batches)\n",
    "\n",
    "  return batches_x,batches_y,num_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1vSPfEz8TdY"
   },
   "source": [
    "We train the embeddings using triplet loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 234474,
     "status": "ok",
     "timestamp": 1586446702526,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "dJzS5VHEO6XQ",
    "outputId": "af28ccc3-cfeb-46c0-ca42-ae545ca1472e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900.0\n",
      "step:  10 Loss 0.50422466 Fraction positive triplets 0.009341253\n",
      "step:  20 Loss 0.44714245 Fraction positive triplets 0.019443942\n",
      "step:  30 Loss 0.48731408 Fraction positive triplets 0.029292094\n",
      "step:  40 Loss 0.61029047 Fraction positive triplets 0.018208342\n",
      "step:  50 Loss 0.7752452 Fraction positive triplets 0.014164867\n",
      "step:  60 Loss 0.53884 Fraction positive triplets 0.016198045\n",
      "step:  70 Loss 0.4249631 Fraction positive triplets 0.0037630536\n",
      "step:  80 Loss 0.16261636 Fraction positive triplets 0.0010281386\n",
      "step:  90 Loss 0.3114167 Fraction positive triplets 0.003457217\n",
      "step:  100 Loss 0.3327564 Fraction positive triplets 0.0011699065\n",
      "step:  110 Loss 0.49655762 Fraction positive triplets 0.015997117\n",
      "step:  120 Loss 0.47886065 Fraction positive triplets 0.005943162\n",
      "step:  130 Loss 0.58597267 Fraction positive triplets 0.0086269\n",
      "step:  140 Loss 0.28610748 Fraction positive triplets 0.002339813\n",
      "step:  150 Loss 0.2268829 Fraction positive triplets 0.0029175521\n",
      "step:  160 Loss 0.36806956 Fraction positive triplets 0.0032932623\n",
      "step:  170 Loss 0.30425873 Fraction positive triplets 0.0021058316\n",
      "step:  180 Loss 0.3594811 Fraction positive triplets 0.0024135446\n",
      "step:  190 Loss 0.6468951 Fraction positive triplets 0.011414374\n",
      "step:  200 Loss 0.28866604 Fraction positive triplets 0.0017648118\n",
      "step:  210 Loss 0.42652556 Fraction positive triplets 0.0032234828\n",
      "step:  220 Loss 0.35856435 Fraction positive triplets 0.00093643076\n",
      "step:  230 Loss 0.14933898 Fraction positive triplets 0.00019835545\n",
      "step:  240 Loss 0.1751289 Fraction positive triplets 0.00018037518\n",
      "step:  250 Loss 0.21753018 Fraction positive triplets 0.00088231056\n",
      "step:  260 Loss 0.3325676 Fraction positive triplets 0.0016347502\n",
      "step:  270 Loss 0.43163088 Fraction positive triplets 0.0006292248\n",
      "step:  280 Loss 0.030518213 Fraction positive triplets 0.0001079797\n",
      "step:  290 Loss 0.29861182 Fraction positive triplets 0.0011165136\n",
      "step:  300 Loss 0.2801168 Fraction positive triplets 0.00036029544\n",
      "step:  310 Loss 0.25475338 Fraction positive triplets 0.000524602\n",
      "step:  320 Loss 0.123712264 Fraction positive triplets 0.00043181\n",
      "step:  330 Loss 0.32559717 Fraction positive triplets 0.0010861823\n",
      "step:  340 Loss 0.309181 Fraction positive triplets 0.00025175328\n",
      "step:  350 Loss 0.3593311 Fraction positive triplets 0.0025388936\n",
      "step:  360 Loss 0.29243347 Fraction positive triplets 0.0002881637\n",
      "step:  370 Loss 0.25977686 Fraction positive triplets 0.00019805545\n",
      "step:  380 Loss 0.23242879 Fraction positive triplets 3.6010082e-05\n",
      "step:  390 Loss 0.35196254 Fraction positive triplets 0.0026206397\n",
      "step:  400 Loss 0.21790165 Fraction positive triplets 0.0005397625\n",
      "step:  410 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  420 Loss 0.38794008 Fraction positive triplets 0.0010814707\n",
      "step:  430 Loss 0.32728824 Fraction positive triplets 0.00077354826\n",
      "step:  440 Loss 0.30268687 Fraction positive triplets 0.00048625868\n",
      "step:  450 Loss 0.4636254 Fraction positive triplets 0.0010985053\n",
      "step:  460 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  470 Loss 0.20017958 Fraction positive triplets 7.1415816e-05\n",
      "step:  480 Loss 0.385452 Fraction positive triplets 0.0005568929\n",
      "step:  490 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  500 Loss 0.056831837 Fraction positive triplets 1.8008284e-05\n",
      "step:  510 Loss 0.5943068 Fraction positive triplets 0.00073794095\n",
      "step:  520 Loss 0.40832648 Fraction positive triplets 0.0019420967\n",
      "step:  530 Loss 0.23689957 Fraction positive triplets 8.947745e-05\n",
      "step:  540 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  550 Loss 0.10014868 Fraction positive triplets 1.8006338e-05\n",
      "step:  560 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  570 Loss 0.0023417473 Fraction positive triplets 1.8037517e-05\n",
      "step:  580 Loss 0.3674709 Fraction positive triplets 0.00012662807\n",
      "step:  590 Loss 0.19715047 Fraction positive triplets 0.00014406627\n",
      "step:  600 Loss 0.30399388 Fraction positive triplets 0.0005402874\n",
      "step:  610 Loss 0.20402603 Fraction positive triplets 0.000180538\n",
      "step:  620 Loss 0.12937331 Fraction positive triplets 7.204611e-05\n",
      "step:  630 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  640 Loss 0.53130645 Fraction positive triplets 0.0005553565\n",
      "step:  650 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  660 Loss 0.27448046 Fraction positive triplets 7.195798e-05\n",
      "step:  670 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  680 Loss 0.24761152 Fraction positive triplets 3.6019163e-05\n",
      "step:  690 Loss 0.016985098 Fraction positive triplets 5.368647e-05\n",
      "step:  700 Loss 0.53047895 Fraction positive triplets 7.2293515e-05\n",
      "step:  710 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  720 Loss 0.4362041 Fraction positive triplets 0.00068436406\n",
      "step:  730 Loss 0.29375592 Fraction positive triplets 0.00053933554\n",
      "step:  740 Loss 0.19802895 Fraction positive triplets 8.890786e-05\n",
      "step:  750 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  760 Loss 0.108435154 Fraction positive triplets 1.8000504e-05\n",
      "step:  770 Loss 0.31723285 Fraction positive triplets 0.00037810588\n",
      "step:  780 Loss 0.27256313 Fraction positive triplets 0.00012603529\n",
      "step:  790 Loss 0.3559066 Fraction positive triplets 7.1929506e-05\n",
      "step:  800 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  810 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  820 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  830 Loss 0.0661484 Fraction positive triplets 7.171029e-05\n",
      "step:  840 Loss 0.066520214 Fraction positive triplets 1.7946877e-05\n",
      "step:  850 Loss 0.0 Fraction positive triplets 0.0\n",
      "step:  860 Loss 0.2029025 Fraction positive triplets 0.00023431868\n",
      "step:  870 Loss 0.21874298 Fraction positive triplets 0.00032417246\n",
      "step:  880 Loss 0.1399405 Fraction positive triplets 3.6038633e-05\n",
      "step:  890 Loss 0.22382227 Fraction positive triplets 0.00017914726\n",
      "step:  900 Loss 0.40477362 Fraction positive triplets 0.0002699784\n",
      "Saved checkpoint for step Weights/EWC/Pad-IMS/embedding/ckpt-1\n"
     ]
    }
   ],
   "source": [
    "_,_,num_batches = get_all_batches(X_train_Pad,y_train_Pad,batch_size)\n",
    "print(num_batches*training_iters)\n",
    "\n",
    "train_emb = Train_Embeddings(embedding, learning_rate, int(num_batches*training_iters),\n",
    "                             batch_size, display_step, triplet_strategy, margin, \n",
    "                             squared, filepath, restore=False)\n",
    "\n",
    "train_emb.fit(X_train_Pad, y_train_Pad, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_SuENTyO6PA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from Weights/EWC/Pad-IMS/embedding/ckpt-1\n",
      "WARNING:tensorflow:Layer embedding_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embeddings.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embeddings.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embeddings.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embeddings.beta\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "# Comment this line to use the model you just trained and \n",
    "embedding, conv_net = load_model(\"Weights/EWC/Pad-IMS/embedding/\", net=\"emb\")\n",
    "\n",
    "emb2 = embedding(X_test_Pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lb9r4Lac8bnt"
   },
   "source": [
    "We visualize the embeddings using UMAP.  For  more information about UMAP see the notebook *Siamese network ofr bearing fault classification*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6340,
     "status": "ok",
     "timestamp": 1586446862036,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "tNNyEMvY7gWc",
    "outputId": "b07adf5d-38c5-4910-a0af-729063a04e35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laia.domingo.colomer\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\umap\\spectral.py:229: UserWarning: Embedding a total of 3 separate connected components using meta-embedding (experimental)\n",
      "  n_components\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = emb2\n",
    "\n",
    "embed= umap.UMAP(n_neighbors=100,\n",
    "                      min_dist=0.5,\n",
    "                      metric='correlation').fit_transform(features)\n",
    "\n",
    "color = pd.DataFrame(y_test_Pad,columns=['color'])\n",
    "color.replace({0:'red', 1:'blue', 2:'green', 3:'orange'},inplace=True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.scatter(embed[:,0], embed[:,1], \n",
    "            c=color.values.flatten(),\n",
    "            cmap=\"Spectral\", \n",
    "            s=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "plt.title(\"Extracted features Paderborn\", fontsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFlk_6GTDH0-"
   },
   "source": [
    "We can see that the embeddings are far apart for different failures, and close together for the same failure.\n",
    "\n",
    "Now we compute the embeddings for the IMS data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9555,
     "status": "ok",
     "timestamp": 1586446892939,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "2QtEVCxolMgI",
    "outputId": "53ccb0f6-d3bd-45e0-9948-a81a0560887a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb = embedding(X_test_IMS)\n",
    "\n",
    "features = emb\n",
    "\n",
    "embed= umap.UMAP(n_neighbors=100,\n",
    "                      min_dist=0.5,\n",
    "                      metric='correlation').fit_transform(features)\n",
    "\n",
    "color = pd.DataFrame(y_test_IMS,columns=['color'])\n",
    "color.replace({0:'red', 1:'blue', 2:'green', 3:'orange'},inplace=True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.scatter(embed[:,0], embed[:,1], \n",
    "            c=color.values.flatten(),\n",
    "            cmap=\"Spectral\", \n",
    "            s=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "plt.title(\"Extracted features IMS\", fontsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00cj7gEDD7TO"
   },
   "source": [
    "## Classification model using triplet loss embeddings\n",
    "\n",
    "Now that we have trained the embeddings using triplet loss, we will train the classifier using cross entropy loss. During this training the embedding layers will be frozen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6tUtCzD7kkh"
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "num_classes = 4\n",
    "batch_size= 64\n",
    "training_iters=200\n",
    "learning_rate =0.0001\n",
    "filepath = \"Weights/EWC/Pad-IMS/classifier/training/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1131,
     "status": "ok",
     "timestamp": 1585907538070,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "LFPsejs3Fpa3",
    "outputId": "a5153ed4-3002-4d15-b4bd-e750e5b46f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10, loss: 0.774755, accuracy: 1.000000\n",
      "step: 20, loss: 0.763366, accuracy: 1.000000\n",
      "step: 30, loss: 0.753875, accuracy: 1.000000\n",
      "step: 40, loss: 0.749617, accuracy: 1.000000\n",
      "step: 50, loss: 0.748159, accuracy: 1.000000\n",
      "step: 60, loss: 0.752161, accuracy: 1.000000\n",
      "step: 70, loss: 0.763001, accuracy: 0.984375\n",
      "step: 80, loss: 0.749763, accuracy: 1.000000\n",
      "step: 90, loss: 0.746223, accuracy: 1.000000\n",
      "step: 100, loss: 0.749516, accuracy: 1.000000\n",
      "step: 110, loss: 0.745420, accuracy: 1.000000\n",
      "step: 120, loss: 0.747463, accuracy: 1.000000\n",
      "step: 130, loss: 0.745546, accuracy: 1.000000\n",
      "step: 140, loss: 0.746108, accuracy: 1.000000\n",
      "step: 150, loss: 0.746910, accuracy: 1.000000\n",
      "step: 160, loss: 0.745138, accuracy: 1.000000\n",
      "step: 170, loss: 0.745334, accuracy: 1.000000\n",
      "step: 180, loss: 0.745409, accuracy: 1.000000\n",
      "step: 190, loss: 0.746383, accuracy: 1.000000\n",
      "step: 200, loss: 0.744639, accuracy: 1.000000\n",
      "Saved checkpoint for step Weights/EWC/Pad-IMS/classifier/training/ckpt-1\n"
     ]
    }
   ],
   "source": [
    "# Build neural network model.\n",
    "classifier = Training_classifier(conv_net, learning_rate, training_iters, batch_size, display_step, filepath, restore=False)\n",
    "\n",
    "classifier.fit(X_train_Pad, y_train_Pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9mFYSd89A6w"
   },
   "source": [
    "We check the final accuracy in the IMS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1585901059113,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "m6RvHGkZUUZh",
    "outputId": "d30a8328-1e13-4025-8f4e-bffba976a9c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from Weights/EWC/Pad-IMS/classifier/training/ckpt-1\n",
      "WARNING:tensorflow:Layer classification_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.out.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.out.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.out.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.out.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Test Accuracy: 0.191281\n"
     ]
    }
   ],
   "source": [
    "# Comment this line to use the model you just trained and \n",
    "embedding, conv_net = load_model(\"Weights/EWC/Pad-IMS/classifier/training/\")\n",
    "\n",
    "# Test model on test set.\n",
    "pred= conv_net(X_test_IMS)\n",
    "y_pred = np.argmax(pred.numpy(), axis=1)\n",
    "print(\"Test Accuracy: %f\" % accuracy_score(y_pred, y_test_IMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1585901060282,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "xQ9TLzMcaezu",
    "outputId": "eda0d7b5-d4e0-46c8-9934-6ce42e5dd87a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[350,   0,   0,   0],\n",
       "       [589,   1,   0,   0],\n",
       "       [305,   0,   0,   0],\n",
       "       [590,   0,   0,   0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test_IMS, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FMVbB8RaGU4x"
   },
   "source": [
    "Now we test the accuracy on Padeborn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1585901061431,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "hsBif9c2GSJz",
    "outputId": "96948e64-2761-435e-ae4a-13252c0f0343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[399   0   1]\n",
      " [  0 400   0]\n",
      " [  0   0 400]]\n",
      "Test Accuracy: 0.999167\n"
     ]
    }
   ],
   "source": [
    "# Test model on test set.\n",
    "pred= conv_net(X_test_Pad)\n",
    "y_pred = np.argmax(pred.numpy(), axis=1)\n",
    "print(confusion_matrix(y_test_Pad, y_pred))\n",
    "\n",
    "print(\"Test Accuracy: %f\" % accuracy_score(y_pred, y_test_Pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JKV2WI7xJQL"
   },
   "source": [
    "We notice that the model has a bad accuracy for IMS data. \n",
    "If this data is thought as concept drift, or a behaviour that can happen, we want the model to capture both behaviours. For this reason, we will try Elastic weight consolidation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywvfYXwVGnrO"
   },
   "source": [
    "## Elastic Weight Consolidation\n",
    "\n",
    "The goal of this technique is to train a model to perform task B without forgetting about task A. In order to do so, we will add a weighted L2 regularization. We describe briefely the theory behind this technique:\n",
    "\n",
    "We model the posterior log probability of the parameters given the two datasets using Bayes theorem:\n",
    "\n",
    "$$ \\log p(\\theta|D_A,D_B ) = \\log p(D_B|\\theta) + \\log p(\\theta|D_A) - \\log p(D_B|D_A)$$\n",
    "\n",
    "The left hand side is the log likelihood of the final model (in oder words, the loss function we want to minimize), the right-hand term can be thought of as an adaptive elastic weight regularizer that tries to maintain knowledge of the first task.\n",
    "\n",
    "The term $ \\log p(D_B|\\theta)$ is the loss function for task B, and the term $\\log p(D_B|D_A)$ does not depend on $\\theta$. We have to calculate $\\log p(\\theta|D_A)$. However, this term is intractable, in the sense that it is difficult to find a closed form. Assuming that $p(\\theta|D_A)$ is smooth and with a peak at $\\theta_A^*$ (the optimal weights of task A), we can approximate it with a normal distribution with mean $\\theta_A^*$ and variance $\\mathbb{1}_A$. This technique is called Laplace approximation. Refer to the Master Thesis pdf for more information about the derivation of the Laplace Approximation. The term $\\mathbb{1}_A$ is the hessian matrix of $\\log p(\\theta|D_A)$ and it is called Fisher information matrix. Here we will do a further approximation by using only the diagonal terms of the Fisher information matrix, which we will denote as $\\hat{\\mathbb{1}}_A$. Finally, the loss function we will minimize is:\n",
    "\n",
    "\n",
    "$$\\mathcal{L}_{B|A}(\\theta) = \\mathcal{L}_B(\\theta) + \\lambda (\\theta - \\theta_A^*)^T \\hat{\\mathbb{1}}_A (\\theta - \\theta_A^*) $$\n",
    "\n",
    "Where $\\lambda$ is a parameter to control the importance to the regularization term. Since $\\hat{\\mathbb{1}}_A$ is a diagonal matrix, the previous expression can be written as\n",
    "\n",
    "$$\\mathcal{L}_{B|A}(\\theta) = \\mathcal{L}_B(\\theta) + \\lambda \\sum_i [\\hat{\\mathbb{1}}_A]_{i} (\\theta_i - \\theta_{A_i}^*)^2 $$\n",
    "\n",
    "Where the sum is over all the parameters of the network, and $[\\hat{\\mathbb{1}}_A]_{i}$ is the ith diagonal term of $F_A$. Finally, we need to define how to compute $[F_A]_{i}$:\n",
    "\n",
    "$$[\\hat{\\mathbb{1}}_A]_{i} = \\frac{1}{N} \\sum_i \\Big(\\frac{\\partial \\mathcal{L}_A(x_{A_i}| \\theta_A^*)}{\\partial \\theta_{A_i}}\\Big)^2 $$\n",
    "\n",
    "Where the sum is made over the samples of the dataset from task A.\n",
    "\n",
    "With this information we have been able to train a model for task $B$ without forgetting about taks $A$. What happens if we want to learn task $C$ without forgetting about tasks A and B? Note that this will be the case in our setting, since the model will be updated when new data arribes and we do not want to forget about *all* the previously learnt tasks. In this case we just need to replace the fisher matrix by the one of tasks A and B.  That is:\n",
    "\n",
    "$$\\mathcal{L}_{C|A,B}(\\theta) = \\mathcal{L}_C(\\theta) + \\lambda \\sum_i[\\hat{\\mathbb{1}}_{A,B}]_i (\\theta_i - \\theta_{{A,B}_i}^*)^2 $$\n",
    "\n",
    "That is, the optimal parameters $\\theta_{{A,B}_i}^*$ are the ones obtained after applying EWC to task B, and the new Fisher information matrix is the one obtained after training both tasks (A and B). This process can be extended for many tasks, just by updating theFisher information matrix.\n",
    "\n",
    "**Error in EWC**\n",
    "\n",
    "EWC gives a way to approximate the posterior probability $p(\\theta|D_A, D_B)$, but it does not say anything about the quality of this approximation. The sources of error in EWC are the following:\n",
    "\n",
    "+ **Laplace Approximation**: It is used to approximate $p(\\theta|D_A)$ by using the second  order Taylor expansion. The Laplace approximation has formally proven to be correct at the limit of large data sets. The Bernstein-von Mises theorem proves this fact when the model is regular, the prior probability density function is smooth and the sample size n is large.\n",
    "\n",
    "+ **Diagonalization of Fisher information matrix (FIM)**: Instead of considering the whole FIM, only the diagonal components are taken into account. Although there are not clear error bounds for this approximation, if the matrix is diagonally-dominant, the approximated matrix contains the most relevant terms of the original matrix. Since the FIM is the Hessian matrix of $ f(\\theta) = -log(p(\\theta | D_A))$ around $\\theta_A^*$, and $f(\\theta)$ is concave around its maximum $\\theta_A^*$, the FIM is positive definite and therefore diagonally-dominant. \n",
    "\n",
    "For more information about the Errors in EWC, refer to the Master Thesis pdf.\n",
    "\n",
    "**References:**\n",
    "\n",
    "\n",
    "+ Kirkpatrick, James; Pascanu, Razvan; Rabinowitz, Neil; Veness, Joel; Desjardins, Guillaume; Rusu, Andrei A.; Milan, Kieran; Quan, John; Ramalho, Tiago; Grabska-Barwinska, Agnieszka; Hassabis, Demis; Clopath, Claudia; Kumaran, Dharshan; Hadsell, Raia. *Overcoming catastrophic forgetting in neural networks.* arXiv.org/1612.00796v2\n",
    "\n",
    "+ Guillaume P. Dehaene.A deterministic and computable Bernstein-von Mises theo-rem. 2019. arXiv: 1904.02505\n",
    "\n",
    "+ Ferenc Huszar. *Comment on \"Overcoming catastrophic forgetting in NNs\":Are multiple penalties needed?*.  URL: https://www.inference.vc/comment-on-overcoming-catastrophic-forgetting-in-nns-are-multiple-penalties-needed-2/.\n",
    "\n",
    "##  Fisher information matrix for triplet loss\n",
    "\n",
    "First we create a class that calculates the fisher matrix for an existing model.\n",
    "We will define the two fisher matrices: for the embeddings (triplet loss) and the classifier (cross entropy loss). \n",
    "\n",
    "The fisher information matrix for the embeddings is a bit tricky, since we do not have a loss function for every sample of the dataset, but for every hard/semi-hard triplet. In order to speed up the computations, we will use the batch hard strategy to compute the loss for every hard triplet. We will sample a batch, calculate the hard loss for all the elements of the batch, slice the tensor to get the $ith$ element, calculate its loss function,get the gradients and append the square gradients to a list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DiJ3HcFeEhXt"
   },
   "source": [
    "We create a class to compute the fisher information matrix for both the Triplet loss (embeddings) and Cross entropy (classification) losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xly5vqtwyQdH"
   },
   "source": [
    "Now we calculate the fisher information matrix for both the embeddings (triplet loss) and the classifier (cross entropy loss).\n",
    "\n",
    "You can skip all these cells (until the validation). The training of EWC may take some hours to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 177171,
     "status": "ok",
     "timestamp": 1585901241233,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "r2C-27rOqg7u",
    "outputId": "112861f5-7f3b-4f13-b4d8-30e595391901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 10 calls to <function Embedding.call at 0x7fc9d81758c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    }
   ],
   "source": [
    "#Triplet loss fisher matrix\n",
    "conv_net.unfreeze()\n",
    "fisher_matrix_TL =  Fisher_matrix(X_train_Pad, y_train_Pad, embedding, task = 'triplet loss', batch_size = 32,\n",
    "                     triplet_strategy='batch_hard', margin=margin, squared = squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIozUnReERgA"
   },
   "outputs": [],
   "source": [
    "#Cross entropy loss fisher matrix\n",
    "conv_net.freeze()\n",
    "\n",
    "fisher_matrix_CL =  Fisher_matrix(X_train_Pad, y_train_Pad, conv_net, task = 'classsification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5IBvgwmyvoc"
   },
   "source": [
    "Now we will train the embeddings with triplet loss and EWC and then the classifier with cross entropy and EWC.\n",
    "\n",
    "## Triplet loss + EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1585901480606,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "ZZPEhwx9IUef",
    "outputId": "b2aba4c5-8864-483f-f77c-58ce3b77f887"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_iters = 1000\n",
    "display_step=1\n",
    "learning_rate = 0.0001\n",
    "filepath =\"Weights/EWC/Pad-IMS/fisher/embedding/\"\n",
    "lamb = 1000\n",
    "int(num_batches*training_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3760896,
     "status": "ok",
     "timestamp": 1585907207381,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "kN9gku4rPUT4",
    "outputId": "f14895d9-6736-4ed5-b2a0-10864d0d0359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 loss 1.37866 fisher_loss 0 triplet loss 1.37866 l2_loss 0 fraction B 0.766808093 lossA 0.41639182 fraction A 0.000210272701\n",
      "step 2 loss 1.17830491 fisher_loss 0.00327405916 triplet loss 1.17503083 l2_loss 0.000579645 fraction B 0.81277287 lossA 0.428887159 fraction A 0.000260513567\n",
      "step 3 loss 1.14255285 fisher_loss 0.00970569346 triplet loss 1.13284719 l2_loss 0.00175813958 fraction B 0.813587189 lossA 0.548332691 fraction A 0.00114126818\n",
      "step 4 loss 0.972041965 fisher_loss 0.0209718049 triplet loss 0.951070189 l2_loss 0.00360836252 fraction B 0.797840118 lossA 0.880504549 fraction A 0.0078730369\n",
      "step 5 loss 0.903111756 fisher_loss 0.03609965 triplet loss 0.867012084 l2_loss 0.00603108294 fraction B 0.842161953 lossA 1.31802833 fraction A 0.0195279401\n",
      "step 6 loss 0.842536 fisher_loss 0.0531547964 triplet loss 0.789381146 l2_loss 0.00894904416 fraction B 0.893650115 lossA 1.61867785 fraction A 0.0329302251\n",
      "step 7 loss 0.905727267 fisher_loss 0.0716518536 triplet loss 0.834075391 l2_loss 0.0122871101 fraction B 0.858071 lossA 1.75739348 fraction A 0.0451934151\n",
      "step 8 loss 0.819540083 fisher_loss 0.0867489651 triplet loss 0.732791126 l2_loss 0.0157038327 fraction B 0.667034328 lossA 1.81552112 fraction A 0.0533249751\n",
      "step 9 loss 0.868366599 fisher_loss 0.0948922187 triplet loss 0.773474395 l2_loss 0.0187276844 fraction B 0.804139256 lossA 1.85339916 fraction A 0.058509022\n",
      "step 10 loss 0.994865716 fisher_loss 0.0980766043 triplet loss 0.896789134 l2_loss 0.0214930307 fraction B 0.746894598 lossA 1.86309779 fraction A 0.0583804287\n",
      "step 11 loss 0.938914657 fisher_loss 0.0954922587 triplet loss 0.843422413 l2_loss 0.0235652663 fraction B 0.802361 lossA 1.85543072 fraction A 0.0534868538\n",
      "step 12 loss 0.926746368 fisher_loss 0.0876217186 triplet loss 0.83912468 l2_loss 0.0246820636 fraction B 0.743497 lossA 1.83877611 fraction A 0.0477546118\n",
      "step 13 loss 0.785975814 fisher_loss 0.080067344 triplet loss 0.705908477 l2_loss 0.0257322919 fraction B 0.774501562 lossA 1.81221747 fraction A 0.0417019837\n",
      "step 14 loss 0.86694324 fisher_loss 0.0729351342 triplet loss 0.794008076 l2_loss 0.026613513 fraction B 0.765479267 lossA 1.76362669 fraction A 0.0343598612\n",
      "step 15 loss 0.959768355 fisher_loss 0.0657042041 triplet loss 0.894064128 l2_loss 0.0272370316 fraction B 0.803942263 lossA 1.70257092 fraction A 0.0292622447\n",
      "step 16 loss 0.877121 fisher_loss 0.0609084666 triplet loss 0.816212535 l2_loss 0.0280408058 fraction B 0.812848 lossA 1.65664089 fraction A 0.0266798194\n",
      "step 17 loss 0.896918476 fisher_loss 0.0592140071 triplet loss 0.83770448 l2_loss 0.0292774811 fraction B 0.736685276 lossA 1.61473227 fraction A 0.024967283\n",
      "step 18 loss 0.866726 fisher_loss 0.0586880967 triplet loss 0.808037877 l2_loss 0.030708 fraction B 0.778024256 lossA 1.59246218 fraction A 0.0243245121\n",
      "step 19 loss 0.787632227 fisher_loss 0.0595049188 triplet loss 0.728127301 l2_loss 0.0324063525 fraction B 0.782530189 lossA 1.60762751 fraction A 0.0254872236\n",
      "step 20 loss 0.824227393 fisher_loss 0.0621451251 triplet loss 0.762082279 l2_loss 0.0345002115 fraction B 0.727897286 lossA 1.65419197 fraction A 0.0284428149\n",
      "step 21 loss 0.860584378 fisher_loss 0.0663171038 triplet loss 0.794267297 l2_loss 0.0370991826 fraction B 0.75825429 lossA 1.72313213 fraction A 0.0342869088\n",
      "step 22 loss 0.759390771 fisher_loss 0.0725736767 triplet loss 0.68681711 l2_loss 0.0402851626 fraction B 0.67134577 lossA 1.7799089 fraction A 0.042348884\n",
      "step 23 loss 0.76509428 fisher_loss 0.0807944909 triplet loss 0.684299767 l2_loss 0.0439202972 fraction B 0.592389345 lossA 1.83176529 fraction A 0.0526538044\n",
      "step 24 loss 0.861261 fisher_loss 0.0909607 triplet loss 0.770300329 l2_loss 0.048217129 fraction B 0.555863202 lossA 1.86791718 fraction A 0.0594792813\n",
      "step 25 loss 0.969265103 fisher_loss 0.0981405526 triplet loss 0.871124566 l2_loss 0.0520875268 fraction B 0.671559155 lossA 1.89152825 fraction A 0.064306967\n",
      "step 26 loss 0.899158776 fisher_loss 0.10333135 triplet loss 0.795827448 l2_loss 0.0555290543 fraction B 0.669510901 lossA 1.90389419 fraction A 0.0646299645\n",
      "step 27 loss 0.909705102 fisher_loss 0.104416586 triplet loss 0.805288494 l2_loss 0.0582483336 fraction B 0.606926858 lossA 1.90216637 fraction A 0.0618377402\n",
      "step 28 loss 0.876400232 fisher_loss 0.103320748 triplet loss 0.773079455 l2_loss 0.0606374852 fraction B 0.651092649 lossA 1.89470422 fraction A 0.0567254163\n",
      "step 29 loss 0.895526052 fisher_loss 0.100055493 triplet loss 0.795470536 l2_loss 0.0625155121 fraction B 0.582138598 lossA 1.87439632 fraction A 0.0498710684\n",
      "step 30 loss 0.940697312 fisher_loss 0.0960264 triplet loss 0.844670892 l2_loss 0.064293 fraction B 0.617425799 lossA 1.84679449 fraction A 0.0432346761\n",
      "step 31 loss 0.859261 fisher_loss 0.0928690508 triplet loss 0.766391933 l2_loss 0.0661571473 fraction B 0.633488357 lossA 1.79709423 fraction A 0.0363688469\n",
      "step 32 loss 0.894362032 fisher_loss 0.0903954 triplet loss 0.803966641 l2_loss 0.0682349801 fraction B 0.614234 lossA 1.75712025 fraction A 0.0323828645\n",
      "step 33 loss 0.872021198 fisher_loss 0.0900326073 triplet loss 0.781988621 l2_loss 0.0707372874 fraction B 0.671664417 lossA 1.76632297 fraction A 0.0325614773\n",
      "step 34 loss 0.880120873 fisher_loss 0.0925136209 triplet loss 0.787607253 l2_loss 0.0742201 fraction B 0.638208 lossA 1.81996286 fraction A 0.0368609317\n",
      "step 35 loss 0.840004265 fisher_loss 0.0976694 triplet loss 0.742334843 l2_loss 0.0784951672 fraction B 0.646708906 lossA 1.8839072 fraction A 0.04351772\n",
      "step 36 loss 0.931054473 fisher_loss 0.104665905 triplet loss 0.826388597 l2_loss 0.0833287314 fraction B 0.61072892 lossA 1.93593121 fraction A 0.0514622964\n",
      "step 37 loss 0.868861794 fisher_loss 0.112882763 triplet loss 0.755979061 l2_loss 0.0885480046 fraction B 0.530582786 lossA 1.9821068 fraction A 0.0593986139\n",
      "step 38 loss 0.932103932 fisher_loss 0.121381573 triplet loss 0.810722351 l2_loss 0.0938527882 fraction B 0.558627844 lossA 2.00763369 fraction A 0.0634294376\n",
      "step 39 loss 0.93117553 fisher_loss 0.126582161 triplet loss 0.804593384 l2_loss 0.0983488336 fraction B 0.574752152 lossA 2.02845883 fraction A 0.064642027\n",
      "step 40 loss 0.934237301 fisher_loss 0.128700927 triplet loss 0.805536389 l2_loss 0.102053083 fraction B 0.559183657 lossA 2.03406024 fraction A 0.0606072545\n",
      "step 41 loss 1.0330081 fisher_loss 0.126674712 triplet loss 0.906333387 l2_loss 0.104676746 fraction B 0.574781835 lossA 2.03060317 fraction A 0.0557699353\n",
      "step 42 loss 1.06238961 fisher_loss 0.123893082 triplet loss 0.93849647 l2_loss 0.106949084 fraction B 0.633560777 lossA 2.01111698 fraction A 0.0493245758\n",
      "step 43 loss 0.896293223 fisher_loss 0.120052136 triplet loss 0.776241064 l2_loss 0.108618475 fraction B 0.567071259 lossA 1.99096811 fraction A 0.0442564078\n",
      "step 44 loss 0.959618866 fisher_loss 0.117618881 triplet loss 0.842 l2_loss 0.110581949 fraction B 0.621429145 lossA 1.95662737 fraction A 0.0393671431\n",
      "step 45 loss 0.919321835 fisher_loss 0.115123071 triplet loss 0.804198742 l2_loss 0.112509482 fraction B 0.506670058 lossA 1.9475584 fraction A 0.0382494181\n",
      "step 46 loss 0.842114568 fisher_loss 0.115361221 triplet loss 0.726753354 l2_loss 0.115275614 fraction B 0.477065653 lossA 1.93794727 fraction A 0.0374783836\n",
      "step 47 loss 0.890017748 fisher_loss 0.1164883 triplet loss 0.77352947 l2_loss 0.118532561 fraction B 0.559819818 lossA 1.96170175 fraction A 0.0406020097\n",
      "step 48 loss 0.820004046 fisher_loss 0.120605037 triplet loss 0.699399 l2_loss 0.122772254 fraction B 0.488516062 lossA 2.00074816 fraction A 0.0466444232\n",
      "step 49 loss 0.890233636 fisher_loss 0.127002761 triplet loss 0.76323086 l2_loss 0.127675161 fraction B 0.53316009 lossA 2.03261113 fraction A 0.0519697033\n",
      "step 50 loss 0.945674 fisher_loss 0.132894456 triplet loss 0.812779546 l2_loss 0.132494628 fraction B 0.526309133 lossA 2.05453467 fraction A 0.0551434979\n",
      "step 51 loss 0.819144726 fisher_loss 0.136826783 triplet loss 0.682317913 l2_loss 0.13680844 fraction B 0.527952611 lossA 2.07880259 fraction A 0.0591199882\n",
      "step 52 loss 0.967099249 fisher_loss 0.141366467 triplet loss 0.825732768 l2_loss 0.141271532 fraction B 0.517267942 lossA 2.08818245 fraction A 0.0601878352\n",
      "step 53 loss 0.875031054 fisher_loss 0.143716052 triplet loss 0.731315 l2_loss 0.1451343 fraction B 0.530092895 lossA 2.0760529 fraction A 0.0569505021\n",
      "step 54 loss 0.88937825 fisher_loss 0.142715573 triplet loss 0.746662676 l2_loss 0.147988826 fraction B 0.519043922 lossA 2.06167984 fraction A 0.0529623479\n",
      "step 55 loss 0.853971243 fisher_loss 0.141524121 triplet loss 0.712447107 l2_loss 0.150791451 fraction B 0.519497 lossA 2.03624701 fraction A 0.0483866259\n",
      "step 56 loss 0.921591401 fisher_loss 0.140099972 triplet loss 0.781491399 l2_loss 0.153529197 fraction B 0.500383317 lossA 2.01450944 fraction A 0.0450638831\n",
      "step 57 loss 0.88814491 fisher_loss 0.139258817 triplet loss 0.748886108 l2_loss 0.156180173 fraction B 0.518556 lossA 1.99620771 fraction A 0.0435693376\n",
      "step 58 loss 0.935554624 fisher_loss 0.139970854 triplet loss 0.795583785 l2_loss 0.159519762 fraction B 0.516708612 lossA 1.97431898 fraction A 0.0422943681\n",
      "step 59 loss 0.93655163 fisher_loss 0.140800044 triplet loss 0.795751572 l2_loss 0.162760943 fraction B 0.452566534 lossA 1.95304573 fraction A 0.0412287712\n",
      "step 60 loss 0.844176412 fisher_loss 0.141551241 triplet loss 0.702625155 l2_loss 0.165915102 fraction B 0.469366521 lossA 1.9428823 fraction A 0.0413614176\n",
      "step 61 loss 0.983630657 fisher_loss 0.143225372 triplet loss 0.840405285 l2_loss 0.1695811 fraction B 0.50941819 lossA 1.93486571 fraction A 0.0418880247\n",
      "step 62 loss 0.987112343 fisher_loss 0.14474456 triplet loss 0.842367768 l2_loss 0.173194066 fraction B 0.581044197 lossA 1.94381082 fraction A 0.0441855192\n",
      "step 63 loss 0.88600415 fisher_loss 0.147252575 triplet loss 0.73875159 l2_loss 0.177027732 fraction B 0.479401588 lossA 1.97046781 fraction A 0.0493604019\n",
      "step 64 loss 1.06872392 fisher_loss 0.152675629 triplet loss 0.916048229 l2_loss 0.181858197 fraction B 0.498369217 lossA 1.98723793 fraction A 0.0533943065\n",
      "step 65 loss 1.05763221 fisher_loss 0.157263562 triplet loss 0.90036869 l2_loss 0.186315045 fraction B 0.477644116 lossA 2.00265861 fraction A 0.0546774566\n",
      "step 66 loss 0.964958906 fisher_loss 0.158903956 triplet loss 0.80605495 l2_loss 0.19007811 fraction B 0.499858469 lossA 1.98618329 fraction A 0.0506255515\n",
      "step 67 loss 0.811778903 fisher_loss 0.15754655 triplet loss 0.654232323 l2_loss 0.19269444 fraction B 0.459505528 lossA 1.98126495 fraction A 0.0477717817\n",
      "step 68 loss 1.09325647 fisher_loss 0.157677293 triplet loss 0.935579121 l2_loss 0.195931628 fraction B 0.572430611 lossA 1.96466601 fraction A 0.0431861\n",
      "step 69 loss 1.00028217 fisher_loss 0.156665787 triplet loss 0.843616426 l2_loss 0.198573992 fraction B 0.511189878 lossA 1.93841469 fraction A 0.0390867405\n",
      "step 70 loss 1.00211048 fisher_loss 0.156110063 triplet loss 0.846000433 l2_loss 0.201462463 fraction B 0.44483307 lossA 1.91260552 fraction A 0.0366443545\n",
      "step 71 loss 0.880042791 fisher_loss 0.156059802 triplet loss 0.723983 l2_loss 0.204382613 fraction B 0.542128861 lossA 1.90639639 fraction A 0.0361896157\n",
      "step 72 loss 0.811749816 fisher_loss 0.156716615 triplet loss 0.655033171 l2_loss 0.207867637 fraction B 0.513232291 lossA 1.93077815 fraction A 0.0394978337\n",
      "step 73 loss 0.943163455 fisher_loss 0.15964599 triplet loss 0.78351748 l2_loss 0.212345079 fraction B 0.458225191 lossA 1.96472454 fraction A 0.0451078229\n",
      "step 74 loss 1.02977562 fisher_loss 0.163558573 triplet loss 0.866217 l2_loss 0.217187807 fraction B 0.479133368 lossA 1.97866476 fraction A 0.0486987568\n",
      "step 75 loss 0.959411204 fisher_loss 0.165603355 triplet loss 0.793807864 l2_loss 0.221136 fraction B 0.43745923 lossA 1.98280752 fraction A 0.0508910269\n",
      "step 76 loss 0.960344076 fisher_loss 0.167521656 triplet loss 0.792822421 l2_loss 0.22476989 fraction B 0.402540267 lossA 1.97787344 fraction A 0.0516491681\n",
      "step 77 loss 0.908489347 fisher_loss 0.168694034 triplet loss 0.739795327 l2_loss 0.227998123 fraction B 0.444357455 lossA 1.96761942 fraction A 0.051172331\n",
      "step 78 loss 0.913540602 fisher_loss 0.16953367 triplet loss 0.744006932 l2_loss 0.231296584 fraction B 0.540983617 lossA 1.94593441 fraction A 0.0484412871\n",
      "step 79 loss 0.958783865 fisher_loss 0.16850026 triplet loss 0.79028362 l2_loss 0.233877286 fraction B 0.480832338 lossA 1.931458 fraction A 0.0463696755\n",
      "step 80 loss 0.965719223 fisher_loss 0.167481244 triplet loss 0.798238 l2_loss 0.236580789 fraction B 0.489672124 lossA 1.9151932 fraction A 0.0440959036\n",
      "step 81 loss 0.942659259 fisher_loss 0.166783974 triplet loss 0.77587527 l2_loss 0.23957476 fraction B 0.450238138 lossA 1.89293265 fraction A 0.0409408\n",
      "step 82 loss 0.927577853 fisher_loss 0.165840432 triplet loss 0.761737406 l2_loss 0.242657349 fraction B 0.455286622 lossA 1.90973473 fraction A 0.0426727161\n",
      "step 83 loss 0.914574385 fisher_loss 0.167955533 triplet loss 0.746618867 l2_loss 0.246751979 fraction B 0.459589779 lossA 1.93740165 fraction A 0.0456974544\n",
      "step 84 loss 0.886210084 fisher_loss 0.171453074 triplet loss 0.714757 l2_loss 0.251282692 fraction B 0.415307134 lossA 1.94454217 fraction A 0.0457026\n",
      "step 85 loss 1.0129689 fisher_loss 0.173264816 triplet loss 0.839704037 l2_loss 0.255196184 fraction B 0.392839789 lossA 1.94715965 fraction A 0.0448504239\n",
      "step 86 loss 1.04534745 fisher_loss 0.174316198 triplet loss 0.871031225 l2_loss 0.258688033 fraction B 0.41263783 lossA 1.9481988 fraction A 0.0436989404\n",
      "step 87 loss 0.921979129 fisher_loss 0.174703181 triplet loss 0.747275949 l2_loss 0.261850536 fraction B 0.401672 lossA 1.93623877 fraction A 0.0405455\n",
      "step 88 loss 0.973792732 fisher_loss 0.173241779 triplet loss 0.800550938 l2_loss 0.264504343 fraction B 0.385551065 lossA 1.91591394 fraction A 0.0370559171\n",
      "step 89 loss 0.952458918 fisher_loss 0.171767771 triplet loss 0.780691147 l2_loss 0.267261386 fraction B 0.41307056 lossA 1.91925812 fraction A 0.0368632488\n",
      "step 90 loss 0.829986215 fisher_loss 0.172387153 triplet loss 0.657599092 l2_loss 0.270854741 fraction B 0.482142866 lossA 1.9626323 fraction A 0.0410218313\n",
      "step 91 loss 0.861181855 fisher_loss 0.175899923 triplet loss 0.685281932 l2_loss 0.275922954 fraction B 0.392171055 lossA 1.98965442 fraction A 0.0460610203\n",
      "step 92 loss 1.00041878 fisher_loss 0.180342689 triplet loss 0.820076108 l2_loss 0.281422079 fraction B 0.497489065 lossA 2.01420951 fraction A 0.0537805818\n",
      "step 93 loss 0.99776262 fisher_loss 0.185738087 triplet loss 0.812024534 l2_loss 0.287108719 fraction B 0.425327599 lossA 2.0275619 fraction A 0.0584817082\n",
      "step 94 loss 1.00524759 fisher_loss 0.18987143 triplet loss 0.815376222 l2_loss 0.292444378 fraction B 0.413423836 lossA 2.01514649 fraction A 0.0571941026\n",
      "step 95 loss 1.00252306 fisher_loss 0.189540491 triplet loss 0.812982559 l2_loss 0.296209663 fraction B 0.430122 lossA 1.99397314 fraction A 0.0545129\n",
      "step 96 loss 0.964667559 fisher_loss 0.188076273 triplet loss 0.776591301 l2_loss 0.299606234 fraction B 0.504827559 lossA 1.95137954 fraction A 0.0483462028\n",
      "step 97 loss 0.958452404 fisher_loss 0.184217691 triplet loss 0.774234712 l2_loss 0.302020848 fraction B 0.480896086 lossA 1.89607286 fraction A 0.0412090309\n",
      "step 98 loss 1.06006646 fisher_loss 0.180274963 triplet loss 0.879791439 l2_loss 0.304350406 fraction B 0.484545499 lossA 1.84044182 fraction A 0.0359020084\n",
      "step 99 loss 0.942585349 fisher_loss 0.177443385 triplet loss 0.765141964 l2_loss 0.306994885 fraction B 0.444180578 lossA 1.82391846 fraction A 0.0347970687\n",
      "step 100 loss 0.948102534 fisher_loss 0.177527085 triplet loss 0.770575464 l2_loss 0.31076628 fraction B 0.472244501 lossA 1.83874047 fraction A 0.0358633921\n",
      "step 101 loss 0.967279375 fisher_loss 0.179031566 triplet loss 0.788247824 l2_loss 0.315508693 fraction B 0.436596692 lossA 1.87109303 fraction A 0.0387770757\n",
      "step 102 loss 1.05799067 fisher_loss 0.181835219 triplet loss 0.876155436 l2_loss 0.32073617 fraction B 0.499522418 lossA 1.9212836 fraction A 0.0449300036\n",
      "step 103 loss 0.965109289 fisher_loss 0.186799765 triplet loss 0.778309524 l2_loss 0.326635152 fraction B 0.416971594 lossA 1.9538306 fraction A 0.0509113111\n",
      "step 104 loss 0.961967647 fisher_loss 0.192011118 triplet loss 0.769956529 l2_loss 0.332290888 fraction B 0.429517061 lossA 1.97069454 fraction A 0.0535566919\n",
      "step 105 loss 0.948113799 fisher_loss 0.194630578 triplet loss 0.753483236 l2_loss 0.337113947 fraction B 0.411564887 lossA 1.98871255 fraction A 0.0566256233\n",
      "step 106 loss 1.03421175 fisher_loss 0.197389349 triplet loss 0.836822391 l2_loss 0.342268527 fraction B 0.361893237 lossA 1.99111187 fraction A 0.0562986769\n",
      "step 107 loss 1.03449392 fisher_loss 0.198099181 triplet loss 0.836394787 l2_loss 0.346871465 fraction B 0.480935425 lossA 1.97768724 fraction A 0.0530039333\n",
      "step 108 loss 0.947864234 fisher_loss 0.196913049 triplet loss 0.750951171 l2_loss 0.35083729 fraction B 0.336537689 lossA 1.96332312 fraction A 0.0499405451\n",
      "step 109 loss 0.738157809 fisher_loss 0.196062744 triplet loss 0.542095065 l2_loss 0.354900748 fraction B 0.287299842 lossA 1.97876358 fraction A 0.0514253117\n",
      "step 110 loss 0.929641068 fisher_loss 0.198250607 triplet loss 0.731390476 l2_loss 0.360369772 fraction B 0.398168355 lossA 1.99072289 fraction A 0.0523911901\n",
      "step 111 loss 0.917190313 fisher_loss 0.200339794 triplet loss 0.716850519 l2_loss 0.365944028 fraction B 0.473971426 lossA 1.99995446 fraction A 0.0513276197\n",
      "step 112 loss 0.959807277 fisher_loss 0.201321572 triplet loss 0.758485675 l2_loss 0.371306807 fraction B 0.406552166 lossA 2.0011642 fraction A 0.0493799634\n",
      "step 113 loss 0.998264372 fisher_loss 0.200816646 triplet loss 0.797447741 l2_loss 0.375742286 fraction B 0.396173656 lossA 2.00245404 fraction A 0.0483790934\n",
      "step 114 loss 0.979109168 fisher_loss 0.200862363 triplet loss 0.77824682 l2_loss 0.380197883 fraction B 0.398786783 lossA 2.00406265 fraction A 0.0489599593\n",
      "step 115 loss 0.956360102 fisher_loss 0.202052742 triplet loss 0.754307389 l2_loss 0.38494429 fraction B 0.389392018 lossA 1.99777377 fraction A 0.0490935482\n",
      "step 116 loss 0.876798213 fisher_loss 0.203100875 triplet loss 0.673697352 l2_loss 0.389510065 fraction B 0.291264802 lossA 1.98036027 fraction A 0.0473621413\n",
      "step 117 loss 0.919472337 fisher_loss 0.202765316 triplet loss 0.716707 l2_loss 0.393265784 fraction B 0.404781103 lossA 1.97786987 fraction A 0.0488063022\n",
      "step 118 loss 1.01610315 fisher_loss 0.204498261 triplet loss 0.811604857 l2_loss 0.397648364 fraction B 0.445819616 lossA 1.98161197 fraction A 0.0510165021\n",
      "step 119 loss 1.09359288 fisher_loss 0.206057236 triplet loss 0.887535691 l2_loss 0.401773363 fraction B 0.468080133 lossA 1.99060178 fraction A 0.0529904589\n",
      "step 120 loss 1.15008879 fisher_loss 0.20705992 triplet loss 0.943028867 l2_loss 0.40545395 fraction B 0.469983757 lossA 1.99300694 fraction A 0.0525697321\n",
      "step 121 loss 1.00722575 fisher_loss 0.206405029 triplet loss 0.800820708 l2_loss 0.408602804 fraction B 0.515898466 lossA 1.97641301 fraction A 0.0493014306\n",
      "step 122 loss 0.947014093 fisher_loss 0.203771412 triplet loss 0.743242681 l2_loss 0.410869271 fraction B 0.412038863 lossA 1.94710708 fraction A 0.0444166921\n",
      "step 123 loss 1.02777386 fisher_loss 0.20061098 triplet loss 0.827162921 l2_loss 0.413010538 fraction B 0.490597337 lossA 1.90778732 fraction A 0.0395389087\n",
      "step 124 loss 0.953552067 fisher_loss 0.198177621 triplet loss 0.755374432 l2_loss 0.415174216 fraction B 0.443776786 lossA 1.87179852 fraction A 0.0352811143\n",
      "step 125 loss 1.08709586 fisher_loss 0.196734443 triplet loss 0.890361428 l2_loss 0.417885035 fraction B 0.459087461 lossA 1.83284724 fraction A 0.0318517312\n",
      "step 126 loss 1.03973413 fisher_loss 0.195602655 triplet loss 0.84413147 l2_loss 0.420411438 fraction B 0.427693307 lossA 1.83042145 fraction A 0.0319473967\n",
      "step 127 loss 0.903213263 fisher_loss 0.196424067 triplet loss 0.706789196 l2_loss 0.423544139 fraction B 0.407661885 lossA 1.8480258 fraction A 0.0337538943\n",
      "step 128 loss 0.941237 fisher_loss 0.197863 triplet loss 0.743374 l2_loss 0.426702261 fraction B 0.442698956 lossA 1.88455617 fraction A 0.0376799628\n",
      "step 129 loss 1.17574203 fisher_loss 0.200045735 triplet loss 0.975696325 l2_loss 0.430136085 fraction B 0.452222168 lossA 1.92863417 fraction A 0.0426153019\n",
      "step 130 loss 1.02001464 fisher_loss 0.202957273 triplet loss 0.817057371 l2_loss 0.433320552 fraction B 0.427192539 lossA 1.95398092 fraction A 0.0465681739\n",
      "step 131 loss 0.989803433 fisher_loss 0.205906063 triplet loss 0.78389734 l2_loss 0.436686039 fraction B 0.378687084 lossA 1.96067762 fraction A 0.0478461497\n",
      "step 132 loss 1.0008955 fisher_loss 0.207990855 triplet loss 0.792904675 l2_loss 0.439925522 fraction B 0.449870348 lossA 1.94844949 fraction A 0.04693022\n",
      "step 133 loss 1.13531256 fisher_loss 0.208855703 triplet loss 0.926456809 l2_loss 0.442886621 fraction B 0.405293226 lossA 1.93587565 fraction A 0.0450489223\n",
      "step 134 loss 1.07942879 fisher_loss 0.208528936 triplet loss 0.870899856 l2_loss 0.445270598 fraction B 0.398041 lossA 1.92415726 fraction A 0.043682497\n",
      "step 135 loss 1.07560098 fisher_loss 0.208513096 triplet loss 0.867087841 l2_loss 0.447824746 fraction B 0.452193975 lossA 1.91074049 fraction A 0.041917257\n",
      "step 136 loss 1.04487741 fisher_loss 0.208394393 triplet loss 0.836483061 l2_loss 0.450403631 fraction B 0.414580166 lossA 1.89787984 fraction A 0.0397862755\n",
      "step 137 loss 0.943842769 fisher_loss 0.208209947 triplet loss 0.735632837 l2_loss 0.452875257 fraction B 0.372656107 lossA 1.89315379 fraction A 0.0382876322\n",
      "step 138 loss 1.07415223 fisher_loss 0.208804518 triplet loss 0.865347683 l2_loss 0.456109762 fraction B 0.444567025 lossA 1.89095807 fraction A 0.0380625464\n",
      "step 139 loss 1.02214038 fisher_loss 0.209755912 triplet loss 0.812384427 l2_loss 0.45947203 fraction B 0.389476836 lossA 1.89041555 fraction A 0.0383484885\n",
      "step 140 loss 1.16369736 fisher_loss 0.210831925 triplet loss 0.952865422 l2_loss 0.463036358 fraction B 0.420855612 lossA 1.8961755 fraction A 0.0398797654\n",
      "step 141 loss 0.944681883 fisher_loss 0.212193221 triplet loss 0.732488632 l2_loss 0.46647647 fraction B 0.413539767 lossA 1.92723382 fraction A 0.0456083119\n",
      "step 142 loss 1.04439878 fisher_loss 0.216030777 triplet loss 0.828368068 l2_loss 0.47090131 fraction B 0.402717054 lossA 1.94926572 fraction A 0.0516693816\n",
      "step 143 loss 1.04842818 fisher_loss 0.220203951 triplet loss 0.828224182 l2_loss 0.475235879 fraction B 0.458124489 lossA 1.94969177 fraction A 0.0539161637\n",
      "step 144 loss 0.95403856 fisher_loss 0.222052827 triplet loss 0.731985748 l2_loss 0.478643984 fraction B 0.290388018 lossA 1.9480716 fraction A 0.0536691621\n",
      "step 145 loss 0.841750503 fisher_loss 0.221924424 triplet loss 0.619826078 l2_loss 0.481653124 fraction B 0.269720197 lossA 1.95839405 fraction A 0.0544469\n",
      "step 146 loss 1.01964068 fisher_loss 0.222532496 triplet loss 0.797108233 l2_loss 0.485664189 fraction B 0.3823874 lossA 1.94596481 fraction A 0.0518255718\n",
      "step 147 loss 0.999452949 fisher_loss 0.221102625 triplet loss 0.778350294 l2_loss 0.488766283 fraction B 0.352227867 lossA 1.91945696 fraction A 0.0470038243\n",
      "step 148 loss 0.939941287 fisher_loss 0.218504831 triplet loss 0.721436441 l2_loss 0.491400629 fraction B 0.371422648 lossA 1.88385653 fraction A 0.0410059653\n",
      "step 149 loss 1.02939248 fisher_loss 0.21531041 triplet loss 0.814082086 l2_loss 0.493835777 fraction B 0.437165558 lossA 1.83467674 fraction A 0.0349672809\n",
      "step 150 loss 0.941604853 fisher_loss 0.212486699 triplet loss 0.729118168 l2_loss 0.496403873 fraction B 0.379296571 lossA 1.83356678 fraction A 0.0337531678\n",
      "step 151 loss 0.996631 fisher_loss 0.212322354 triplet loss 0.784308672 l2_loss 0.499961287 fraction B 0.482569814 lossA 1.86433017 fraction A 0.0359230898\n",
      "step 152 loss 0.987534881 fisher_loss 0.213927165 triplet loss 0.773607731 l2_loss 0.50445646 fraction B 0.456917584 lossA 1.91240931 fraction A 0.0403264277\n",
      "step 153 loss 0.999326766 fisher_loss 0.216608703 triplet loss 0.782718062 l2_loss 0.509350717 fraction B 0.422302485 lossA 1.95706904 fraction A 0.0460626148\n",
      "step 154 loss 1.109905 fisher_loss 0.220072448 triplet loss 0.889832497 l2_loss 0.514299631 fraction B 0.359796882 lossA 1.99065793 fraction A 0.0516912602\n",
      "step 155 loss 1.10913157 fisher_loss 0.223295882 triplet loss 0.885835648 l2_loss 0.518942118 fraction B 0.424698412 lossA 2.02456284 fraction A 0.0565547347\n",
      "step 156 loss 0.945467174 fisher_loss 0.225956187 triplet loss 0.719511 l2_loss 0.523124814 fraction B 0.332556158 lossA 2.03588128 fraction A 0.0571720414\n",
      "step 157 loss 1.00115395 fisher_loss 0.225921154 triplet loss 0.775232732 l2_loss 0.526378334 fraction B 0.44150123 lossA 2.03112268 fraction A 0.0559246056\n",
      "step 158 loss 1.00132012 fisher_loss 0.224861488 triplet loss 0.776458621 l2_loss 0.529260695 fraction B 0.413964212 lossA 2.00816584 fraction A 0.051845856\n",
      "step 159 loss 0.926427662 fisher_loss 0.221734509 triplet loss 0.704693139 l2_loss 0.531580269 fraction B 0.376744181 lossA 1.9692148 fraction A 0.0470604412\n",
      "step 160 loss 1.05788743 fisher_loss 0.218938857 triplet loss 0.838948607 l2_loss 0.533897638 fraction B 0.355406821 lossA 1.88105071 fraction A 0.0390798599\n",
      "step 161 loss 0.924539804 fisher_loss 0.215622991 triplet loss 0.708916783 l2_loss 0.535991609 fraction B 0.412725657 lossA 1.80454719 fraction A 0.0331975855\n",
      "step 162 loss 0.985732853 fisher_loss 0.213805914 triplet loss 0.771926939 l2_loss 0.539115369 fraction B 0.421010107 lossA 1.76410806 fraction A 0.0303439982\n",
      "step 163 loss 1.01507056 fisher_loss 0.213939458 triplet loss 0.80113107 l2_loss 0.54310137 fraction B 0.507594764 lossA 1.78666902 fraction A 0.0318951979\n",
      "step 164 loss 0.825271964 fisher_loss 0.216456801 triplet loss 0.608815134 l2_loss 0.548087955 fraction B 0.330789328 lossA 1.83816743 fraction A 0.0350033939\n",
      "step 165 loss 0.889364 fisher_loss 0.220320508 triplet loss 0.669043481 l2_loss 0.553752601 fraction B 0.391305208 lossA 1.90735853 fraction A 0.0403681546\n",
      "step 166 loss 0.988715 fisher_loss 0.226083741 triplet loss 0.762631238 l2_loss 0.559877455 fraction B 0.392462194 lossA 1.95616 fraction A 0.0464797206\n",
      "step 167 loss 0.990600467 fisher_loss 0.232645571 triplet loss 0.757954895 l2_loss 0.566173434 fraction B 0.38444972 lossA 1.97631705 fraction A 0.0504677296\n",
      "step 168 loss 1.0630151 fisher_loss 0.237674981 triplet loss 0.825340152 l2_loss 0.572047114 fraction B 0.331007123 lossA 1.98589146 fraction A 0.0541787781\n",
      "step 169 loss 1.00718403 fisher_loss 0.241924778 triplet loss 0.765259266 l2_loss 0.577354 fraction B 0.373330086 lossA 1.96254146 fraction A 0.0531617552\n",
      "step 170 loss 1.12195683 fisher_loss 0.243460611 triplet loss 0.87849623 l2_loss 0.581537127 fraction B 0.447706223 lossA 1.91375864 fraction A 0.0489282273\n",
      "step 171 loss 1.00826871 fisher_loss 0.242306665 triplet loss 0.765962064 l2_loss 0.584302664 fraction B 0.390779257 lossA 1.87559497 fraction A 0.0471509248\n",
      "step 172 loss 1.19397891 fisher_loss 0.242316917 triplet loss 0.951661944 l2_loss 0.587710321 fraction B 0.416868985 lossA 1.83985937 fraction A 0.0446011759\n",
      "step 173 loss 0.871428967 fisher_loss 0.240754813 triplet loss 0.630674183 l2_loss 0.590325713 fraction B 0.350495398 lossA 1.8063339 fraction A 0.0420979708\n",
      "step 174 loss 1.12646091 fisher_loss 0.239135876 triplet loss 0.887325048 l2_loss 0.593118787 fraction B 0.458689898 lossA 1.78671145 fraction A 0.04077233\n",
      "step 175 loss 1.12081707 fisher_loss 0.237859532 triplet loss 0.882957518 l2_loss 0.59614 fraction B 0.390121788 lossA 1.75711572 fraction A 0.0381062292\n",
      "step 176 loss 0.967453599 fisher_loss 0.235319585 triplet loss 0.732134044 l2_loss 0.598897874 fraction B 0.288905025 lossA 1.76384866 fraction A 0.0376657248\n",
      "step 177 loss 1.01216483 fisher_loss 0.233902603 triplet loss 0.778262258 l2_loss 0.602448046 fraction B 0.399294317 lossA 1.82322347 fraction A 0.0416332334\n",
      "step 178 loss 0.865375042 fisher_loss 0.235293269 triplet loss 0.630081773 l2_loss 0.607664466 fraction B 0.39674449 lossA 1.86536443 fraction A 0.0445375666\n",
      "step 179 loss 1.32550263 fisher_loss 0.237850413 triplet loss 1.08765221 l2_loss 0.614012122 fraction B 0.440864176 lossA 1.90477192 fraction A 0.0470529795\n",
      "step 180 loss 1.14338207 fisher_loss 0.239395142 triplet loss 0.903986871 l2_loss 0.618888736 fraction B 0.407863855 lossA 1.92628479 fraction A 0.0468992889\n",
      "step 181 loss 1.04315341 fisher_loss 0.238914922 triplet loss 0.804238439 l2_loss 0.622584105 fraction B 0.359618276 lossA 1.93539 fraction A 0.0451854467\n",
      "step 182 loss 1.27193153 fisher_loss 0.237512067 triplet loss 1.03441942 l2_loss 0.626129 fraction B 0.421556622 lossA 1.94559884 fraction A 0.0437256\n",
      "step 183 loss 0.919749856 fisher_loss 0.235787421 triplet loss 0.683962405 l2_loss 0.629076421 fraction B 0.418006569 lossA 1.98612928 fraction A 0.0479077622\n",
      "step 184 loss 1.04257417 fisher_loss 0.23775202 triplet loss 0.804822087 l2_loss 0.633616865 fraction B 0.41288358 lossA 2.00892901 fraction A 0.0508437939\n",
      "step 185 loss 0.963026524 fisher_loss 0.23899433 triplet loss 0.724032164 l2_loss 0.637884855 fraction B 0.288555 lossA 2.02165985 fraction A 0.0516052283\n",
      "step 186 loss 1.20125926 fisher_loss 0.239183977 triplet loss 0.962075233 l2_loss 0.641423643 fraction B 0.379617631 lossA 2.01658463 fraction A 0.0505520552\n",
      "step 187 loss 0.943837941 fisher_loss 0.23822546 triplet loss 0.705612481 l2_loss 0.64454031 fraction B 0.409999162 lossA 2.01912618 fraction A 0.0528717563\n",
      "step 188 loss 1.08550584 fisher_loss 0.238982871 triplet loss 0.846523 l2_loss 0.647867084 fraction B 0.442010969 lossA 2.02043962 fraction A 0.0544030331\n",
      "step 189 loss 1.0699544 fisher_loss 0.239146799 triplet loss 0.830807567 l2_loss 0.651035309 fraction B 0.40827474 lossA 2.00367093 fraction A 0.0525572\n",
      "step 190 loss 1.01743543 fisher_loss 0.23777172 triplet loss 0.779663742 l2_loss 0.653601825 fraction B 0.403290182 lossA 1.97334063 fraction A 0.0489954576\n",
      "step 191 loss 1.055493 fisher_loss 0.235168278 triplet loss 0.820324719 l2_loss 0.65546459 fraction B 0.392305106 lossA 1.93096912 fraction A 0.0439277962\n",
      "step 192 loss 1.05030918 fisher_loss 0.232339039 triplet loss 0.817970157 l2_loss 0.657176733 fraction B 0.426424772 lossA 1.88423228 fraction A 0.0397565737\n",
      "step 193 loss 0.933638811 fisher_loss 0.230411932 triplet loss 0.703226864 l2_loss 0.658863068 fraction B 0.374771029 lossA 1.87898314 fraction A 0.039195735\n",
      "step 194 loss 1.05825388 fisher_loss 0.230523333 triplet loss 0.827730536 l2_loss 0.661877275 fraction B 0.35117504 lossA 1.88289523 fraction A 0.0397013314\n",
      "step 195 loss 0.970162272 fisher_loss 0.231294096 triplet loss 0.738868177 l2_loss 0.665293515 fraction B 0.405287057 lossA 1.90202594 fraction A 0.041743204\n",
      "step 196 loss 0.955281913 fisher_loss 0.233132124 triplet loss 0.722149789 l2_loss 0.669255435 fraction B 0.37150839 lossA 1.91354609 fraction A 0.0431109\n",
      "step 197 loss 1.19641554 fisher_loss 0.235089645 triplet loss 0.961325943 l2_loss 0.673402309 fraction B 0.437274843 lossA 1.91096985 fraction A 0.042998068\n",
      "step 198 loss 0.980485082 fisher_loss 0.236333922 triplet loss 0.744151175 l2_loss 0.677131116 fraction B 0.428649455 lossA 1.92963398 fraction A 0.0446717\n",
      "step 199 loss 1.08590317 fisher_loss 0.238829657 triplet loss 0.847073495 l2_loss 0.681286275 fraction B 0.454427093 lossA 1.94881618 fraction A 0.0462374613\n",
      "step 200 loss 1.15345263 fisher_loss 0.240990028 triplet loss 0.912462592 l2_loss 0.685252249 fraction B 0.418467045 lossA 1.95397806 fraction A 0.0466292836\n",
      "step 201 loss 1.10020328 fisher_loss 0.242198944 triplet loss 0.858004332 l2_loss 0.688687325 fraction B 0.397563547 lossA 1.95776093 fraction A 0.0474554151\n",
      "step 202 loss 0.82814467 fisher_loss 0.243476659 triplet loss 0.584668 l2_loss 0.692082882 fraction B 0.339396507 lossA 1.97426248 fraction A 0.0519808605\n",
      "step 203 loss 1.36288524 fisher_loss 0.247238293 triplet loss 1.11564696 l2_loss 0.696989119 fraction B 0.445789337 lossA 1.97411025 fraction A 0.0538615398\n",
      "step 204 loss 0.921575785 fisher_loss 0.249272361 triplet loss 0.672303438 l2_loss 0.700490415 fraction B 0.332073092 lossA 1.96134007 fraction A 0.0531028919\n",
      "step 205 loss 0.922878444 fisher_loss 0.24918665 triplet loss 0.673691809 l2_loss 0.703454375 fraction B 0.316967338 lossA 1.94182897 fraction A 0.0513876751\n",
      "step 206 loss 0.846222222 fisher_loss 0.248162448 triplet loss 0.598059773 l2_loss 0.706297755 fraction B 0.31147182 lossA 1.90311468 fraction A 0.048104126\n",
      "step 207 loss 0.980049 fisher_loss 0.246096343 triplet loss 0.733952641 l2_loss 0.708704233 fraction B 0.301616818 lossA 1.85598111 fraction A 0.0440974608\n",
      "step 208 loss 1.01440287 fisher_loss 0.244062364 triplet loss 0.770340443 l2_loss 0.711271524 fraction B 0.293027073 lossA 1.83316505 fraction A 0.0423647128\n",
      "step 209 loss 0.87444365 fisher_loss 0.243492454 triplet loss 0.630951166 l2_loss 0.714630783 fraction B 0.288389891 lossA 1.83133459 fraction A 0.0426555835\n",
      "step 210 loss 1.07969189 fisher_loss 0.244098216 triplet loss 0.835593641 l2_loss 0.718776524 fraction B 0.373586774 lossA 1.84486592 fraction A 0.0441458188\n",
      "step 211 loss 0.888702452 fisher_loss 0.245226696 triplet loss 0.643475771 l2_loss 0.723225594 fraction B 0.327442408 lossA 1.87805176 fraction A 0.0472322814\n",
      "step 212 loss 1.09381247 fisher_loss 0.247129008 triplet loss 0.846683502 l2_loss 0.728298187 fraction B 0.422013044 lossA 1.8908205 fraction A 0.0480259955\n",
      "step 213 loss 1.03712249 fisher_loss 0.247402757 triplet loss 0.78971976 l2_loss 0.732429922 fraction B 0.410285234 lossA 1.89168024 fraction A 0.0473039299\n",
      "step 214 loss 0.945723057 fisher_loss 0.246790677 triplet loss 0.698932409 l2_loss 0.735922098 fraction B 0.279779851 lossA 1.92226148 fraction A 0.0488428138\n",
      "step 215 loss 0.939400434 fisher_loss 0.24802509 triplet loss 0.691375375 l2_loss 0.740581036 fraction B 0.410582125 lossA 1.95609701 fraction A 0.0504453443\n",
      "step 216 loss 1.04091823 fisher_loss 0.250165731 triplet loss 0.79075253 l2_loss 0.746096849 fraction B 0.426735371 lossA 1.96788716 fraction A 0.0492332242\n",
      "step 217 loss 0.900067508 fisher_loss 0.249922678 triplet loss 0.650144815 l2_loss 0.750301957 fraction B 0.380817622 lossA 1.95707238 fraction A 0.0451068804\n",
      "step 218 loss 0.959025383 fisher_loss 0.248169705 triplet loss 0.710855663 l2_loss 0.75388068 fraction B 0.360959768 lossA 1.94163167 fraction A 0.041653879\n",
      "step 219 loss 1.07367468 fisher_loss 0.246989742 triplet loss 0.826684892 l2_loss 0.757900774 fraction B 0.496844649 lossA 1.91117835 fraction A 0.0387273394\n",
      "step 220 loss 0.959535539 fisher_loss 0.245613 triplet loss 0.71392256 l2_loss 0.761691093 fraction B 0.354212523 lossA 1.89982927 fraction A 0.0395642668\n",
      "step 221 loss 1.02041638 fisher_loss 0.246538833 triplet loss 0.773877501 l2_loss 0.766366124 fraction B 0.35891819 lossA 1.87775958 fraction A 0.0405037\n",
      "step 222 loss 1.07425904 fisher_loss 0.247457564 triplet loss 0.826801479 l2_loss 0.77036345 fraction B 0.466479957 lossA 1.88195848 fraction A 0.043335408\n",
      "step 223 loss 1.08686674 fisher_loss 0.249325961 triplet loss 0.837540746 l2_loss 0.774390578 fraction B 0.383873254 lossA 1.90414512 fraction A 0.046614\n",
      "step 224 loss 1.07548404 fisher_loss 0.251857787 triplet loss 0.82362622 l2_loss 0.77861917 fraction B 0.443337411 lossA 1.92023265 fraction A 0.0485060178\n",
      "step 225 loss 0.978888869 fisher_loss 0.253887922 triplet loss 0.725001 l2_loss 0.782608 fraction B 0.354991615 lossA 1.94359922 fraction A 0.0511353128\n",
      "step 226 loss 1.00574851 fisher_loss 0.25639993 triplet loss 0.749348581 l2_loss 0.787226796 fraction B 0.348056018 lossA 1.96905017 fraction A 0.0540426187\n",
      "step 227 loss 1.08355296 fisher_loss 0.258964777 triplet loss 0.82458818 l2_loss 0.792124033 fraction B 0.4484137 lossA 1.9463352 fraction A 0.0519443117\n",
      "step 228 loss 1.13891673 fisher_loss 0.258825719 triplet loss 0.880091 l2_loss 0.796068311 fraction B 0.376927614 lossA 1.92451763 fraction A 0.0493289232\n",
      "step 229 loss 1.27267039 fisher_loss 0.258248657 triplet loss 1.0144217 l2_loss 0.79944551 fraction B 0.398369789 lossA 1.92954576 fraction A 0.0488462932\n",
      "step 230 loss 1.06767309 fisher_loss 0.258586973 triplet loss 0.809086084 l2_loss 0.802466929 fraction B 0.313915312 lossA 1.94642985 fraction A 0.0488866456\n",
      "step 231 loss 1.06448042 fisher_loss 0.259323806 triplet loss 0.805156648 l2_loss 0.805419564 fraction B 0.380431563 lossA 1.96908617 fraction A 0.0499090664\n",
      "step 232 loss 1.09155858 fisher_loss 0.260554165 triplet loss 0.831004381 l2_loss 0.808534205 fraction B 0.427776814 lossA 1.98245037 fraction A 0.0505506806\n",
      "step 233 loss 1.00402093 fisher_loss 0.261208802 triplet loss 0.742812157 l2_loss 0.811149776 fraction B 0.346515983 lossA 1.98417425 fraction A 0.0500010736\n",
      "step 234 loss 1.23368144 fisher_loss 0.260381937 triplet loss 0.973299503 l2_loss 0.813321948 fraction B 0.362498939 lossA 1.96639943 fraction A 0.0475177169\n",
      "step 235 loss 1.0187366 fisher_loss 0.25836277 triplet loss 0.76037389 l2_loss 0.814884126 fraction B 0.32197696 lossA 1.91982651 fraction A 0.0433321483\n",
      "step 236 loss 1.12459147 fisher_loss 0.25511834 triplet loss 0.869473159 l2_loss 0.816253483 fraction B 0.436419547 lossA 1.8689543 fraction A 0.0399563387\n",
      "step 237 loss 1.07372582 fisher_loss 0.252272308 triplet loss 0.821453512 l2_loss 0.817547321 fraction B 0.422228426 lossA 1.81691718 fraction A 0.0366533734\n",
      "step 238 loss 1.17611063 fisher_loss 0.250207692 triplet loss 0.925902963 l2_loss 0.819314897 fraction B 0.457769573 lossA 1.81506681 fraction A 0.0375821255\n",
      "step 239 loss 1.04108512 fisher_loss 0.250299931 triplet loss 0.790785193 l2_loss 0.822076619 fraction B 0.404396296 lossA 1.83910489 fraction A 0.0405314118\n",
      "step 240 loss 1.19641221 fisher_loss 0.251137793 triplet loss 0.945274413 l2_loss 0.825148642 fraction B 0.444275767 lossA 1.86858439 fraction A 0.0438308641\n",
      "step 241 loss 1.07446718 fisher_loss 0.252114832 triplet loss 0.82235235 l2_loss 0.827977896 fraction B 0.352200985 lossA 1.90916407 fraction A 0.0479415953\n",
      "step 242 loss 1.07455826 fisher_loss 0.253785223 triplet loss 0.820773065 l2_loss 0.831472754 fraction B 0.355801731 lossA 1.93050766 fraction A 0.0499784686\n",
      "step 243 loss 1.12585545 fisher_loss 0.254421711 triplet loss 0.871433735 l2_loss 0.8346892 fraction B 0.365166605 lossA 1.92229927 fraction A 0.0493112095\n",
      "step 244 loss 1.16067994 fisher_loss 0.253749549 triplet loss 0.906930387 l2_loss 0.836917818 fraction B 0.509586632 lossA 1.88738143 fraction A 0.0462238416\n",
      "step 245 loss 1.00916457 fisher_loss 0.252263308 triplet loss 0.756901324 l2_loss 0.838814616 fraction B 0.362870097 lossA 1.85733056 fraction A 0.0430708751\n",
      "step 246 loss 0.947734594 fisher_loss 0.25090012 triplet loss 0.696834505 l2_loss 0.84089762 fraction B 0.351802379 lossA 1.8485018 fraction A 0.0413551889\n",
      "step 247 loss 1.05623746 fisher_loss 0.250255585 triplet loss 0.805981934 l2_loss 0.843766689 fraction B 0.35725981 lossA 1.85670066 fraction A 0.0411665402\n",
      "step 248 loss 1.02851987 fisher_loss 0.249751061 triplet loss 0.778768837 l2_loss 0.846455693 fraction B 0.335060894 lossA 1.87584162 fraction A 0.0426109917\n",
      "step 249 loss 0.847912192 fisher_loss 0.250359565 triplet loss 0.597552598 l2_loss 0.849437058 fraction B 0.293662518 lossA 1.90684354 fraction A 0.0463581197\n",
      "step 250 loss 0.961844563 fisher_loss 0.252941251 triplet loss 0.708903313 l2_loss 0.854032636 fraction B 0.348732412 lossA 1.93445694 fraction A 0.0503479391\n",
      "step 251 loss 1.07392669 fisher_loss 0.256312519 triplet loss 0.817614198 l2_loss 0.859060049 fraction B 0.402740419 lossA 1.94300759 fraction A 0.0527426228\n",
      "step 252 loss 0.987991 fisher_loss 0.258869797 triplet loss 0.729121149 l2_loss 0.863562524 fraction B 0.283909231 lossA 1.92600441 fraction A 0.0512105823\n",
      "step 253 loss 1.00797558 fisher_loss 0.258884311 triplet loss 0.749091208 l2_loss 0.867173672 fraction B 0.321625084 lossA 1.90292454 fraction A 0.048491925\n",
      "step 254 loss 1.10080588 fisher_loss 0.258616537 triplet loss 0.842189372 l2_loss 0.871115625 fraction B 0.304850131 lossA 1.88194263 fraction A 0.0454653054\n",
      "step 255 loss 0.927542686 fisher_loss 0.25805819 triplet loss 0.669484496 l2_loss 0.874767125 fraction B 0.314059615 lossA 1.88617396 fraction A 0.0464707352\n",
      "step 256 loss 1.05356109 fisher_loss 0.260227 triplet loss 0.793334126 l2_loss 0.879917502 fraction B 0.390710503 lossA 1.89106119 fraction A 0.0466303341\n",
      "step 257 loss 1.09330845 fisher_loss 0.261824638 triplet loss 0.831483841 l2_loss 0.885048 fraction B 0.356094807 lossA 1.87899899 fraction A 0.0452490896\n",
      "step 258 loss 1.01079726 fisher_loss 0.26199019 triplet loss 0.748807073 l2_loss 0.889337182 fraction B 0.329697698 lossA 1.87340117 fraction A 0.0443074107\n",
      "step 259 loss 1.03212261 fisher_loss 0.262062699 triplet loss 0.770059943 l2_loss 0.893918693 fraction B 0.377842367 lossA 1.8427856 fraction A 0.0408564024\n",
      "step 260 loss 1.07150173 fisher_loss 0.261511743 triplet loss 0.809990048 l2_loss 0.898330569 fraction B 0.400020421 lossA 1.81103194 fraction A 0.0375297107\n",
      "step 261 loss 1.22128904 fisher_loss 0.261022329 triplet loss 0.960266709 l2_loss 0.902336538 fraction B 0.45081687 lossA 1.78213835 fraction A 0.0352034904\n",
      "step 262 loss 1.01628315 fisher_loss 0.260280281 triplet loss 0.756002843 l2_loss 0.905613124 fraction B 0.30924 lossA 1.7935853 fraction A 0.0360260718\n",
      "step 263 loss 1.15060043 fisher_loss 0.260879636 triplet loss 0.889720738 l2_loss 0.909424603 fraction B 0.432825655 lossA 1.81075203 fraction A 0.037296582\n",
      "step 264 loss 1.15246809 fisher_loss 0.261643529 triplet loss 0.890824556 l2_loss 0.913153291 fraction B 0.382314533 lossA 1.8680625 fraction A 0.0428305\n",
      "step 265 loss 1.02427173 fisher_loss 0.264287472 triplet loss 0.759984195 l2_loss 0.91746664 fraction B 0.358311474 lossA 1.89339745 fraction A 0.0459085256\n",
      "step 266 loss 0.964049101 fisher_loss 0.266051918 triplet loss 0.697997153 l2_loss 0.921343327 fraction B 0.327280343 lossA 1.89676261 fraction A 0.046336133\n",
      "step 267 loss 1.23118162 fisher_loss 0.266156822 triplet loss 0.965024769 l2_loss 0.924620092 fraction B 0.43366763 lossA 1.90694416 fraction A 0.047777\n",
      "step 268 loss 1.03259599 fisher_loss 0.266734779 triplet loss 0.765861213 l2_loss 0.927725255 fraction B 0.394236296 lossA 1.88801289 fraction A 0.0460322239\n",
      "step 269 loss 1.20894361 fisher_loss 0.265796244 triplet loss 0.943147361 l2_loss 0.930462122 fraction B 0.357088357 lossA 1.84528136 fraction A 0.0420079194\n",
      "step 270 loss 1.07844055 fisher_loss 0.263264269 triplet loss 0.815176249 l2_loss 0.932112098 fraction B 0.441396773 lossA 1.81567633 fraction A 0.0398334377\n",
      "step 271 loss 0.975985289 fisher_loss 0.262271166 triplet loss 0.713714123 l2_loss 0.934669197 fraction B 0.269589782 lossA 1.81433582 fraction A 0.0399032\n",
      "step 272 loss 1.02802861 fisher_loss 0.262600452 triplet loss 0.765428126 l2_loss 0.937991 fraction B 0.282680333 lossA 1.821962 fraction A 0.0404359661\n",
      "step 273 loss 0.94090569 fisher_loss 0.263174444 triplet loss 0.677731276 l2_loss 0.941967666 fraction B 0.299447715 lossA 1.86691248 fraction A 0.0449214913\n",
      "step 274 loss 1.01532435 fisher_loss 0.265741527 triplet loss 0.749582767 l2_loss 0.946897149 fraction B 0.387278408 lossA 1.92028332 fraction A 0.0508727\n",
      "step 275 loss 1.06577873 fisher_loss 0.269337684 triplet loss 0.796441 l2_loss 0.952383518 fraction B 0.273688346 lossA 1.96181738 fraction A 0.0562764704\n",
      "step 276 loss 1.11775529 fisher_loss 0.273228407 triplet loss 0.844526887 l2_loss 0.957541227 fraction B 0.327574372 lossA 1.96569169 fraction A 0.0569916889\n",
      "step 277 loss 0.984635115 fisher_loss 0.274895102 triplet loss 0.709740043 l2_loss 0.961904883 fraction B 0.420495063 lossA 1.95521009 fraction A 0.0536679663\n",
      "step 278 loss 1.13800859 fisher_loss 0.273902029 triplet loss 0.864106536 l2_loss 0.965347409 fraction B 0.34318009 lossA 1.94036245 fraction A 0.0490617454\n",
      "step 279 loss 1.02592683 fisher_loss 0.271253973 triplet loss 0.754672885 l2_loss 0.967808783 fraction B 0.298850566 lossA 1.92401958 fraction A 0.0447940975\n",
      "step 280 loss 1.14875388 fisher_loss 0.268394589 triplet loss 0.880359352 l2_loss 0.969874084 fraction B 0.369303733 lossA 1.89259124 fraction A 0.0414682403\n",
      "step 281 loss 0.967308342 fisher_loss 0.265476942 triplet loss 0.7018314 l2_loss 0.971342921 fraction B 0.373412192 lossA 1.84519815 fraction A 0.03823312\n",
      "step 282 loss 0.993971527 fisher_loss 0.262756407 triplet loss 0.731215119 l2_loss 0.972869098 fraction B 0.336297482 lossA 1.82504916 fraction A 0.0385253616\n",
      "step 283 loss 1.08193362 fisher_loss 0.262395084 triplet loss 0.819538534 l2_loss 0.975616217 fraction B 0.31515 lossA 1.82149482 fraction A 0.0400605537\n",
      "step 284 loss 1.05365849 fisher_loss 0.2628856 triplet loss 0.790772855 l2_loss 0.978266 fraction B 0.375671685 lossA 1.84604108 fraction A 0.0433306657\n",
      "step 285 loss 1.13452709 fisher_loss 0.263981849 triplet loss 0.870545208 l2_loss 0.981241405 fraction B 0.374284029 lossA 1.90537632 fraction A 0.0499651395\n",
      "step 286 loss 1.05235982 fisher_loss 0.266992956 triplet loss 0.785366833 l2_loss 0.984809101 fraction B 0.323793441 lossA 1.94054544 fraction A 0.0541324876\n",
      "step 287 loss 1.07487416 fisher_loss 0.269056976 triplet loss 0.805817246 l2_loss 0.988261282 fraction B 0.364150286 lossA 1.96262181 fraction A 0.0563584082\n",
      "step 288 loss 1.01638472 fisher_loss 0.269548237 triplet loss 0.746836483 l2_loss 0.990730822 fraction B 0.356666654 lossA 1.97388101 fraction A 0.0575504974\n",
      "step 289 loss 1.09280312 fisher_loss 0.26933533 triplet loss 0.823467791 l2_loss 0.992818475 fraction B 0.398016334 lossA 1.94522154 fraction A 0.0539393127\n",
      "step 290 loss 0.943705082 fisher_loss 0.266607553 triplet loss 0.677097559 l2_loss 0.993845224 fraction B 0.340241343 lossA 1.90310025 fraction A 0.0485342\n",
      "step 291 loss 1.02270496 fisher_loss 0.263796836 triplet loss 0.758908153 l2_loss 0.995239556 fraction B 0.328572 lossA 1.89215064 fraction A 0.0461294465\n",
      "step 292 loss 1.11644256 fisher_loss 0.262661457 triplet loss 0.853781104 l2_loss 0.997705162 fraction B 0.373084962 lossA 1.89221096 fraction A 0.0455657504\n",
      "step 293 loss 0.882553935 fisher_loss 0.262386918 triplet loss 0.620167 l2_loss 1.00040948 fraction B 0.26435113 lossA 1.93616629 fraction A 0.0502912886\n",
      "step 294 loss 0.96067977 fisher_loss 0.265516967 triplet loss 0.695162773 l2_loss 1.00516975 fraction B 0.354025126 lossA 1.95870936 fraction A 0.0529128686\n",
      "step 295 loss 1.00051904 fisher_loss 0.267334551 triplet loss 0.733184457 l2_loss 1.0090363 fraction B 0.220260903 lossA 1.98763752 fraction A 0.0571475178\n",
      "step 296 loss 1.00230944 fisher_loss 0.270617366 triplet loss 0.731692076 l2_loss 1.01391268 fraction B 0.273412526 lossA 1.99195564 fraction A 0.0581846833\n",
      "step 297 loss 1.05761909 fisher_loss 0.271951437 triplet loss 0.785667717 l2_loss 1.01812661 fraction B 0.383398 lossA 1.97787559 fraction A 0.056378331\n",
      "step 298 loss 1.03647399 fisher_loss 0.271096885 triplet loss 0.765377045 l2_loss 1.02117932 fraction B 0.322226286 lossA 1.94498444 fraction A 0.0524265431\n",
      "step 299 loss 1.02043307 fisher_loss 0.269431353 triplet loss 0.751001716 l2_loss 1.02398443 fraction B 0.338195503 lossA 1.90212095 fraction A 0.0472013466\n",
      "step 300 loss 1.23460913 fisher_loss 0.267745405 triplet loss 0.966863692 l2_loss 1.02734721 fraction B 0.409361333 lossA 1.83966839 fraction A 0.0408463664\n",
      "step 301 loss 0.97503674 fisher_loss 0.265604943 triplet loss 0.709431767 l2_loss 1.02977288 fraction B 0.375180185 lossA 1.81004465 fraction A 0.0381717198\n",
      "step 302 loss 1.2106998 fisher_loss 0.265154451 triplet loss 0.945545375 l2_loss 1.03272712 fraction B 0.293929249 lossA 1.79432905 fraction A 0.0366271138\n",
      "step 303 loss 1.01589465 fisher_loss 0.264858693 triplet loss 0.751036 l2_loss 1.0352416 fraction B 0.378372 lossA 1.78497767 fraction A 0.035940513\n",
      "step 304 loss 1.0185492 fisher_loss 0.265177 triplet loss 0.753372133 l2_loss 1.03818822 fraction B 0.375810772 lossA 1.81356645 fraction A 0.0382764749\n",
      "step 305 loss 1.1136862 fisher_loss 0.266913682 triplet loss 0.846772492 l2_loss 1.04212046 fraction B 0.399740279 lossA 1.86152303 fraction A 0.0427133925\n",
      "step 306 loss 1.09608567 fisher_loss 0.269655526 triplet loss 0.826430142 l2_loss 1.04648542 fraction B 0.356132656 lossA 1.90268242 fraction A 0.0471506\n",
      "step 307 loss 1.11098492 fisher_loss 0.272800982 triplet loss 0.838183939 l2_loss 1.05122876 fraction B 0.407471418 lossA 1.92484236 fraction A 0.0491880551\n",
      "step 308 loss 1.01624298 fisher_loss 0.274527609 triplet loss 0.741715372 l2_loss 1.05529535 fraction B 0.353647172 lossA 1.94201016 fraction A 0.0498328917\n",
      "step 309 loss 1.05167222 fisher_loss 0.275495976 triplet loss 0.776176274 l2_loss 1.05959249 fraction B 0.3259 lossA 1.92776442 fraction A 0.0468441211\n",
      "step 310 loss 1.08513176 fisher_loss 0.274071306 triplet loss 0.811060429 l2_loss 1.06258035 fraction B 0.379509926 lossA 1.91636515 fraction A 0.0442747362\n",
      "step 311 loss 1.25277555 fisher_loss 0.273048878 triplet loss 0.979726672 l2_loss 1.06599331 fraction B 0.406429917 lossA 1.90134299 fraction A 0.0418658182\n",
      "step 312 loss 1.20512533 fisher_loss 0.271546841 triplet loss 0.933578551 l2_loss 1.06853306 fraction B 0.440604895 lossA 1.88964713 fraction A 0.0407895707\n",
      "step 313 loss 1.09859371 fisher_loss 0.270424485 triplet loss 0.828169167 l2_loss 1.0709182 fraction B 0.347554564 lossA 1.87443209 fraction A 0.0399250798\n",
      "step 314 loss 0.898467183 fisher_loss 0.269452304 triplet loss 0.629014909 l2_loss 1.07338941 fraction B 0.400577873 lossA 1.89891863 fraction A 0.0427410677\n",
      "step 315 loss 1.00903046 fisher_loss 0.270419985 triplet loss 0.738610446 l2_loss 1.07696974 fraction B 0.2964122 lossA 1.93647623 fraction A 0.0464094467\n",
      "step 316 loss 1.27982223 fisher_loss 0.272029042 triplet loss 1.00779319 l2_loss 1.08096337 fraction B 0.417668968 lossA 1.97376931 fraction A 0.0496571027\n",
      "step 317 loss 1.01964593 fisher_loss 0.27363354 triplet loss 0.746012449 l2_loss 1.08479404 fraction B 0.358454108 lossA 1.9886564 fraction A 0.0508465469\n",
      "step 318 loss 0.762758315 fisher_loss 0.274330974 triplet loss 0.488427341 l2_loss 1.08843112 fraction B 0.246873036 lossA 2.02293468 fraction A 0.054130964\n",
      "step 319 loss 0.917814076 fisher_loss 0.276733696 triplet loss 0.641080379 l2_loss 1.09375274 fraction B 0.362003386 lossA 2.03391671 fraction A 0.0558503829\n",
      "step 320 loss 0.950532258 fisher_loss 0.278245628 triplet loss 0.67228663 l2_loss 1.09857571 fraction B 0.346728027 lossA 2.00633121 fraction A 0.0534926131\n",
      "step 321 loss 1.19094896 fisher_loss 0.277466923 triplet loss 0.91348207 l2_loss 1.10263491 fraction B 0.333979338 lossA 1.972386 fraction A 0.0500850379\n",
      "step 322 loss 0.943367481 fisher_loss 0.275726676 triplet loss 0.667640805 l2_loss 1.10605848 fraction B 0.352206588 lossA 1.90915453 fraction A 0.0436617024\n",
      "step 323 loss 1.03934956 fisher_loss 0.273004055 triplet loss 0.766345441 l2_loss 1.10908186 fraction B 0.405579835 lossA 1.84026742 fraction A 0.0370702259\n",
      "step 324 loss 0.921780527 fisher_loss 0.270443201 triplet loss 0.651337326 l2_loss 1.11187112 fraction B 0.327620655 lossA 1.8314141 fraction A 0.0357038677\n",
      "step 325 loss 1.05767822 fisher_loss 0.27077803 triplet loss 0.786900222 l2_loss 1.11600983 fraction B 0.390579969 lossA 1.84447634 fraction A 0.0365595594\n",
      "step 326 loss 1.02634168 fisher_loss 0.272230208 triplet loss 0.754111409 l2_loss 1.12051082 fraction B 0.353457719 lossA 1.90071177 fraction A 0.0414356366\n",
      "step 327 loss 1.03463399 fisher_loss 0.27572909 triplet loss 0.758904874 l2_loss 1.12610865 fraction B 0.393214285 lossA 1.95130455 fraction A 0.0481715724\n",
      "step 328 loss 1.00587595 fisher_loss 0.280137122 triplet loss 0.725738823 l2_loss 1.13201594 fraction B 0.285951138 lossA 2.0024302 fraction A 0.0566446744\n",
      "step 329 loss 1.17776656 fisher_loss 0.285802841 triplet loss 0.891963661 l2_loss 1.13871133 fraction B 0.34605819 lossA 2.02137303 fraction A 0.0604656264\n",
      "step 330 loss 1.02070546 fisher_loss 0.28839463 triplet loss 0.732310832 l2_loss 1.14403522 fraction B 0.339657336 lossA 2.02860093 fraction A 0.061535608\n",
      "step 331 loss 1.07456756 fisher_loss 0.288603067 triplet loss 0.785964489 l2_loss 1.14850521 fraction B 0.335202694 lossA 2.00217605 fraction A 0.0578069165\n",
      "step 332 loss 1.00399804 fisher_loss 0.286132932 triplet loss 0.717865109 l2_loss 1.15177691 fraction B 0.370629221 lossA 1.9494549 fraction A 0.0514229201\n",
      "step 333 loss 1.05770695 fisher_loss 0.282604277 triplet loss 0.775102675 l2_loss 1.1548686 fraction B 0.311869651 lossA 1.89938247 fraction A 0.0459078737\n",
      "step 334 loss 1.11876798 fisher_loss 0.280048847 triplet loss 0.83871907 l2_loss 1.1582917 fraction B 0.387002021 lossA 1.83949959 fraction A 0.0401911\n",
      "step 335 loss 1.12915373 fisher_loss 0.277086049 triplet loss 0.852067709 l2_loss 1.16053891 fraction B 0.377299696 lossA 1.79994237 fraction A 0.0368046388\n",
      "step 336 loss 1.00335658 fisher_loss 0.274829894 triplet loss 0.728526711 l2_loss 1.1627214 fraction B 0.379610687 lossA 1.81216061 fraction A 0.0374452397\n",
      "step 337 loss 1.14493954 fisher_loss 0.274578601 triplet loss 0.870360911 l2_loss 1.16631591 fraction B 0.385648936 lossA 1.82301033 fraction A 0.0379659757\n",
      "step 338 loss 1.09004092 fisher_loss 0.274491608 triplet loss 0.815549314 l2_loss 1.16972589 fraction B 0.365134895 lossA 1.84925 fraction A 0.0395467691\n",
      "step 339 loss 1.03538215 fisher_loss 0.274673373 triplet loss 0.760708749 l2_loss 1.17367709 fraction B 0.386746734 lossA 1.87368011 fraction A 0.0411569066\n",
      "step 340 loss 0.980929613 fisher_loss 0.275920182 triplet loss 0.705009401 l2_loss 1.1787889 fraction B 0.312323302 lossA 1.91956174 fraction A 0.0450759083\n",
      "step 341 loss 1.28516078 fisher_loss 0.27854833 triplet loss 1.00661242 l2_loss 1.18512022 fraction B 0.39833957 lossA 1.96221 fraction A 0.0488838553\n",
      "step 342 loss 1.0181495 fisher_loss 0.280758232 triplet loss 0.737391293 l2_loss 1.1908865 fraction B 0.344323188 lossA 2.00575066 fraction A 0.0531114042\n",
      "step 343 loss 1.12255 fisher_loss 0.283572882 triplet loss 0.838977158 l2_loss 1.19680536 fraction B 0.323518783 lossA 2.02203584 fraction A 0.0543404408\n",
      "step 344 loss 1.1410265 fisher_loss 0.284759909 triplet loss 0.856266558 l2_loss 1.20188093 fraction B 0.359351188 lossA 2.01394868 fraction A 0.0530330576\n",
      "step 345 loss 1.16508341 fisher_loss 0.28400147 triplet loss 0.881081879 l2_loss 1.20570469 fraction B 0.335980892 lossA 2.00217748 fraction A 0.050691586\n",
      "step 346 loss 1.01679623 fisher_loss 0.282664895 triplet loss 0.734131336 l2_loss 1.20942473 fraction B 0.317957312 lossA 1.99539709 fraction A 0.0493577197\n",
      "step 347 loss 1.34188962 fisher_loss 0.281963497 triplet loss 1.05992615 l2_loss 1.21364224 fraction B 0.359406918 lossA 1.9819597 fraction A 0.0484320149\n",
      "step 348 loss 1.09305847 fisher_loss 0.281160146 triplet loss 0.811898291 l2_loss 1.21668828 fraction B 0.322803497 lossA 1.98707891 fraction A 0.0498244502\n",
      "step 349 loss 1.06401813 fisher_loss 0.281315655 triplet loss 0.782702506 l2_loss 1.22003949 fraction B 0.356442362 lossA 2.00961471 fraction A 0.0526788346\n",
      "step 350 loss 1.13613605 fisher_loss 0.282848448 triplet loss 0.853287578 l2_loss 1.22398818 fraction B 0.404911935 lossA 2.03556037 fraction A 0.0559239164\n",
      "step 351 loss 1.29498649 fisher_loss 0.284338653 triplet loss 1.01064777 l2_loss 1.22742474 fraction B 0.314674884 lossA 2.0024817 fraction A 0.0530213192\n",
      "step 352 loss 1.11395109 fisher_loss 0.282797068 triplet loss 0.831154 l2_loss 1.22935164 fraction B 0.342862666 lossA 1.96159148 fraction A 0.0495594107\n",
      "step 353 loss 0.991638541 fisher_loss 0.280727714 triplet loss 0.710910857 l2_loss 1.23050261 fraction B 0.32803756 lossA 1.90730178 fraction A 0.0452456139\n",
      "step 354 loss 1.20031905 fisher_loss 0.278424978 triplet loss 0.921894073 l2_loss 1.2319361 fraction B 0.383330524 lossA 1.85532212 fraction A 0.0410827212\n",
      "step 355 loss 0.900136709 fisher_loss 0.276181072 triplet loss 0.623955667 l2_loss 1.23284614 fraction B 0.354009837 lossA 1.86206853 fraction A 0.0417662784\n",
      "step 356 loss 1.05349433 fisher_loss 0.276589125 triplet loss 0.776905179 l2_loss 1.2356261 fraction B 0.392369777 lossA 1.89159381 fraction A 0.0441109\n",
      "step 357 loss 0.9226107 fisher_loss 0.278320312 triplet loss 0.644290388 l2_loss 1.23948979 fraction B 0.339685529 lossA 1.93865883 fraction A 0.0489085615\n",
      "step 358 loss 1.0409739 fisher_loss 0.281649232 triplet loss 0.75932467 l2_loss 1.24443305 fraction B 0.27765283 lossA 1.98573506 fraction A 0.054394342\n",
      "step 359 loss 1.04262125 fisher_loss 0.285582691 triplet loss 0.757038593 l2_loss 1.24998546 fraction B 0.396 lossA 2.00487638 fraction A 0.0565096\n",
      "step 360 loss 1.10554194 fisher_loss 0.28734833 triplet loss 0.818193555 l2_loss 1.25455153 fraction B 0.287531644 lossA 1.9936341 fraction A 0.0546368137\n",
      "step 361 loss 1.23993564 fisher_loss 0.286162943 triplet loss 0.953772724 l2_loss 1.2574904 fraction B 0.367075294 lossA 1.9536593 fraction A 0.049655579\n",
      "step 362 loss 1.05524087 fisher_loss 0.283357412 triplet loss 0.771883428 l2_loss 1.25995421 fraction B 0.314507395 lossA 1.90524399 fraction A 0.045207832\n",
      "step 363 loss 1.11446035 fisher_loss 0.280616671 triplet loss 0.833843708 l2_loss 1.26249373 fraction B 0.364137799 lossA 1.8561089 fraction A 0.0415826291\n",
      "step 364 loss 0.919456244 fisher_loss 0.278198689 triplet loss 0.641257524 l2_loss 1.26543283 fraction B 0.286697447 lossA 1.84998381 fraction A 0.0417787768\n",
      "step 365 loss 1.07216203 fisher_loss 0.278009266 triplet loss 0.794152796 l2_loss 1.26983762 fraction B 0.438692153 lossA 1.86757302 fraction A 0.0436888337\n",
      "step 366 loss 1.07814693 fisher_loss 0.278807402 triplet loss 0.799339592 l2_loss 1.27477694 fraction B 0.378673166 lossA 1.88008916 fraction A 0.0444650501\n",
      "step 367 loss 1.02617621 fisher_loss 0.279165864 triplet loss 0.74701035 l2_loss 1.27979684 fraction B 0.324475348 lossA 1.90339208 fraction A 0.0464708805\n",
      "step 368 loss 1.13370705 fisher_loss 0.280547291 triplet loss 0.853159726 l2_loss 1.28516948 fraction B 0.407001942 lossA 1.90669918 fraction A 0.0470973514\n",
      "step 369 loss 1.08461452 fisher_loss 0.281496823 triplet loss 0.803117752 l2_loss 1.29023731 fraction B 0.414257824 lossA 1.88257146 fraction A 0.0457749367\n",
      "step 370 loss 0.950385571 fisher_loss 0.282563895 triplet loss 0.667821705 l2_loss 1.29553604 fraction B 0.314644516 lossA 1.87473249 fraction A 0.0455904156\n",
      "step 371 loss 1.08633518 fisher_loss 0.284231901 triplet loss 0.802103281 l2_loss 1.30141664 fraction B 0.385179698 lossA 1.84657586 fraction A 0.0431545153\n",
      "step 372 loss 1.31416368 fisher_loss 0.284464568 triplet loss 1.02969909 l2_loss 1.30653834 fraction B 0.391165704 lossA 1.84904206 fraction A 0.0433720686\n",
      "step 373 loss 0.86776197 fisher_loss 0.285291642 triplet loss 0.582470298 l2_loss 1.31106913 fraction B 0.247790247 lossA 1.86118042 fraction A 0.0447650477\n",
      "step 374 loss 1.07318664 fisher_loss 0.286687732 triplet loss 0.786498845 l2_loss 1.31632161 fraction B 0.399076194 lossA 1.88095403 fraction A 0.0469749197\n",
      "step 375 loss 1.28527784 fisher_loss 0.287888885 triplet loss 0.997388959 l2_loss 1.32084394 fraction B 0.36012575 lossA 1.92210984 fraction A 0.051853139\n",
      "step 376 loss 1.01247513 fisher_loss 0.290188402 triplet loss 0.722286701 l2_loss 1.3251611 fraction B 0.325157553 lossA 1.95239472 fraction A 0.0554067269\n",
      "step 377 loss 0.951725781 fisher_loss 0.291938186 triplet loss 0.659787595 l2_loss 1.32959652 fraction B 0.275750428 lossA 1.96800399 fraction A 0.057317622\n",
      "step 378 loss 1.08557546 fisher_loss 0.292611331 triplet loss 0.792964101 l2_loss 1.33381808 fraction B 0.376212984 lossA 1.96482325 fraction A 0.056828253\n",
      "step 379 loss 1.02362132 fisher_loss 0.291906118 triplet loss 0.731715202 l2_loss 1.33737576 fraction B 0.303246886 lossA 1.95113206 fraction A 0.0548811741\n",
      "step 380 loss 1.01643968 fisher_loss 0.29068017 triplet loss 0.725759447 l2_loss 1.34073102 fraction B 0.305315107 lossA 1.93094444 fraction A 0.0523418561\n",
      "step 381 loss 1.26544595 fisher_loss 0.289224476 triplet loss 0.976221442 l2_loss 1.34406066 fraction B 0.304199129 lossA 1.903808 fraction A 0.0496691279\n",
      "step 382 loss 0.989978433 fisher_loss 0.287964195 triplet loss 0.702014208 l2_loss 1.34691346 fraction B 0.333875209 lossA 1.89862394 fraction A 0.0497646108\n",
      "step 383 loss 1.14400125 fisher_loss 0.288152546 triplet loss 0.85584873 l2_loss 1.35041797 fraction B 0.448214769 lossA 1.88464892 fraction A 0.0487806946\n",
      "step 384 loss 1.22084379 fisher_loss 0.287598491 triplet loss 0.933245242 l2_loss 1.35360289 fraction B 0.362725168 lossA 1.88964415 fraction A 0.0496998094\n",
      "step 385 loss 0.997554541 fisher_loss 0.287827343 triplet loss 0.709727228 l2_loss 1.35630655 fraction B 0.330159068 lossA 1.89356899 fraction A 0.0500767045\n",
      "step 386 loss 0.987210155 fisher_loss 0.287970394 triplet loss 0.699239731 l2_loss 1.35900319 fraction B 0.313443273 lossA 1.88683081 fraction A 0.0492612235\n",
      "step 387 loss 1.10838497 fisher_loss 0.287375897 triplet loss 0.821009099 l2_loss 1.3610363 fraction B 0.401319653 lossA 1.88628054 fraction A 0.0489350371\n",
      "step 388 loss 0.996402502 fisher_loss 0.286691129 triplet loss 0.709711373 l2_loss 1.36294878 fraction B 0.308442622 lossA 1.89201558 fraction A 0.0491979048\n",
      "step 389 loss 1.04398656 fisher_loss 0.286497802 triplet loss 0.757488787 l2_loss 1.36548471 fraction B 0.361461 lossA 1.89817154 fraction A 0.0494807325\n",
      "step 390 loss 1.23977923 fisher_loss 0.286372662 triplet loss 0.953406572 l2_loss 1.36825931 fraction B 0.357121974 lossA 1.8957299 fraction A 0.0490698218\n",
      "step 391 loss 0.9417243 fisher_loss 0.285601 triplet loss 0.656123281 l2_loss 1.37042665 fraction B 0.325811863 lossA 1.89369702 fraction A 0.0487579107\n",
      "step 392 loss 0.939574301 fisher_loss 0.285306394 triplet loss 0.654267907 l2_loss 1.37303174 fraction B 0.280097276 lossA 1.86161649 fraction A 0.0455222093\n",
      "step 393 loss 0.976067603 fisher_loss 0.284062028 triplet loss 0.692005575 l2_loss 1.37602413 fraction B 0.290482 lossA 1.86762834 fraction A 0.04567286\n",
      "step 394 loss 1.0909059 fisher_loss 0.284999073 triplet loss 0.805906892 l2_loss 1.38089681 fraction B 0.355178624 lossA 1.9165839 fraction A 0.0503100529\n",
      "step 395 loss 0.932873368 fisher_loss 0.287739038 triplet loss 0.64513433 l2_loss 1.38615012 fraction B 0.281253815 lossA 1.93105018 fraction A 0.0514072\n",
      "step 396 loss 1.07720387 fisher_loss 0.289005041 triplet loss 0.788198829 l2_loss 1.39092696 fraction B 0.294958234 lossA 1.94065464 fraction A 0.0522764362\n",
      "step 397 loss 1.12847233 fisher_loss 0.289585531 triplet loss 0.838886857 l2_loss 1.39514613 fraction B 0.281291127 lossA 1.92528 fraction A 0.050006\n",
      "step 398 loss 1.23430812 fisher_loss 0.288818747 triplet loss 0.945489407 l2_loss 1.39900386 fraction B 0.372245699 lossA 1.90665162 fraction A 0.047579512\n",
      "step 399 loss 1.04767668 fisher_loss 0.287651092 triplet loss 0.760025561 l2_loss 1.40247762 fraction B 0.38652721 lossA 1.90933156 fraction A 0.0475748032\n",
      "step 400 loss 1.14029312 fisher_loss 0.287480175 triplet loss 0.852812946 l2_loss 1.40629125 fraction B 0.38660714 lossA 1.9093163 fraction A 0.0477567129\n",
      "step 401 loss 1.15660787 fisher_loss 0.287825942 triplet loss 0.868781865 l2_loss 1.41030884 fraction B 0.33237654 lossA 1.89647436 fraction A 0.0461871848\n",
      "step 402 loss 1.02483225 fisher_loss 0.286838472 triplet loss 0.737993777 l2_loss 1.41360795 fraction B 0.335944 lossA 1.89296412 fraction A 0.0456421413\n",
      "step 403 loss 1.31685388 fisher_loss 0.286408633 triplet loss 1.03044522 l2_loss 1.41735566 fraction B 0.377867877 lossA 1.89479542 fraction A 0.0459871292\n",
      "step 404 loss 1.26671457 fisher_loss 0.286566228 triplet loss 0.980148375 l2_loss 1.4208678 fraction B 0.388280034 lossA 1.89856207 fraction A 0.0464776196\n",
      "step 405 loss 1.17743349 fisher_loss 0.286488891 triplet loss 0.8909446 l2_loss 1.42330706 fraction B 0.410752416 lossA 1.90208614 fraction A 0.0471997559\n",
      "step 406 loss 1.22110677 fisher_loss 0.286269844 triplet loss 0.934836924 l2_loss 1.4254725 fraction B 0.292884469 lossA 1.90618551 fraction A 0.0481687114\n",
      "step 407 loss 1.15242863 fisher_loss 0.286660463 triplet loss 0.865768135 l2_loss 1.42757428 fraction B 0.332992017 lossA 1.91345024 fraction A 0.0493046194\n",
      "step 408 loss 0.949219882 fisher_loss 0.286969602 triplet loss 0.66225028 l2_loss 1.42961466 fraction B 0.297017872 lossA 1.94023383 fraction A 0.0523998849\n",
      "step 409 loss 1.06001544 fisher_loss 0.28833428 triplet loss 0.77168119 l2_loss 1.43267155 fraction B 0.327413291 lossA 1.9568572 fraction A 0.0538525581\n",
      "step 410 loss 1.19697666 fisher_loss 0.288491368 triplet loss 0.908485353 l2_loss 1.43529367 fraction B 0.34069255 lossA 1.98026454 fraction A 0.0566530786\n",
      "step 411 loss 1.10805643 fisher_loss 0.288986027 triplet loss 0.819070399 l2_loss 1.43788302 fraction B 0.347592354 lossA 1.98240733 fraction A 0.0564881936\n",
      "step 412 loss 1.03776848 fisher_loss 0.287715 triplet loss 0.750053465 l2_loss 1.43994904 fraction B 0.332384318 lossA 1.99825895 fraction A 0.0578521602\n",
      "step 413 loss 1.14355981 fisher_loss 0.287053078 triplet loss 0.856506705 l2_loss 1.44239688 fraction B 0.404215515 lossA 1.98906934 fraction A 0.0563024059\n",
      "step 414 loss 1.2642138 fisher_loss 0.284832597 triplet loss 0.979381263 l2_loss 1.4437331 fraction B 0.420384139 lossA 1.97052765 fraction A 0.0536607243\n",
      "step 415 loss 0.961391807 fisher_loss 0.281803727 triplet loss 0.679588079 l2_loss 1.44401872 fraction B 0.248222396 lossA 1.96589446 fraction A 0.0522636846\n",
      "step 416 loss 1.07431436 fisher_loss 0.280055523 triplet loss 0.794258833 l2_loss 1.44564867 fraction B 0.396396399 lossA 1.9477303 fraction A 0.0497064739\n",
      "step 417 loss 1.15662813 fisher_loss 0.277962893 triplet loss 0.878665268 l2_loss 1.44704711 fraction B 0.373868912 lossA 1.91369057 fraction A 0.0457492545\n",
      "step 418 loss 1.01835942 fisher_loss 0.275351524 triplet loss 0.743007839 l2_loss 1.44818652 fraction B 0.348698437 lossA 1.89569163 fraction A 0.0433288887\n",
      "step 419 loss 1.1957283 fisher_loss 0.273771942 triplet loss 0.92195636 l2_loss 1.44964015 fraction B 0.351847082 lossA 1.89785564 fraction A 0.0433238894\n",
      "step 420 loss 0.943640888 fisher_loss 0.273032486 triplet loss 0.670608401 l2_loss 1.4512229 fraction B 0.257926911 lossA 1.93014681 fraction A 0.0461757407\n",
      "step 421 loss 1.04555309 fisher_loss 0.273615032 triplet loss 0.771938086 l2_loss 1.45336711 fraction B 0.32003811 lossA 1.98599887 fraction A 0.0516351126\n",
      "step 422 loss 0.901512384 fisher_loss 0.275962025 triplet loss 0.62555033 l2_loss 1.45667517 fraction B 0.28405869 lossA 2.02974081 fraction A 0.0568412915\n",
      "step 423 loss 1.11129284 fisher_loss 0.27875337 triplet loss 0.832539499 l2_loss 1.4604528 fraction B 0.338785231 lossA 2.03400874 fraction A 0.0578750521\n",
      "step 424 loss 1.13967609 fisher_loss 0.279653966 triplet loss 0.860022128 l2_loss 1.46342051 fraction B 0.386535108 lossA 2.00174499 fraction A 0.054747656\n",
      "step 425 loss 1.16223967 fisher_loss 0.278362483 triplet loss 0.883877218 l2_loss 1.46582162 fraction B 0.349947035 lossA 1.9460597 fraction A 0.0495369509\n",
      "step 426 loss 1.11967587 fisher_loss 0.276318908 triplet loss 0.843357 l2_loss 1.46732605 fraction B 0.367214143 lossA 1.88956714 fraction A 0.0445620529\n",
      "step 427 loss 1.08368087 fisher_loss 0.274584711 triplet loss 0.809096158 l2_loss 1.46930659 fraction B 0.333682626 lossA 1.85064495 fraction A 0.0415516607\n",
      "step 428 loss 1.06089461 fisher_loss 0.273643881 triplet loss 0.787250698 l2_loss 1.47148228 fraction B 0.379309624 lossA 1.81870306 fraction A 0.0396251567\n",
      "step 429 loss 1.00581276 fisher_loss 0.273177296 triplet loss 0.732635498 l2_loss 1.47435558 fraction B 0.37111026 lossA 1.80186 fraction A 0.0387830511\n",
      "step 430 loss 1.10821486 fisher_loss 0.273590863 triplet loss 0.834623933 l2_loss 1.47834277 fraction B 0.333333343 lossA 1.79805088 fraction A 0.0388997234\n",
      "step 431 loss 1.05326664 fisher_loss 0.274244934 triplet loss 0.77902174 l2_loss 1.48224032 fraction B 0.3808164 lossA 1.78718412 fraction A 0.0384531356\n",
      "step 432 loss 1.05007684 fisher_loss 0.274841517 triplet loss 0.775235355 l2_loss 1.48599052 fraction B 0.359088749 lossA 1.80780709 fraction A 0.0402590893\n",
      "step 433 loss 1.10936213 fisher_loss 0.27613464 triplet loss 0.833227515 l2_loss 1.49006867 fraction B 0.321009427 lossA 1.84182703 fraction A 0.0431429222\n",
      "step 434 loss 0.899782896 fisher_loss 0.277892441 triplet loss 0.621890426 l2_loss 1.49456382 fraction B 0.317848206 lossA 1.90383661 fraction A 0.0488798\n",
      "step 435 loss 1.1852808 fisher_loss 0.281009585 triplet loss 0.904271245 l2_loss 1.50007653 fraction B 0.35209474 lossA 1.97329521 fraction A 0.0560940914\n",
      "step 436 loss 1.2682426 fisher_loss 0.285129577 triplet loss 0.983113 l2_loss 1.50558519 fraction B 0.265113264 lossA 2.00029683 fraction A 0.0588951558\n",
      "step 437 loss 1.32630444 fisher_loss 0.286833823 triplet loss 1.03947055 l2_loss 1.51012337 fraction B 0.373156965 lossA 2.00541544 fraction A 0.0585748367\n",
      "step 438 loss 1.11150646 fisher_loss 0.285636544 triplet loss 0.825869918 l2_loss 1.51285648 fraction B 0.286475152 lossA 1.98871779 fraction A 0.056165304\n",
      "step 439 loss 0.933174431 fisher_loss 0.283793509 triplet loss 0.649380922 l2_loss 1.51545668 fraction B 0.309344232 lossA 1.95523894 fraction A 0.0520294718\n",
      "step 440 loss 0.889768779 fisher_loss 0.280964077 triplet loss 0.608804703 l2_loss 1.5176146 fraction B 0.266578138 lossA 1.90781069 fraction A 0.0464529507\n",
      "step 441 loss 0.890765071 fisher_loss 0.278043598 triplet loss 0.612721503 l2_loss 1.52064097 fraction B 0.324655086 lossA 1.88253725 fraction A 0.0426079854\n",
      "step 442 loss 1.18573928 fisher_loss 0.276669562 triplet loss 0.909069717 l2_loss 1.52538538 fraction B 0.335244685 lossA 1.85796368 fraction A 0.0390758738\n",
      "step 443 loss 0.988613904 fisher_loss 0.275467336 triplet loss 0.713146567 l2_loss 1.52999771 fraction B 0.31177941 lossA 1.85293734 fraction A 0.0382177234\n",
      "step 444 loss 1.02012718 fisher_loss 0.275496036 triplet loss 0.744631171 l2_loss 1.53507435 fraction B 0.393721759 lossA 1.87634134 fraction A 0.0399976335\n",
      "step 445 loss 1.09875691 fisher_loss 0.276659101 triplet loss 0.822097838 l2_loss 1.54086769 fraction B 0.381088525 lossA 1.92068052 fraction A 0.0449350402\n",
      "step 446 loss 0.955512166 fisher_loss 0.278819054 triplet loss 0.676693082 l2_loss 1.54684103 fraction B 0.272309124 lossA 1.97477329 fraction A 0.050699085\n",
      "step 447 loss 1.20102513 fisher_loss 0.281974167 triplet loss 0.919050932 l2_loss 1.55430675 fraction B 0.433936268 lossA 2.02461576 fraction A 0.0558693297\n",
      "step 448 loss 1.10745215 fisher_loss 0.284510791 triplet loss 0.822941363 l2_loss 1.56084228 fraction B 0.306055516 lossA 2.08539295 fraction A 0.0615574121\n",
      "step 449 loss 1.30446553 fisher_loss 0.287210435 triplet loss 1.01725507 l2_loss 1.56724632 fraction B 0.316264808 lossA 2.12952161 fraction A 0.0654198\n",
      "step 450 loss 1.15171778 fisher_loss 0.288845718 triplet loss 0.862872064 l2_loss 1.57247651 fraction B 0.420787901 lossA 2.12251592 fraction A 0.0639602467\n",
      "step 451 loss 1.05260849 fisher_loss 0.288109481 triplet loss 0.764499 l2_loss 1.57654738 fraction B 0.365780294 lossA 2.07431674 fraction A 0.0590287782\n",
      "step 452 loss 0.974984109 fisher_loss 0.286046088 triplet loss 0.688938 l2_loss 1.58062863 fraction B 0.278199852 lossA 2.01494622 fraction A 0.052645836\n",
      "step 453 loss 0.983469725 fisher_loss 0.284197837 triplet loss 0.699271858 l2_loss 1.58510852 fraction B 0.292921335 lossA 1.97035336 fraction A 0.0458253212\n",
      "step 454 loss 0.990560532 fisher_loss 0.282538235 triplet loss 0.708022296 l2_loss 1.58942175 fraction B 0.320107818 lossA 1.94207275 fraction A 0.0410155281\n",
      "step 455 loss 1.10221291 fisher_loss 0.281968355 triplet loss 0.82024461 l2_loss 1.59466922 fraction B 0.314890534 lossA 1.93856323 fraction A 0.0391469449\n",
      "step 456 loss 1.09550846 fisher_loss 0.282601923 triplet loss 0.812906563 l2_loss 1.60026884 fraction B 0.406193018 lossA 1.95504725 fraction A 0.04093986\n",
      "step 457 loss 1.17196107 fisher_loss 0.28441444 triplet loss 0.887546599 l2_loss 1.60631251 fraction B 0.37949428 lossA 1.98888075 fraction A 0.0457774\n",
      "step 458 loss 1.02608883 fisher_loss 0.287042141 triplet loss 0.739046693 l2_loss 1.61208677 fraction B 0.326823711 lossA 2.02533197 fraction A 0.0513016842\n",
      "step 459 loss 1.022686 fisher_loss 0.2900877 triplet loss 0.732598364 l2_loss 1.61835253 fraction B 0.300100416 lossA 2.07241 fraction A 0.0580313504\n",
      "step 460 loss 1.09559441 fisher_loss 0.293929517 triplet loss 0.801664889 l2_loss 1.625157 fraction B 0.311174601 lossA 2.09377503 fraction A 0.0612958483\n",
      "step 461 loss 0.971728444 fisher_loss 0.296734184 triplet loss 0.67499423 l2_loss 1.63145733 fraction B 0.267184317 lossA 2.10111761 fraction A 0.0622233301\n",
      "step 462 loss 0.958549261 fisher_loss 0.297942489 triplet loss 0.660606742 l2_loss 1.63677216 fraction B 0.255722493 lossA 2.09129167 fraction A 0.0618316196\n",
      "step 463 loss 1.13645756 fisher_loss 0.298977286 triplet loss 0.837480307 l2_loss 1.64220178 fraction B 0.31564647 lossA 2.07048941 fraction A 0.0600977466\n",
      "step 464 loss 1.03054714 fisher_loss 0.299072385 triplet loss 0.731474698 l2_loss 1.64690399 fraction B 0.292565137 lossA 2.03800154 fraction A 0.056960173\n",
      "step 465 loss 1.22367573 fisher_loss 0.298641711 triplet loss 0.925034046 l2_loss 1.65127814 fraction B 0.333692849 lossA 2.01327968 fraction A 0.0544658452\n",
      "step 466 loss 1.03318107 fisher_loss 0.298541725 triplet loss 0.734639347 l2_loss 1.65556407 fraction B 0.320058823 lossA 1.99845803 fraction A 0.0527717806\n",
      "step 467 loss 1.18924737 fisher_loss 0.298586637 triplet loss 0.890660703 l2_loss 1.66014481 fraction B 0.406567603 lossA 1.98754263 fraction A 0.051700931\n",
      "step 468 loss 1.19779706 fisher_loss 0.298670292 triplet loss 0.899126828 l2_loss 1.66472471 fraction B 0.355854809 lossA 1.96419179 fraction A 0.0499917977\n",
      "step 469 loss 1.01028752 fisher_loss 0.298591822 triplet loss 0.711695671 l2_loss 1.66942167 fraction B 0.301873207 lossA 1.93300319 fraction A 0.0470592827\n",
      "step 470 loss 1.06022167 fisher_loss 0.298037708 triplet loss 0.762184 l2_loss 1.67432678 fraction B 0.305714786 lossA 1.89612126 fraction A 0.0437182821\n",
      "step 471 loss 1.17495847 fisher_loss 0.296875954 triplet loss 0.878082454 l2_loss 1.67919528 fraction B 0.381302625 lossA 1.87723279 fraction A 0.0421449877\n",
      "step 472 loss 1.16539907 fisher_loss 0.296072483 triplet loss 0.869326651 l2_loss 1.68403924 fraction B 0.325490773 lossA 1.88270628 fraction A 0.042712342\n",
      "step 473 loss 1.34944189 fisher_loss 0.295924187 triplet loss 1.0535177 l2_loss 1.68887293 fraction B 0.328510553 lossA 1.89521933 fraction A 0.0440836623\n",
      "step 474 loss 1.11858773 fisher_loss 0.295379847 triplet loss 0.823207855 l2_loss 1.69312835 fraction B 0.378726035 lossA 1.91522479 fraction A 0.0456286334\n",
      "step 475 loss 0.927148 fisher_loss 0.294922322 triplet loss 0.632225692 l2_loss 1.69745827 fraction B 0.294653118 lossA 1.97799051 fraction A 0.0506630428\n",
      "step 476 loss 1.10989475 fisher_loss 0.296570241 triplet loss 0.813324511 l2_loss 1.70335424 fraction B 0.375950396 lossA 2.04765439 fraction A 0.0569558628\n",
      "step 477 loss 1.01030719 fisher_loss 0.299027234 triplet loss 0.711279929 l2_loss 1.70942664 fraction B 0.28401798 lossA 2.07583427 fraction A 0.060350474\n",
      "step 478 loss 1.19568491 fisher_loss 0.30057469 triplet loss 0.89511019 l2_loss 1.71499479 fraction B 0.372249454 lossA 2.0839088 fraction A 0.061673291\n",
      "step 479 loss 1.16735232 fisher_loss 0.300759822 triplet loss 0.866592467 l2_loss 1.71896648 fraction B 0.343259394 lossA 2.08328557 fraction A 0.0621165112\n",
      "step 480 loss 1.2001617 fisher_loss 0.300649852 triplet loss 0.899511814 l2_loss 1.72257769 fraction B 0.239790142 lossA 2.09267282 fraction A 0.0634241477\n",
      "step 481 loss 0.90809834 fisher_loss 0.300673753 triplet loss 0.607424557 l2_loss 1.72623467 fraction B 0.249083564 lossA 2.07518649 fraction A 0.0612971187\n",
      "step 482 loss 1.2755245 fisher_loss 0.299320698 triplet loss 0.976203799 l2_loss 1.72996891 fraction B 0.312740177 lossA 2.03163481 fraction A 0.05568441\n",
      "step 483 loss 1.10307252 fisher_loss 0.296774179 triplet loss 0.806298316 l2_loss 1.73383725 fraction B 0.42713967 lossA 1.9825561 fraction A 0.0490864143\n",
      "step 484 loss 1.04670179 fisher_loss 0.29396984 triplet loss 0.752732 l2_loss 1.73819828 fraction B 0.357889265 lossA 1.94661546 fraction A 0.0450169742\n",
      "step 485 loss 1.06745374 fisher_loss 0.292066127 triplet loss 0.775387645 l2_loss 1.74294245 fraction B 0.374201506 lossA 1.94557261 fraction A 0.0454460345\n",
      "step 486 loss 1.29696441 fisher_loss 0.29216677 triplet loss 1.0047977 l2_loss 1.74883461 fraction B 0.385805458 lossA 1.9579699 fraction A 0.0476034209\n",
      "step 487 loss 1.0108006 fisher_loss 0.292283058 triplet loss 0.718517542 l2_loss 1.75415623 fraction B 0.316000342 lossA 2.00712371 fraction A 0.0534373\n",
      "step 488 loss 1.11099076 fisher_loss 0.293986678 triplet loss 0.817004085 l2_loss 1.76035535 fraction B 0.328018963 lossA 2.04381108 fraction A 0.0575925857\n",
      "step 489 loss 1.12694097 fisher_loss 0.295179278 triplet loss 0.831761718 l2_loss 1.76530647 fraction B 0.305918574 lossA 2.05987906 fraction A 0.0592517294\n",
      "step 490 loss 1.00945795 fisher_loss 0.295476 triplet loss 0.713981926 l2_loss 1.76993752 fraction B 0.301903516 lossA 2.07408023 fraction A 0.0610338859\n",
      "step 491 loss 1.05764139 fisher_loss 0.296101719 triplet loss 0.761539638 l2_loss 1.77496862 fraction B 0.329556018 lossA 2.06040239 fraction A 0.05954336\n",
      "step 492 loss 1.00129437 fisher_loss 0.295102417 triplet loss 0.706191897 l2_loss 1.77875435 fraction B 0.317782283 lossA 2.03389096 fraction A 0.0557651147\n",
      "step 493 loss 1.00796628 fisher_loss 0.293555379 triplet loss 0.714410961 l2_loss 1.78204238 fraction B 0.29730466 lossA 2.02214885 fraction A 0.0527858734\n",
      "step 494 loss 0.98137182 fisher_loss 0.292508781 triplet loss 0.688863039 l2_loss 1.78531599 fraction B 0.330107838 lossA 2.03800631 fraction A 0.0534420088\n",
      "step 495 loss 1.28299296 fisher_loss 0.293343127 triplet loss 0.989649832 l2_loss 1.79005301 fraction B 0.473725349 lossA 2.04133248 fraction A 0.052501779\n",
      "step 496 loss 1.27064466 fisher_loss 0.29305768 triplet loss 0.977587044 l2_loss 1.79339099 fraction B 0.402264357 lossA 2.05101466 fraction A 0.0547939874\n",
      "step 497 loss 0.998651624 fisher_loss 0.293875754 triplet loss 0.70477587 l2_loss 1.79669344 fraction B 0.390444458 lossA 2.07776713 fraction A 0.0598504916\n",
      "step 498 loss 1.28064954 fisher_loss 0.295686841 triplet loss 0.984962702 l2_loss 1.80047631 fraction B 0.319205701 lossA 2.09991 fraction A 0.0633552149\n",
      "step 499 loss 0.967675686 fisher_loss 0.296676 triplet loss 0.670999646 l2_loss 1.80314279 fraction B 0.251701087 lossA 2.09458852 fraction A 0.0623043962\n",
      "step 500 loss 1.18671584 fisher_loss 0.295797706 triplet loss 0.890918136 l2_loss 1.80548 fraction B 0.322813898 lossA 2.06483817 fraction A 0.058804635\n",
      "step 501 loss 1.21862423 fisher_loss 0.29388985 triplet loss 0.924734414 l2_loss 1.80728376 fraction B 0.350047 lossA 2.02007151 fraction A 0.0538958423\n",
      "step 502 loss 1.10657418 fisher_loss 0.291432649 triplet loss 0.815141499 l2_loss 1.80914497 fraction B 0.243475735 lossA 1.94957328 fraction A 0.0474485308\n",
      "step 503 loss 1.3562007 fisher_loss 0.289092869 triplet loss 1.0671078 l2_loss 1.81205 fraction B 0.283566743 lossA 1.92266715 fraction A 0.0448429622\n",
      "step 504 loss 1.01215255 fisher_loss 0.288095504 triplet loss 0.724057078 l2_loss 1.81517172 fraction B 0.315687507 lossA 1.91273963 fraction A 0.0438363701\n",
      "step 505 loss 0.995535672 fisher_loss 0.288187683 triplet loss 0.707348 l2_loss 1.81876397 fraction B 0.337665915 lossA 1.94472492 fraction A 0.0463694222\n",
      "step 506 loss 1.20445299 fisher_loss 0.289596438 triplet loss 0.914856493 l2_loss 1.82343292 fraction B 0.286730736 lossA 1.97197938 fraction A 0.0487864166\n",
      "step 507 loss 1.39351642 fisher_loss 0.2907511 triplet loss 1.10276532 l2_loss 1.82778037 fraction B 0.291779727 lossA 2.00211716 fraction A 0.0516866222\n",
      "step 508 loss 1.0848645 fisher_loss 0.291717589 triplet loss 0.793146908 l2_loss 1.8313489 fraction B 0.326910883 lossA 2.01725245 fraction A 0.0532747321\n",
      "step 509 loss 0.903832436 fisher_loss 0.292324275 triplet loss 0.611508131 l2_loss 1.83479929 fraction B 0.253037244 lossA 2.03177881 fraction A 0.0552529283\n",
      "step 510 loss 0.948579133 fisher_loss 0.293676674 triplet loss 0.654902458 l2_loss 1.83930063 fraction B 0.361297876 lossA 2.05203605 fraction A 0.0579401068\n",
      "step 511 loss 1.3288126 fisher_loss 0.295529187 triplet loss 1.03328347 l2_loss 1.84476888 fraction B 0.354852319 lossA 2.07016969 fraction A 0.0601133965\n",
      "step 512 loss 1.00833094 fisher_loss 0.296450168 triplet loss 0.711880744 l2_loss 1.84874678 fraction B 0.345624089 lossA 2.06390691 fraction A 0.0592992194\n",
      "step 513 loss 1.11661744 fisher_loss 0.295911372 triplet loss 0.820706129 l2_loss 1.85239041 fraction B 0.379856795 lossA 2.04622912 fraction A 0.0573077686\n",
      "step 514 loss 1.0182879 fisher_loss 0.294975728 triplet loss 0.72331214 l2_loss 1.85627174 fraction B 0.327436 lossA 1.99071169 fraction A 0.0510055646\n",
      "step 515 loss 0.990780234 fisher_loss 0.292235017 triplet loss 0.698545218 l2_loss 1.8589102 fraction B 0.312891871 lossA 1.94124603 fraction A 0.0455533601\n",
      "step 516 loss 1.14173913 fisher_loss 0.290364653 triplet loss 0.851374507 l2_loss 1.86227882 fraction B 0.347214133 lossA 1.90959847 fraction A 0.0420494676\n",
      "step 517 loss 1.09802079 fisher_loss 0.289274 triplet loss 0.808746755 l2_loss 1.86585569 fraction B 0.362883806 lossA 1.91688669 fraction A 0.0421945043\n",
      "step 518 loss 1.0172323 fisher_loss 0.289417148 triplet loss 0.727815151 l2_loss 1.86996901 fraction B 0.347838938 lossA 1.94037449 fraction A 0.0445689\n",
      "step 519 loss 0.979596496 fisher_loss 0.289895266 triplet loss 0.6897012 l2_loss 1.87394071 fraction B 0.281803221 lossA 1.99056017 fraction A 0.050627\n",
      "step 520 loss 1.20912898 fisher_loss 0.291751146 triplet loss 0.91737783 l2_loss 1.87842906 fraction B 0.332291186 lossA 2.04356551 fraction A 0.0575426705\n",
      "step 521 loss 1.15429497 fisher_loss 0.293834418 triplet loss 0.860460579 l2_loss 1.88290226 fraction B 0.241956294 lossA 2.09182119 fraction A 0.0632138401\n",
      "step 522 loss 1.19671416 fisher_loss 0.295514137 triplet loss 0.9012 l2_loss 1.88697588 fraction B 0.32257998 lossA 2.11884856 fraction A 0.065884538\n",
      "step 523 loss 1.12229109 fisher_loss 0.295296162 triplet loss 0.826994896 l2_loss 1.88959658 fraction B 0.357036233 lossA 2.12254977 fraction A 0.0658672228\n",
      "step 524 loss 1.14523 fisher_loss 0.294087231 triplet loss 0.851142883 l2_loss 1.89176381 fraction B 0.387200624 lossA 2.10124421 fraction A 0.0626891553\n",
      "step 525 loss 1.06308627 fisher_loss 0.290890396 triplet loss 0.772195816 l2_loss 1.89241517 fraction B 0.296024144 lossA 2.05058932 fraction A 0.0566948056\n",
      "step 526 loss 1.03497148 fisher_loss 0.286777496 triplet loss 0.748194039 l2_loss 1.89299715 fraction B 0.266990304 lossA 1.98736274 fraction A 0.0494001731\n",
      "step 527 loss 1.11629164 fisher_loss 0.282840252 triplet loss 0.83345139 l2_loss 1.89479458 fraction B 0.318980694 lossA 1.91254568 fraction A 0.0417584926\n",
      "step 528 loss 1.04995728 fisher_loss 0.279240608 triplet loss 0.770716608 l2_loss 1.89646435 fraction B 0.406548887 lossA 1.87048888 fraction A 0.0380091891\n",
      "step 529 loss 0.887355447 fisher_loss 0.277205616 triplet loss 0.61014986 l2_loss 1.89859343 fraction B 0.355401278 lossA 1.91723621 fraction A 0.041482076\n",
      "step 530 loss 1.18635988 fisher_loss 0.278281748 triplet loss 0.908078134 l2_loss 1.90334773 fraction B 0.406174809 lossA 1.99929392 fraction A 0.0491743274\n",
      "step 531 loss 1.31695223 fisher_loss 0.280992657 triplet loss 1.0359596 l2_loss 1.90872991 fraction B 0.335626155 lossA 2.07021451 fraction A 0.0573331229\n",
      "step 532 loss 1.02018142 fisher_loss 0.284153342 triplet loss 0.736028 l2_loss 1.91331244 fraction B 0.264993429 lossA 2.12596464 fraction A 0.0644419715\n",
      "step 533 loss 1.08929873 fisher_loss 0.287579358 triplet loss 0.801719427 l2_loss 1.91822314 fraction B 0.323918849 lossA 2.14147019 fraction A 0.0669888258\n",
      "step 534 loss 1.17757702 fisher_loss 0.289565533 triplet loss 0.888011515 l2_loss 1.92321539 fraction B 0.231070444 lossA 2.1339798 fraction A 0.0663690567\n",
      "step 535 loss 1.10491776 fisher_loss 0.289685756 triplet loss 0.815232038 l2_loss 1.92715776 fraction B 0.249299258 lossA 2.12490916 fraction A 0.0653386265\n",
      "step 536 loss 1.08504272 fisher_loss 0.289474308 triplet loss 0.795568407 l2_loss 1.93091631 fraction B 0.256314754 lossA 2.10157013 fraction A 0.0617787689\n",
      "step 537 loss 1.20373487 fisher_loss 0.287776858 triplet loss 0.915958047 l2_loss 1.93418539 fraction B 0.334657133 lossA 2.07007194 fraction A 0.0576855\n",
      "step 538 loss 1.06820071 fisher_loss 0.285633206 triplet loss 0.782567501 l2_loss 1.93716037 fraction B 0.289106041 lossA 2.03323746 fraction A 0.0536026955\n",
      "step 539 loss 1.10393214 fisher_loss 0.284284711 triplet loss 0.819647491 l2_loss 1.94106102 fraction B 0.311841339 lossA 2.01119566 fraction A 0.0515851267\n",
      "step 540 loss 1.20017254 fisher_loss 0.283974439 triplet loss 0.916198075 l2_loss 1.94575405 fraction B 0.359688044 lossA 1.97962749 fraction A 0.0485945456\n",
      "step 541 loss 1.09878016 fisher_loss 0.283444405 triplet loss 0.815335751 l2_loss 1.94990993 fraction B 0.429580927 lossA 1.92827523 fraction A 0.0442767665\n",
      "step 542 loss 1.12362158 fisher_loss 0.282440513 triplet loss 0.84118104 l2_loss 1.95417726 fraction B 0.377372295 lossA 1.90148199 fraction A 0.0420775414\n",
      "step 543 loss 1.03032279 fisher_loss 0.282200485 triplet loss 0.748122334 l2_loss 1.95897222 fraction B 0.393866271 lossA 1.91751826 fraction A 0.0430939123\n",
      "step 544 loss 1.17478943 fisher_loss 0.282984 triplet loss 0.89180541 l2_loss 1.96477556 fraction B 0.361396551 lossA 1.96408582 fraction A 0.0466631502\n",
      "step 545 loss 1.05897713 fisher_loss 0.28460595 triplet loss 0.774371147 l2_loss 1.97093523 fraction B 0.364908516 lossA 2.00738454 fraction A 0.0502540879\n",
      "step 546 loss 0.973077893 fisher_loss 0.286182314 triplet loss 0.686895549 l2_loss 1.97670376 fraction B 0.337086737 lossA 2.06600308 fraction A 0.0567002036\n",
      "step 547 loss 1.17054045 fisher_loss 0.289386034 triplet loss 0.881154418 l2_loss 1.98397732 fraction B 0.354348719 lossA 2.12324357 fraction A 0.063986361\n",
      "step 548 loss 1.04155862 fisher_loss 0.293655246 triplet loss 0.747903347 l2_loss 1.99132514 fraction B 0.322066516 lossA 2.13345957 fraction A 0.0655064806\n",
      "step 549 loss 1.15672553 fisher_loss 0.295579076 triplet loss 0.86114645 l2_loss 1.99712729 fraction B 0.304154307 lossA 2.13698697 fraction A 0.0664193705\n",
      "step 550 loss 1.13795543 fisher_loss 0.297136 triplet loss 0.840819418 l2_loss 2.00238109 fraction B 0.331869453 lossA 2.12712908 fraction A 0.0653388798\n",
      "step 551 loss 1.08491719 fisher_loss 0.297080666 triplet loss 0.787836552 l2_loss 2.00661063 fraction B 0.359038115 lossA 2.10974979 fraction A 0.0629589409\n",
      "step 552 loss 1.09498358 fisher_loss 0.296128273 triplet loss 0.798855305 l2_loss 2.00998306 fraction B 0.265925437 lossA 2.07799149 fraction A 0.0591317602\n",
      "step 553 loss 1.12599778 fisher_loss 0.294158667 triplet loss 0.831839085 l2_loss 2.01321697 fraction B 0.354272962 lossA 2.05526829 fraction A 0.0565471277\n",
      "step 554 loss 1.10461831 fisher_loss 0.292622864 triplet loss 0.811995506 l2_loss 2.01684785 fraction B 0.307855725 lossA 2.04232454 fraction A 0.0547274463\n",
      "step 555 loss 0.948084 fisher_loss 0.291504085 triplet loss 0.656579912 l2_loss 2.02084446 fraction B 0.297855765 lossA 2.04323626 fraction A 0.0545045286\n",
      "step 556 loss 0.893963516 fisher_loss 0.291682124 triplet loss 0.602281392 l2_loss 2.02666664 fraction B 0.259127975 lossA 2.06994104 fraction A 0.0569559373\n",
      "step 557 loss 1.06711817 fisher_loss 0.293087125 triplet loss 0.774031043 l2_loss 2.03357816 fraction B 0.373439342 lossA 2.07184243 fraction A 0.0571314357\n",
      "step 558 loss 0.953891 fisher_loss 0.293266475 triplet loss 0.660624504 l2_loss 2.03958941 fraction B 0.27925235 lossA 2.09025717 fraction A 0.0589100048\n",
      "step 559 loss 1.11545253 fisher_loss 0.294100583 triplet loss 0.821351945 l2_loss 2.0456605 fraction B 0.323297918 lossA 2.09772563 fraction A 0.0598285049\n",
      "step 560 loss 1.196545 fisher_loss 0.294927627 triplet loss 0.901617408 l2_loss 2.05199599 fraction B 0.339623839 lossA 2.13179088 fraction A 0.0643513054\n",
      "step 561 loss 1.11402392 fisher_loss 0.297486484 triplet loss 0.816537499 l2_loss 2.0588007 fraction B 0.328669399 lossA 2.14248252 fraction A 0.0661099926\n",
      "step 562 loss 1.12159646 fisher_loss 0.298665047 triplet loss 0.822931409 l2_loss 2.06520438 fraction B 0.360391647 lossA 2.15128398 fraction A 0.06681557\n",
      "step 563 loss 1.08179295 fisher_loss 0.298848778 triplet loss 0.782944202 l2_loss 2.07079864 fraction B 0.37934041 lossA 2.15333223 fraction A 0.0663380846\n",
      "step 564 loss 1.01011252 fisher_loss 0.298160374 triplet loss 0.711952209 l2_loss 2.07542181 fraction B 0.277177274 lossA 2.16305852 fraction A 0.0665853769\n",
      "step 565 loss 1.18430257 fisher_loss 0.297782928 triplet loss 0.886519611 l2_loss 2.08082414 fraction B 0.326850951 lossA 2.14379048 fraction A 0.0633356944\n",
      "step 566 loss 0.87939477 fisher_loss 0.296156853 triplet loss 0.583237886 l2_loss 2.08571219 fraction B 0.266203314 lossA 2.14578676 fraction A 0.0631577298\n",
      "step 567 loss 0.927699447 fisher_loss 0.296564817 triplet loss 0.631134629 l2_loss 2.09200978 fraction B 0.226956874 lossA 2.12290978 fraction A 0.060255859\n",
      "step 568 loss 0.947166085 fisher_loss 0.295995712 triplet loss 0.651170373 l2_loss 2.09904742 fraction B 0.29168728 lossA 2.10278082 fraction A 0.0575295575\n",
      "step 569 loss 1.20991588 fisher_loss 0.295951515 triplet loss 0.913964331 l2_loss 2.10740042 fraction B 0.30183208 lossA 2.08854818 fraction A 0.0557161048\n",
      "step 570 loss 0.952935755 fisher_loss 0.296459854 triplet loss 0.656475902 l2_loss 2.11524558 fraction B 0.281502694 lossA 2.09350538 fraction A 0.0552665815\n",
      "step 571 loss 1.06123412 fisher_loss 0.297326982 triplet loss 0.763907135 l2_loss 2.12369084 fraction B 0.336449355 lossA 2.09282923 fraction A 0.0548928753\n",
      "step 572 loss 1.09778547 fisher_loss 0.298876345 triplet loss 0.798909068 l2_loss 2.13271523 fraction B 0.34903726 lossA 2.11709023 fraction A 0.0575690046\n",
      "step 573 loss 1.23263979 fisher_loss 0.301534891 triplet loss 0.931104958 l2_loss 2.1415081 fraction B 0.334842682 lossA 2.14762473 fraction A 0.0608599819\n",
      "step 574 loss 1.20875144 fisher_loss 0.304065913 triplet loss 0.904685557 l2_loss 2.14968729 fraction B 0.306163639 lossA 2.14281106 fraction A 0.0602420233\n",
      "step 575 loss 1.0296 fisher_loss 0.30488196 triplet loss 0.724718034 l2_loss 2.15643716 fraction B 0.273109078 lossA 2.12401676 fraction A 0.057900805\n",
      "step 576 loss 0.977141142 fisher_loss 0.304976195 triplet loss 0.672164917 l2_loss 2.16296458 fraction B 0.304492742 lossA 2.10747361 fraction A 0.0557286032\n",
      "step 577 loss 1.05836523 fisher_loss 0.3047975 triplet loss 0.753567755 l2_loss 2.16901088 fraction B 0.266125381 lossA 2.11801052 fraction A 0.0565774105\n",
      "step 578 loss 1.11640596 fisher_loss 0.305773526 triplet loss 0.810632467 l2_loss 2.17599607 fraction B 0.297417521 lossA 2.13531 fraction A 0.0581986643\n",
      "step 579 loss 1.190171 fisher_loss 0.306658894 triplet loss 0.88351208 l2_loss 2.18242073 fraction B 0.318052977 lossA 2.09071136 fraction A 0.0534190089\n",
      "step 580 loss 1.05517316 fisher_loss 0.305285573 triplet loss 0.749887586 l2_loss 2.18779421 fraction B 0.261244774 lossA 2.05023217 fraction A 0.0494681634\n",
      "step 581 loss 1.42867041 fisher_loss 0.304186553 triplet loss 1.12448382 l2_loss 2.1939218 fraction B 0.400238782 lossA 2.01144075 fraction A 0.0459057353\n",
      "step 582 loss 1.03513324 fisher_loss 0.302993536 triplet loss 0.732139707 l2_loss 2.19861031 fraction B 0.394716769 lossA 2.02236438 fraction A 0.0467727631\n",
      "step 583 loss 1.0422039 fisher_loss 0.303763032 triplet loss 0.738440931 l2_loss 2.20466733 fraction B 0.250520408 lossA 2.0396235 fraction A 0.047979448\n",
      "step 584 loss 0.992321372 fisher_loss 0.30436185 triplet loss 0.687959492 l2_loss 2.21025968 fraction B 0.321335793 lossA 2.05010629 fraction A 0.0490921363\n",
      "step 585 loss 1.12828 fisher_loss 0.305229962 triplet loss 0.823050082 l2_loss 2.21608496 fraction B 0.369626909 lossA 2.07393956 fraction A 0.0517273359\n",
      "step 586 loss 1.35185242 fisher_loss 0.306719452 triplet loss 1.04513299 l2_loss 2.22231102 fraction B 0.381787926 lossA 2.13637733 fraction A 0.0582542308\n",
      "step 587 loss 1.16544604 fisher_loss 0.30975616 triplet loss 0.855689883 l2_loss 2.22926044 fraction B 0.278494388 lossA 2.15737391 fraction A 0.0613270737\n",
      "step 588 loss 1.20595694 fisher_loss 0.311299592 triplet loss 0.894657373 l2_loss 2.23538113 fraction B 0.402647525 lossA 2.14885807 fraction A 0.0612853095\n",
      "step 589 loss 1.23196697 fisher_loss 0.311255485 triplet loss 0.920711458 l2_loss 2.2407527 fraction B 0.416426629 lossA 2.12734318 fraction A 0.0595325641\n",
      "step 590 loss 1.12143993 fisher_loss 0.310064584 triplet loss 0.81137532 l2_loss 2.24486136 fraction B 0.347730577 lossA 2.10317707 fraction A 0.0567283481\n",
      "step 591 loss 1.06387281 fisher_loss 0.308265179 triplet loss 0.755607605 l2_loss 2.24910855 fraction B 0.375767857 lossA 2.10012841 fraction A 0.055961661\n",
      "step 592 loss 1.38224864 fisher_loss 0.306632459 triplet loss 1.07561624 l2_loss 2.25282645 fraction B 0.36844793 lossA 2.09976554 fraction A 0.0554964505\n",
      "step 593 loss 1.35416985 fisher_loss 0.3049016 triplet loss 1.04926825 l2_loss 2.25605559 fraction B 0.368175596 lossA 2.09711742 fraction A 0.0548490435\n",
      "step 594 loss 1.0084542 fisher_loss 0.303376049 triplet loss 0.705078125 l2_loss 2.25947881 fraction B 0.291858256 lossA 2.10060263 fraction A 0.0547298\n",
      "step 595 loss 1.05771852 fisher_loss 0.301891744 triplet loss 0.755826712 l2_loss 2.26264763 fraction B 0.30893907 lossA 2.13470221 fraction A 0.0575363338\n",
      "step 596 loss 0.950181782 fisher_loss 0.301730335 triplet loss 0.648451447 l2_loss 2.26664305 fraction B 0.27198723 lossA 2.13517714 fraction A 0.056559626\n",
      "step 597 loss 1.07257473 fisher_loss 0.300652832 triplet loss 0.771921873 l2_loss 2.27122045 fraction B 0.290272474 lossA 2.085361 fraction A 0.0513821356\n",
      "step 598 loss 1.10610771 fisher_loss 0.298576355 triplet loss 0.807531357 l2_loss 2.27561879 fraction B 0.402134418 lossA 2.03901482 fraction A 0.0470986217\n",
      "step 599 loss 1.13966501 fisher_loss 0.296885908 triplet loss 0.8427791 l2_loss 2.27989483 fraction B 0.362313867 lossA 2.02475262 fraction A 0.0458174236\n",
      "step 600 loss 1.05378401 fisher_loss 0.296573 triplet loss 0.75721097 l2_loss 2.28497887 fraction B 0.382169724 lossA 2.04545689 fraction A 0.0474360697\n",
      "step 601 loss 1.01212716 fisher_loss 0.297545314 triplet loss 0.714581788 l2_loss 2.29113889 fraction B 0.342865199 lossA 2.07356024 fraction A 0.0502526388\n",
      "step 602 loss 0.969611049 fisher_loss 0.299121171 triplet loss 0.670489848 l2_loss 2.2981503 fraction B 0.307853371 lossA 2.11727548 fraction A 0.0556025468\n",
      "step 603 loss 1.09509611 fisher_loss 0.302185714 triplet loss 0.792910337 l2_loss 2.30618882 fraction B 0.355434656 lossA 2.17607474 fraction A 0.0623676777\n",
      "step 604 loss 0.962453723 fisher_loss 0.305571526 triplet loss 0.656882226 l2_loss 2.31378722 fraction B 0.291757047 lossA 2.2163434 fraction A 0.0675048158\n",
      "step 605 loss 1.38579917 fisher_loss 0.308711052 triplet loss 1.07708812 l2_loss 2.32217646 fraction B 0.392915756 lossA 2.23634863 fraction A 0.0710045472\n",
      "step 606 loss 0.941782236 fisher_loss 0.310631603 triplet loss 0.631150603 l2_loss 2.32882071 fraction B 0.250682384 lossA 2.2362144 fraction A 0.0716326088\n",
      "step 607 loss 1.17627048 fisher_loss 0.311247498 triplet loss 0.865023 l2_loss 2.3347888 fraction B 0.313530654 lossA 2.18360662 fraction A 0.0661036894\n",
      "step 608 loss 1.00681245 fisher_loss 0.309209883 triplet loss 0.69760257 l2_loss 2.34046245 fraction B 0.227421835 lossA 2.11845303 fraction A 0.0584939867\n",
      "step 609 loss 1.08532405 fisher_loss 0.3064031 triplet loss 0.778921 l2_loss 2.34573 fraction B 0.315300912 lossA 2.03878617 fraction A 0.0497904383\n",
      "step 610 loss 1.15865827 fisher_loss 0.303580672 triplet loss 0.855077624 l2_loss 2.35112858 fraction B 0.366280794 lossA 1.95765424 fraction A 0.0429510139\n",
      "step 611 loss 1.03568363 fisher_loss 0.301486909 triplet loss 0.734196663 l2_loss 2.35599613 fraction B 0.336903304 lossA 1.91565359 fraction A 0.0409040712\n",
      "step 612 loss 1.04869521 fisher_loss 0.301248819 triplet loss 0.747446418 l2_loss 2.36248159 fraction B 0.362837672 lossA 1.92339861 fraction A 0.0424415804\n",
      "step 613 loss 1.16967666 fisher_loss 0.302736312 triplet loss 0.866940379 l2_loss 2.37011671 fraction B 0.420098186 lossA 1.9634639 fraction A 0.0459333025\n",
      "step 614 loss 0.959826887 fisher_loss 0.305061162 triplet loss 0.654765725 l2_loss 2.37799072 fraction B 0.334718913 lossA 2.04239345 fraction A 0.0527566038\n",
      "step 615 loss 1.13997543 fisher_loss 0.309014648 triplet loss 0.83096081 l2_loss 2.38753104 fraction B 0.410365 lossA 2.11469316 fraction A 0.059100464\n",
      "step 616 loss 1.06763399 fisher_loss 0.312468171 triplet loss 0.755165815 l2_loss 2.39623737 fraction B 0.30941686 lossA 2.16057229 fraction A 0.0636284426\n",
      "step 617 loss 1.06166065 fisher_loss 0.315149605 triplet loss 0.746511042 l2_loss 2.40417361 fraction B 0.264697224 lossA 2.16394329 fraction A 0.0650025904\n",
      "step 618 loss 1.2629931 fisher_loss 0.316556782 triplet loss 0.946436286 l2_loss 2.41159916 fraction B 0.372731894 lossA 2.135638 fraction A 0.0634362847\n",
      "step 619 loss 1.24815059 fisher_loss 0.31614998 triplet loss 0.932000637 l2_loss 2.41745901 fraction B 0.311946899 lossA 2.09855533 fraction A 0.0606567375\n",
      "step 620 loss 1.05261457 fisher_loss 0.314554125 triplet loss 0.738060415 l2_loss 2.42211795 fraction B 0.331858605 lossA 2.05236983 fraction A 0.0568692908\n",
      "step 621 loss 1.09680927 fisher_loss 0.312323064 triplet loss 0.784486234 l2_loss 2.42665958 fraction B 0.267449468 lossA 1.97170639 fraction A 0.0495985672\n",
      "step 622 loss 1.1355691 fisher_loss 0.309340656 triplet loss 0.826228499 l2_loss 2.4310019 fraction B 0.322472453 lossA 1.90305293 fraction A 0.0418288372\n",
      "step 623 loss 0.980020165 fisher_loss 0.306045681 triplet loss 0.673974514 l2_loss 2.43488741 fraction B 0.354480594 lossA 1.87930715 fraction A 0.0392355807\n",
      "step 624 loss 1.061831 fisher_loss 0.304989129 triplet loss 0.756841898 l2_loss 2.44028926 fraction B 0.301749051 lossA 1.91672516 fraction A 0.0419266373\n",
      "step 625 loss 1.0116837 fisher_loss 0.305616 triplet loss 0.706067741 l2_loss 2.44666314 fraction B 0.334917724 lossA 1.98346293 fraction A 0.0485023595\n",
      "step 626 loss 1.2948736 fisher_loss 0.307735443 triplet loss 0.987138152 l2_loss 2.45403743 fraction B 0.406271 lossA 2.04272938 fraction A 0.0552859604\n",
      "step 627 loss 1.1422087 fisher_loss 0.309762597 triplet loss 0.832446098 l2_loss 2.46039 fraction B 0.324730605 lossA 2.07681608 fraction A 0.0588445142\n",
      "step 628 loss 1.20377302 fisher_loss 0.310009 triplet loss 0.893764 l2_loss 2.46478081 fraction B 0.364741504 lossA 2.07816815 fraction A 0.0584234633\n",
      "step 629 loss 1.14213204 fisher_loss 0.308709472 triplet loss 0.833422542 l2_loss 2.46802759 fraction B 0.345637143 lossA 2.0746541 fraction A 0.0572465137\n",
      "step 630 loss 1.04059124 fisher_loss 0.306781411 triplet loss 0.733809829 l2_loss 2.47080183 fraction B 0.283517897 lossA 2.10021853 fraction A 0.0595398471\n",
      "step 631 loss 1.04495013 fisher_loss 0.306882411 triplet loss 0.738067687 l2_loss 2.47517419 fraction B 0.288974494 lossA 2.12068391 fraction A 0.061554987\n",
      "step 632 loss 1.22494876 fisher_loss 0.307171911 triplet loss 0.917776823 l2_loss 2.48036361 fraction B 0.312781662 lossA 2.11381602 fraction A 0.0605240874\n",
      "step 633 loss 1.15746355 fisher_loss 0.305839121 triplet loss 0.851624429 l2_loss 2.48408365 fraction B 0.294161797 lossA 2.11294293 fraction A 0.059840966\n",
      "step 634 loss 1.18278027 fisher_loss 0.304365456 triplet loss 0.87841475 l2_loss 2.48857141 fraction B 0.37240237 lossA 2.07242274 fraction A 0.055360727\n",
      "step 635 loss 1.12569141 fisher_loss 0.301526785 triplet loss 0.824164689 l2_loss 2.49252582 fraction B 0.299937785 lossA 2.02674747 fraction A 0.0503047258\n",
      "step 636 loss 1.09976709 fisher_loss 0.298452556 triplet loss 0.801314533 l2_loss 2.49686337 fraction B 0.308697104 lossA 1.9776547 fraction A 0.0454697236\n",
      "step 637 loss 1.3661499 fisher_loss 0.295951188 triplet loss 1.07019877 l2_loss 2.50189757 fraction B 0.401007354 lossA 1.94900608 fraction A 0.0429001935\n",
      "step 638 loss 1.01426923 fisher_loss 0.294189066 triplet loss 0.720080137 l2_loss 2.50675917 fraction B 0.352849454 lossA 1.94083798 fraction A 0.042656634\n",
      "step 639 loss 1.10026383 fisher_loss 0.293783218 triplet loss 0.806480646 l2_loss 2.51316071 fraction B 0.415935636 lossA 1.96647203 fraction A 0.0451042\n",
      "step 640 loss 1.06826019 fisher_loss 0.294574231 triplet loss 0.773686 l2_loss 2.52081656 fraction B 0.338151515 lossA 2.00726914 fraction A 0.0488999039\n",
      "step 641 loss 1.02375722 fisher_loss 0.296227813 triplet loss 0.727529407 l2_loss 2.52888536 fraction B 0.279896468 lossA 2.04802608 fraction A 0.0536413081\n",
      "step 642 loss 1.08414936 fisher_loss 0.299010813 triplet loss 0.785138607 l2_loss 2.5378263 fraction B 0.277751356 lossA 2.09875488 fraction A 0.0599287339\n",
      "step 643 loss 1.06052852 fisher_loss 0.30256176 triplet loss 0.757966757 l2_loss 2.54666829 fraction B 0.282907 lossA 2.1259532 fraction A 0.0640148669\n",
      "step 644 loss 1.10407054 fisher_loss 0.305415094 triplet loss 0.79865545 l2_loss 2.55484462 fraction B 0.367717683 lossA 2.12823606 fraction A 0.065435417\n",
      "step 645 loss 0.862288356 fisher_loss 0.306950778 triplet loss 0.555337548 l2_loss 2.56155634 fraction B 0.196258 lossA 2.12084413 fraction A 0.0651094466\n",
      "step 646 loss 1.15937138 fisher_loss 0.307451576 triplet loss 0.85191983 l2_loss 2.56790638 fraction B 0.30682534 lossA 2.10828304 fraction A 0.0638259649\n",
      "step 647 loss 1.08942008 fisher_loss 0.30709362 triplet loss 0.78232646 l2_loss 2.57386804 fraction B 0.30296582 lossA 2.07651 fraction A 0.0600955747\n",
      "step 648 loss 1.08769369 fisher_loss 0.305371761 triplet loss 0.78232193 l2_loss 2.57906199 fraction B 0.337945253 lossA 2.03904939 fraction A 0.0552570932\n",
      "step 649 loss 1.12135279 fisher_loss 0.302749127 triplet loss 0.818603694 l2_loss 2.58422828 fraction B 0.381456673 lossA 1.99800336 fraction A 0.0503574684\n",
      "step 650 loss 0.870784044 fisher_loss 0.300046563 triplet loss 0.570737481 l2_loss 2.58950353 fraction B 0.294185072 lossA 2.0130012 fraction A 0.0508995019\n",
      "step 651 loss 1.12242103 fisher_loss 0.299994618 triplet loss 0.822426438 l2_loss 2.59794712 fraction B 0.362411946 lossA 2.01377773 fraction A 0.0505723022\n",
      "step 652 loss 1.36009169 fisher_loss 0.299856782 triplet loss 1.0602349 l2_loss 2.60614944 fraction B 0.307608157 lossA 2.04517865 fraction A 0.0533567779\n",
      "step 653 loss 1.21095335 fisher_loss 0.300712675 triplet loss 0.91024071 l2_loss 2.61395693 fraction B 0.351055294 lossA 2.10203743 fraction A 0.0587438531\n",
      "step 654 loss 1.14035058 fisher_loss 0.302298188 triplet loss 0.838052392 l2_loss 2.62193036 fraction B 0.288339436 lossA 2.13478804 fraction A 0.0612114146\n",
      "step 655 loss 0.95862174 fisher_loss 0.302830696 triplet loss 0.655791044 l2_loss 2.62915516 fraction B 0.204930201 lossA 2.13526034 fraction A 0.060748633\n",
      "step 656 loss 1.14724255 fisher_loss 0.302155733 triplet loss 0.845086753 l2_loss 2.63618422 fraction B 0.380964369 lossA 2.12421751 fraction A 0.0588256083\n",
      "step 657 loss 1.15339077 fisher_loss 0.300706834 triplet loss 0.852683902 l2_loss 2.64219356 fraction B 0.2574929 lossA 2.13962722 fraction A 0.0596364513\n",
      "step 658 loss 1.04559982 fisher_loss 0.300087333 triplet loss 0.745512486 l2_loss 2.64830542 fraction B 0.308738858 lossA 2.17183805 fraction A 0.0625806674\n",
      "step 659 loss 1.25928664 fisher_loss 0.300627 triplet loss 0.958659708 l2_loss 2.65602541 fraction B 0.294954866 lossA 2.17544842 fraction A 0.0624509528\n",
      "step 660 loss 1.11507201 fisher_loss 0.300209939 triplet loss 0.814862 l2_loss 2.6624341 fraction B 0.276391447 lossA 2.17647 fraction A 0.0613292828\n",
      "step 661 loss 1.13438582 fisher_loss 0.299175471 triplet loss 0.835210323 l2_loss 2.66787434 fraction B 0.333163708 lossA 2.18022943 fraction A 0.06089193\n",
      "step 662 loss 1.13727272 fisher_loss 0.298814595 triplet loss 0.838458121 l2_loss 2.67394328 fraction B 0.281533092 lossA 2.15408421 fraction A 0.0578276366\n",
      "step 663 loss 1.23832023 fisher_loss 0.297784567 triplet loss 0.940535665 l2_loss 2.67910457 fraction B 0.420006692 lossA 2.12315917 fraction A 0.0545690805\n",
      "step 664 loss 1.41877377 fisher_loss 0.296296746 triplet loss 1.12247705 l2_loss 2.68299747 fraction B 0.415658742 lossA 2.10889268 fraction A 0.0530047305\n",
      "step 665 loss 0.884602189 fisher_loss 0.295243949 triplet loss 0.58935827 l2_loss 2.68621373 fraction B 0.30099839 lossA 2.12320495 fraction A 0.0542401411\n",
      "step 666 loss 1.06861067 fisher_loss 0.295849502 triplet loss 0.772761106 l2_loss 2.69198966 fraction B 0.357060522 lossA 2.14451241 fraction A 0.0564855114\n",
      "step 667 loss 1.17581367 fisher_loss 0.296913683 triplet loss 0.878899932 l2_loss 2.69879532 fraction B 0.338332206 lossA 2.15316749 fraction A 0.0575006902\n",
      "step 668 loss 1.04289877 fisher_loss 0.297622591 triplet loss 0.745276153 l2_loss 2.70513 fraction B 0.356336623 lossA 2.14112496 fraction A 0.0571778\n",
      "step 669 loss 0.988864183 fisher_loss 0.29826358 triplet loss 0.690600574 l2_loss 2.71134806 fraction B 0.265686274 lossA 2.09650397 fraction A 0.0534025654\n",
      "step 670 loss 1.04504144 fisher_loss 0.297753394 triplet loss 0.747288048 l2_loss 2.71689606 fraction B 0.303487241 lossA 2.10035849 fraction A 0.0541954786\n",
      "step 671 loss 0.934231639 fisher_loss 0.298716187 triplet loss 0.635515451 l2_loss 2.72323322 fraction B 0.287354648 lossA 2.13078761 fraction A 0.0577471144\n",
      "step 672 loss 1.1485287 fisher_loss 0.301039696 triplet loss 0.847489 l2_loss 2.73176193 fraction B 0.348673582 lossA 2.14326429 fraction A 0.0595529228\n",
      "step 673 loss 1.03550494 fisher_loss 0.302775651 triplet loss 0.732729316 l2_loss 2.73954082 fraction B 0.297676593 lossA 2.14381742 fraction A 0.0602870099\n",
      "step 674 loss 1.09687197 fisher_loss 0.304032177 triplet loss 0.792839825 l2_loss 2.74705219 fraction B 0.256368518 lossA 2.14318299 fraction A 0.0602437258\n",
      "step 675 loss 0.989553452 fisher_loss 0.304172784 triplet loss 0.685380697 l2_loss 2.75330353 fraction B 0.221188948 lossA 2.13048816 fraction A 0.0590397902\n",
      "step 676 loss 1.16258502 fisher_loss 0.304655135 triplet loss 0.857929826 l2_loss 2.7604816 fraction B 0.323120922 lossA 2.09447956 fraction A 0.0553958267\n",
      "step 677 loss 1.15415514 fisher_loss 0.303770959 triplet loss 0.850384176 l2_loss 2.76684499 fraction B 0.35797441 lossA 2.05992031 fraction A 0.051787287\n",
      "step 678 loss 1.04118621 fisher_loss 0.302340478 triplet loss 0.738845706 l2_loss 2.7718668 fraction B 0.316813439 lossA 1.99873006 fraction A 0.0463781506\n",
      "step 679 loss 0.962615728 fisher_loss 0.30043149 triplet loss 0.662184238 l2_loss 2.7767787 fraction B 0.276917726 lossA 1.99172604 fraction A 0.0456942655\n",
      "step 680 loss 1.00000989 fisher_loss 0.300542772 triplet loss 0.699467123 l2_loss 2.78376484 fraction B 0.341240138 lossA 2.0250442 fraction A 0.048168242\n",
      "step 681 loss 1.04944396 fisher_loss 0.30193913 triplet loss 0.74750483 l2_loss 2.79298377 fraction B 0.312084794 lossA 2.06242847 fraction A 0.0512514785\n",
      "step 682 loss 1.01248562 fisher_loss 0.303439826 triplet loss 0.709045827 l2_loss 2.80188632 fraction B 0.268919766 lossA 2.08245206 fraction A 0.0528376\n",
      "step 683 loss 0.988576591 fisher_loss 0.303883672 triplet loss 0.684692919 l2_loss 2.80956292 fraction B 0.250250965 lossA 2.10706973 fraction A 0.0549548157\n",
      "step 684 loss 1.06004906 fisher_loss 0.30471012 triplet loss 0.755338967 l2_loss 2.81847072 fraction B 0.29803592 lossA 2.09352303 fraction A 0.0536642\n",
      "step 685 loss 1.12078905 fisher_loss 0.304430276 triplet loss 0.816358805 l2_loss 2.8275156 fraction B 0.385566562 lossA 2.06327319 fraction A 0.0506379046\n",
      "step 686 loss 1.09521818 fisher_loss 0.303123 triplet loss 0.792095184 l2_loss 2.83547831 fraction B 0.223648518 lossA 2.04164147 fraction A 0.0483815186\n",
      "step 687 loss 1.06061447 fisher_loss 0.301699787 triplet loss 0.758914649 l2_loss 2.84334898 fraction B 0.289489537 lossA 2.01981735 fraction A 0.0460757278\n",
      "step 688 loss 1.1217711 fisher_loss 0.300286949 triplet loss 0.821484089 l2_loss 2.8515408 fraction B 0.311990052 lossA 2.01997304 fraction A 0.0457039736\n",
      "step 689 loss 0.980072737 fisher_loss 0.299877852 triplet loss 0.680194914 l2_loss 2.86036849 fraction B 0.273959845 lossA 2.01535106 fraction A 0.044857271\n",
      "step 690 loss 1.08636761 fisher_loss 0.29932788 triplet loss 0.787039757 l2_loss 2.86962843 fraction B 0.360656083 lossA 2.0137682 fraction A 0.0443766303\n",
      "step 691 loss 0.858150363 fisher_loss 0.298935145 triplet loss 0.559215248 l2_loss 2.87888241 fraction B 0.302585185 lossA 2.07930064 fraction A 0.0494589619\n",
      "step 692 loss 1.00696266 fisher_loss 0.301094443 triplet loss 0.705868244 l2_loss 2.89139342 fraction B 0.318652749 lossA 2.1731627 fraction A 0.0584221222\n",
      "step 693 loss 1.11942387 fisher_loss 0.30513984 triplet loss 0.814284086 l2_loss 2.90559053 fraction B 0.290023059 lossA 2.27822924 fraction A 0.0701717809\n",
      "step 694 loss 1.10489678 fisher_loss 0.31074363 triplet loss 0.794153154 l2_loss 2.91996598 fraction B 0.253136903 lossA 2.31994438 fraction A 0.0752119571\n",
      "step 695 loss 1.04737031 fisher_loss 0.313380122 triplet loss 0.733990192 l2_loss 2.93184257 fraction B 0.32652843 lossA 2.33629203 fraction A 0.0772384331\n",
      "step 696 loss 1.17977917 fisher_loss 0.314094484 triplet loss 0.865684688 l2_loss 2.94137263 fraction B 0.361526936 lossA 2.3168807 fraction A 0.0750043616\n",
      "step 697 loss 1.20517337 fisher_loss 0.31241107 triplet loss 0.892762303 l2_loss 2.94772887 fraction B 0.350711107 lossA 2.27074075 fraction A 0.0695181787\n",
      "step 698 loss 1.10990357 fisher_loss 0.308748215 triplet loss 0.801155329 l2_loss 2.95132589 fraction B 0.24585171 lossA 2.181113 fraction A 0.0597765967\n",
      "step 699 loss 1.00290251 fisher_loss 0.304288566 triplet loss 0.698613942 l2_loss 2.95315933 fraction B 0.258310407 lossA 2.06269932 fraction A 0.047286652\n",
      "step 700 loss 1.30579293 fisher_loss 0.299759179 triplet loss 1.00603378 l2_loss 2.95513058 fraction B 0.268961459 lossA 1.95405519 fraction A 0.0387500525\n",
      "step 701 loss 0.982690573 fisher_loss 0.297277391 triplet loss 0.685413182 l2_loss 2.95827675 fraction B 0.386945069 lossA 1.90737832 fraction A 0.0364342257\n",
      "step 702 loss 1.09140396 fisher_loss 0.296866506 triplet loss 0.794537425 l2_loss 2.96402025 fraction B 0.430200964 lossA 1.96549368 fraction A 0.0409758277\n",
      "step 703 loss 1.14628029 fisher_loss 0.298394114 triplet loss 0.847886145 l2_loss 2.97206759 fraction B 0.463231623 lossA 2.08965445 fraction A 0.051314652\n",
      "step 704 loss 1.17582297 fisher_loss 0.301417589 triplet loss 0.874405324 l2_loss 2.9815321 fraction B 0.326957911 lossA 2.2196331 fraction A 0.0640397891\n",
      "step 705 loss 1.05117941 fisher_loss 0.306075573 triplet loss 0.745103896 l2_loss 2.99164891 fraction B 0.215183556 lossA 2.31602859 fraction A 0.0751438513\n",
      "step 706 loss 1.33278608 fisher_loss 0.311056435 triplet loss 1.02172971 l2_loss 3.00111103 fraction B 0.284056127 lossA 2.35883188 fraction A 0.080396\n",
      "step 707 loss 1.19818866 fisher_loss 0.314012825 triplet loss 0.884175837 l2_loss 3.00794506 fraction B 0.296189696 lossA 2.32907438 fraction A 0.0776423141\n",
      "step 708 loss 1.31277323 fisher_loss 0.313626498 triplet loss 0.99914676 l2_loss 3.01216316 fraction B 0.424858153 lossA 2.2635839 fraction A 0.0703849941\n",
      "step 709 loss 1.03634977 fisher_loss 0.310833514 triplet loss 0.7255162 l2_loss 3.01368403 fraction B 0.255492538 lossA 2.1987474 fraction A 0.063076809\n",
      "step 710 loss 0.95701611 fisher_loss 0.308146626 triplet loss 0.648869514 l2_loss 3.01516223 fraction B 0.174410924 lossA 2.11362 fraction A 0.0546863303\n",
      "step 711 loss 1.09571528 fisher_loss 0.305787504 triplet loss 0.789927781 l2_loss 3.0173347 fraction B 0.292617232 lossA 2.06550932 fraction A 0.0503351167\n",
      "step 712 loss 1.07998598 fisher_loss 0.304938555 triplet loss 0.775047421 l2_loss 3.02115393 fraction B 0.312807679 lossA 2.03383136 fraction A 0.0472961776\n",
      "step 713 loss 1.16874838 fisher_loss 0.304492205 triplet loss 0.864256144 l2_loss 3.02646 fraction B 0.317333072 lossA 2.00101495 fraction A 0.0443654731\n",
      "step 714 loss 1.13137197 fisher_loss 0.30418241 triplet loss 0.827189505 l2_loss 3.03227615 fraction B 0.366149336 lossA 2.00798488 fraction A 0.0453157388\n",
      "step 715 loss 1.06240976 fisher_loss 0.304983735 triplet loss 0.757426 l2_loss 3.03915429 fraction B 0.308754712 lossA 2.02422976 fraction A 0.0470852554\n",
      "step 716 loss 1.11918139 fisher_loss 0.306513727 triplet loss 0.812667608 l2_loss 3.04674673 fraction B 0.391694844 lossA 2.05190444 fraction A 0.0491542928\n",
      "step 717 loss 1.01294863 fisher_loss 0.307973474 triplet loss 0.704975128 l2_loss 3.0546875 fraction B 0.345154315 lossA 2.08836508 fraction A 0.0520566739\n",
      "step 718 loss 1.09063625 fisher_loss 0.309668869 triplet loss 0.780967355 l2_loss 3.06272268 fraction B 0.333481371 lossA 2.09524894 fraction A 0.0527067631\n",
      "step 719 loss 1.21443796 fisher_loss 0.310814649 triplet loss 0.903623283 l2_loss 3.07036304 fraction B 0.299854279 lossA 2.09360361 fraction A 0.0525271334\n",
      "step 720 loss 1.29328656 fisher_loss 0.311262727 triplet loss 0.982023895 l2_loss 3.07713532 fraction B 0.391651541 lossA 2.09404516 fraction A 0.0528434664\n",
      "step 721 loss 1.09884632 fisher_loss 0.31180343 triplet loss 0.787042916 l2_loss 3.0839026 fraction B 0.277181417 lossA 2.07068777 fraction A 0.0513359867\n",
      "step 722 loss 1.13732147 fisher_loss 0.311480433 triplet loss 0.825841069 l2_loss 3.09021854 fraction B 0.281915337 lossA 2.07119536 fraction A 0.0520301946\n",
      "step 723 loss 1.23296928 fisher_loss 0.312009722 triplet loss 0.920959592 l2_loss 3.09733391 fraction B 0.251384556 lossA 2.06143117 fraction A 0.0516379401\n",
      "step 724 loss 1.33763909 fisher_loss 0.311639786 triplet loss 1.02599931 l2_loss 3.10298014 fraction B 0.292896926 lossA 2.05660129 fraction A 0.0516704656\n",
      "step 725 loss 0.886107147 fisher_loss 0.311250627 triplet loss 0.57485652 l2_loss 3.10818458 fraction B 0.202015147 lossA 2.03902674 fraction A 0.0506527536\n",
      "step 726 loss 1.1163671 fisher_loss 0.310781717 triplet loss 0.805585384 l2_loss 3.1145246 fraction B 0.285810679 lossA 2.04154563 fraction A 0.0509197526\n",
      "step 727 loss 1.06893289 fisher_loss 0.310365587 triplet loss 0.758567333 l2_loss 3.12124586 fraction B 0.370133728 lossA 2.06204677 fraction A 0.0527645\n",
      "step 728 loss 1.03588784 fisher_loss 0.310649037 triplet loss 0.7252388 l2_loss 3.12851906 fraction B 0.297062606 lossA 2.10183716 fraction A 0.0565976948\n",
      "step 729 loss 1.35371625 fisher_loss 0.31174466 triplet loss 1.04197156 l2_loss 3.136307 fraction B 0.259979904 lossA 2.12830019 fraction A 0.0590096191\n",
      "step 730 loss 1.155599 fisher_loss 0.312048972 triplet loss 0.84355 l2_loss 3.14267898 fraction B 0.406772316 lossA 2.1254189 fraction A 0.0584806949\n",
      "step 731 loss 0.981685638 fisher_loss 0.31089434 triplet loss 0.670791328 l2_loss 3.14791298 fraction B 0.210919365 lossA 2.1048708 fraction A 0.0560015775\n",
      "step 732 loss 1.0131458 fisher_loss 0.308690101 triplet loss 0.704455733 l2_loss 3.15283632 fraction B 0.284162521 lossA 2.08547878 fraction A 0.053381227\n",
      "step 733 loss 1.17019236 fisher_loss 0.306674182 triplet loss 0.863518178 l2_loss 3.15814018 fraction B 0.269026697 lossA 2.06313205 fraction A 0.0510479808\n",
      "step 734 loss 1.07696927 fisher_loss 0.304706722 triplet loss 0.772262573 l2_loss 3.16260123 fraction B 0.3711285 lossA 2.03277779 fraction A 0.0482792258\n",
      "step 735 loss 0.976324201 fisher_loss 0.302662522 triplet loss 0.673661649 l2_loss 3.16646743 fraction B 0.247559905 lossA 2.02050662 fraction A 0.0469659\n",
      "step 736 loss 1.11837459 fisher_loss 0.301325202 triplet loss 0.817049444 l2_loss 3.17146492 fraction B 0.310969174 lossA 2.02245522 fraction A 0.0468606\n",
      "step 737 loss 0.881151736 fisher_loss 0.300736785 triplet loss 0.580414951 l2_loss 3.17757773 fraction B 0.334927946 lossA 2.05090165 fraction A 0.0489630736\n",
      "step 738 loss 1.10825455 fisher_loss 0.301608294 triplet loss 0.806646287 l2_loss 3.18644738 fraction B 0.352019161 lossA 2.07735753 fraction A 0.0509690866\n",
      "step 739 loss 1.09281576 fisher_loss 0.302378207 triplet loss 0.79043752 l2_loss 3.19531417 fraction B 0.273627 lossA 2.10548735 fraction A 0.0527751148\n",
      "step 740 loss 1.15301538 fisher_loss 0.302826941 triplet loss 0.850188494 l2_loss 3.20384812 fraction B 0.387619585 lossA 2.14029717 fraction A 0.0550331287\n",
      "step 741 loss 1.13707685 fisher_loss 0.303335041 triplet loss 0.833741784 l2_loss 3.21211958 fraction B 0.267453343 lossA 2.15190434 fraction A 0.0554942787\n",
      "step 742 loss 1.0010916 fisher_loss 0.303469092 triplet loss 0.697622478 l2_loss 3.21951437 fraction B 0.23946929 lossA 2.16058064 fraction A 0.0557471141\n",
      "step 743 loss 0.868581712 fisher_loss 0.303932428 triplet loss 0.564649284 l2_loss 3.22754622 fraction B 0.188522592 lossA 2.14760756 fraction A 0.0543924943\n",
      "step 744 loss 0.977897763 fisher_loss 0.304722339 triplet loss 0.673175395 l2_loss 3.23710132 fraction B 0.249492809 lossA 2.16405272 fraction A 0.0558754876\n",
      "step 745 loss 1.14284182 fisher_loss 0.306575119 triplet loss 0.836266637 l2_loss 3.24775147 fraction B 0.327242166 lossA 2.18739343 fraction A 0.0577781573\n",
      "step 746 loss 1.25239503 fisher_loss 0.308149397 triplet loss 0.944245636 l2_loss 3.25752783 fraction B 0.272188365 lossA 2.21325159 fraction A 0.060151428\n",
      "step 747 loss 1.13205111 fisher_loss 0.309986889 triplet loss 0.822064221 l2_loss 3.26707864 fraction B 0.336126894 lossA 2.20633125 fraction A 0.0597692095\n",
      "step 748 loss 1.02925086 fisher_loss 0.311094344 triplet loss 0.718156576 l2_loss 3.27660108 fraction B 0.260055691 lossA 2.16028786 fraction A 0.0559313074\n",
      "step 749 loss 1.01300442 fisher_loss 0.310459465 triplet loss 0.702545 l2_loss 3.28401613 fraction B 0.309913397 lossA 2.14485288 fraction A 0.0546523184\n",
      "step 750 loss 1.09483373 fisher_loss 0.310517758 triplet loss 0.784315944 l2_loss 3.2919755 fraction B 0.22538662 lossA 2.11827898 fraction A 0.0524356365\n",
      "step 751 loss 1.16051829 fisher_loss 0.309783638 triplet loss 0.850734651 l2_loss 3.29883599 fraction B 0.434264898 lossA 2.08744717 fraction A 0.0499448925\n",
      "step 752 loss 0.950185061 fisher_loss 0.308653414 triplet loss 0.641531646 l2_loss 3.304353 fraction B 0.299855083 lossA 2.12440968 fraction A 0.0528335795\n",
      "step 753 loss 0.992745638 fisher_loss 0.309797585 triplet loss 0.682948053 l2_loss 3.31224704 fraction B 0.303123653 lossA 2.1537497 fraction A 0.0549519546\n",
      "step 754 loss 0.995972157 fisher_loss 0.310910434 triplet loss 0.685061753 l2_loss 3.31992722 fraction B 0.296079665 lossA 2.20938706 fraction A 0.0600653663\n",
      "step 755 loss 1.22954237 fisher_loss 0.3138192 triplet loss 0.915723145 l2_loss 3.32967186 fraction B 0.27503255 lossA 2.23233962 fraction A 0.0631699413\n",
      "step 756 loss 0.932656765 fisher_loss 0.315917283 triplet loss 0.616739511 l2_loss 3.33830333 fraction B 0.287893772 lossA 2.20545483 fraction A 0.0619119965\n",
      "step 757 loss 1.10481441 fisher_loss 0.316242903 triplet loss 0.788571477 l2_loss 3.34717488 fraction B 0.379324704 lossA 2.1382618 fraction A 0.056723386\n",
      "step 758 loss 1.23208475 fisher_loss 0.314614892 triplet loss 0.917469919 l2_loss 3.35489798 fraction B 0.397218764 lossA 2.03893 fraction A 0.0488767214\n",
      "step 759 loss 1.04546082 fisher_loss 0.312192947 triplet loss 0.733267844 l2_loss 3.36162353 fraction B 0.228389904 lossA 1.95115125 fraction A 0.0421330705\n",
      "step 760 loss 1.13211346 fisher_loss 0.310426086 triplet loss 0.8216874 l2_loss 3.36897254 fraction B 0.400392532 lossA 1.92643309 fraction A 0.0403175876\n",
      "step 761 loss 1.09475851 fisher_loss 0.310674101 triplet loss 0.784084439 l2_loss 3.37797213 fraction B 0.399209797 lossA 1.98291695 fraction A 0.0449008457\n",
      "step 762 loss 1.24731731 fisher_loss 0.313127249 triplet loss 0.934190094 l2_loss 3.38888359 fraction B 0.328283876 lossA 2.03816748 fraction A 0.0493898876\n",
      "step 763 loss 1.06829131 fisher_loss 0.315040439 triplet loss 0.753250897 l2_loss 3.39860463 fraction B 0.324968547 lossA 2.11459422 fraction A 0.0551637113\n",
      "step 764 loss 1.06374657 fisher_loss 0.317363918 triplet loss 0.746382654 l2_loss 3.40878153 fraction B 0.370530903 lossA 2.16202831 fraction A 0.0584527664\n",
      "step 765 loss 1.0443269 fisher_loss 0.319051564 triplet loss 0.725275338 l2_loss 3.41778851 fraction B 0.290410787 lossA 2.17621779 fraction A 0.0594797172\n",
      "step 766 loss 1.21545982 fisher_loss 0.319757938 triplet loss 0.895701945 l2_loss 3.42607951 fraction B 0.358442783 lossA 2.14516234 fraction A 0.0565141663\n",
      "step 767 loss 1.14004827 fisher_loss 0.318332911 triplet loss 0.821715415 l2_loss 3.43212795 fraction B 0.280920357 lossA 2.10933065 fraction A 0.0532321371\n",
      "step 768 loss 1.11005473 fisher_loss 0.316256166 triplet loss 0.793798506 l2_loss 3.43740058 fraction B 0.317709863 lossA 2.05464864 fraction A 0.048647251\n",
      "step 769 loss 0.94869256 fisher_loss 0.313889682 triplet loss 0.634802878 l2_loss 3.44237542 fraction B 0.260727704 lossA 2.0316875 fraction A 0.0467463173\n",
      "step 770 loss 1.26191545 fisher_loss 0.312629342 triplet loss 0.949286044 l2_loss 3.44813442 fraction B 0.328071594 lossA 2.00606656 fraction A 0.0445980243\n",
      "step 771 loss 1.17933011 fisher_loss 0.311116576 triplet loss 0.868213594 l2_loss 3.45339918 fraction B 0.453305572 lossA 1.97990179 fraction A 0.042479828\n",
      "step 772 loss 0.958193779 fisher_loss 0.30963698 triplet loss 0.648556828 l2_loss 3.45807195 fraction B 0.236198798 lossA 2.02037907 fraction A 0.0451705605\n",
      "step 773 loss 1.08927464 fisher_loss 0.310020834 triplet loss 0.77925384 l2_loss 3.4653995 fraction B 0.366467625 lossA 2.05825686 fraction A 0.0478191264\n",
      "step 774 loss 0.807562947 fisher_loss 0.310478479 triplet loss 0.497084498 l2_loss 3.47155857 fraction B 0.198654 lossA 2.11947179 fraction A 0.052835606\n",
      "step 775 loss 0.919877529 fisher_loss 0.312421829 triplet loss 0.60745573 l2_loss 3.48017263 fraction B 0.296042025 lossA 2.20163846 fraction A 0.0601031445\n",
      "step 776 loss 1.15521169 fisher_loss 0.315642774 triplet loss 0.839568913 l2_loss 3.49007607 fraction B 0.26647535 lossA 2.25123119 fraction A 0.0649894\n",
      "step 777 loss 1.32671535 fisher_loss 0.317885071 triplet loss 1.00883031 l2_loss 3.49855375 fraction B 0.411882758 lossA 2.28050256 fraction A 0.0676015317\n",
      "step 778 loss 1.15286183 fisher_loss 0.318548948 triplet loss 0.834312856 l2_loss 3.50501585 fraction B 0.1995309 lossA 2.2775147 fraction A 0.0668001771\n",
      "step 779 loss 0.922363937 fisher_loss 0.317632973 triplet loss 0.604730964 l2_loss 3.51005816 fraction B 0.218618914 lossA 2.19408059 fraction A 0.0590287074\n",
      "step 780 loss 1.21502817 fisher_loss 0.314617068 triplet loss 0.900411129 l2_loss 3.51539636 fraction B 0.399880171 lossA 2.09554935 fraction A 0.050299149\n",
      "step 781 loss 1.01360488 fisher_loss 0.311297774 triplet loss 0.702307045 l2_loss 3.51968145 fraction B 0.245379746 lossA 1.97324657 fraction A 0.0408857055\n",
      "step 782 loss 1.23582745 fisher_loss 0.308770508 triplet loss 0.927056968 l2_loss 3.52451 fraction B 0.355507225 lossA 1.89571297 fraction A 0.0357448719\n",
      "step 783 loss 1.07251894 fisher_loss 0.307629853 triplet loss 0.764889121 l2_loss 3.52993393 fraction B 0.353971928 lossA 1.88108468 fraction A 0.0350735597\n",
      "step 784 loss 1.01578069 fisher_loss 0.307783216 triplet loss 0.707997441 l2_loss 3.53697133 fraction B 0.393176466 lossA 1.96558404 fraction A 0.0409870557\n",
      "step 785 loss 1.04891419 fisher_loss 0.310185611 triplet loss 0.738728523 l2_loss 3.54628539 fraction B 0.295714527 lossA 2.08930516 fraction A 0.0507738478\n",
      "step 786 loss 1.06630492 fisher_loss 0.314598233 triplet loss 0.75170666 l2_loss 3.5577805 fraction B 0.246056587 lossA 2.19076395 fraction A 0.0604409203\n",
      "step 787 loss 1.18430591 fisher_loss 0.319560319 triplet loss 0.864745617 l2_loss 3.57020211 fraction B 0.286943614 lossA 2.2558527 fraction A 0.0674899295\n",
      "step 788 loss 1.24623561 fisher_loss 0.323298723 triplet loss 0.922936857 l2_loss 3.58057284 fraction B 0.393985868 lossA 2.27673 fraction A 0.0700155199\n",
      "step 789 loss 1.06045032 fisher_loss 0.324699551 triplet loss 0.735750735 l2_loss 3.58926225 fraction B 0.32231304 lossA 2.24811673 fraction A 0.067498982\n",
      "step 790 loss 1.15811014 fisher_loss 0.323639601 triplet loss 0.83447051 l2_loss 3.59641933 fraction B 0.356865197 lossA 2.17699718 fraction A 0.0601952225\n",
      "step 791 loss 1.31599426 fisher_loss 0.320120215 triplet loss 0.995874107 l2_loss 3.60154033 fraction B 0.311475068 lossA 2.05613256 fraction A 0.0490708724\n",
      "step 792 loss 1.10341191 fisher_loss 0.315420359 triplet loss 0.787991524 l2_loss 3.60487437 fraction B 0.314540237 lossA 1.92522609 fraction A 0.0384057201\n",
      "step 793 loss 1.0862906 fisher_loss 0.311335027 triplet loss 0.774955511 l2_loss 3.60770726 fraction B 0.310428679 lossA 1.81890607 fraction A 0.0315424614\n",
      "step 794 loss 1.18425214 fisher_loss 0.308732212 triplet loss 0.875519931 l2_loss 3.61180711 fraction B 0.351636589 lossA 1.82334554 fraction A 0.0317103192\n",
      "step 795 loss 0.982230067 fisher_loss 0.308310091 triplet loss 0.67392 l2_loss 3.61825538 fraction B 0.327140123 lossA 1.89194739 fraction A 0.0359485187\n",
      "step 796 loss 1.19230938 fisher_loss 0.309381872 triplet loss 0.882927477 l2_loss 3.6270659 fraction B 0.354560137 lossA 1.98759127 fraction A 0.0430850387\n",
      "step 797 loss 0.973856 fisher_loss 0.311401665 triplet loss 0.662454307 l2_loss 3.63641691 fraction B 0.284408301 lossA 2.1045897 fraction A 0.0531896837\n",
      "step 798 loss 1.35520792 fisher_loss 0.314768821 triplet loss 1.04043913 l2_loss 3.64729738 fraction B 0.305443406 lossA 2.20811105 fraction A 0.0625625923\n",
      "step 799 loss 1.0524776 fisher_loss 0.317619085 triplet loss 0.734858513 l2_loss 3.65660048 fraction B 0.298457384 lossA 2.25945091 fraction A 0.0669080094\n",
      "step 800 loss 1.32437372 fisher_loss 0.318613321 triplet loss 1.00576043 l2_loss 3.66502738 fraction B 0.305437952 lossA 2.28394723 fraction A 0.0681610629\n",
      "step 801 loss 1.20380759 fisher_loss 0.317911863 triplet loss 0.885895729 l2_loss 3.67180681 fraction B 0.285975456 lossA 2.30292463 fraction A 0.0685165524\n",
      "step 802 loss 1.17012393 fisher_loss 0.316693753 triplet loss 0.853430152 l2_loss 3.67768383 fraction B 0.355710536 lossA 2.26349473 fraction A 0.0638164431\n",
      "step 803 loss 1.0671829 fisher_loss 0.313588083 triplet loss 0.753594816 l2_loss 3.6814425 fraction B 0.322301984 lossA 2.17934585 fraction A 0.0558786392\n",
      "step 804 loss 1.2450974 fisher_loss 0.309404701 triplet loss 0.935692668 l2_loss 3.684268 fraction B 0.374215186 lossA 2.07903886 fraction A 0.047632616\n",
      "step 805 loss 0.945598364 fisher_loss 0.305450141 triplet loss 0.640148222 l2_loss 3.68631196 fraction B 0.328448176 lossA 2.00713372 fraction A 0.0420355946\n",
      "step 806 loss 1.13353765 fisher_loss 0.303125262 triplet loss 0.830412388 l2_loss 3.69095063 fraction B 0.288867921 lossA 1.98409271 fraction A 0.0399809\n",
      "step 807 loss 1.30944979 fisher_loss 0.301939517 triplet loss 1.0075103 l2_loss 3.69658613 fraction B 0.353422344 lossA 1.99471796 fraction A 0.0403691716\n",
      "step 808 loss 1.05031836 fisher_loss 0.301130891 triplet loss 0.749187469 l2_loss 3.70206785 fraction B 0.280934453 lossA 2.03081489 fraction A 0.0433337055\n",
      "step 809 loss 1.17530239 fisher_loss 0.301025 triplet loss 0.874277353 l2_loss 3.70856285 fraction B 0.333668 lossA 2.08789611 fraction A 0.0479568094\n",
      "step 810 loss 0.881094337 fisher_loss 0.30149138 triplet loss 0.579602957 l2_loss 3.71558 fraction B 0.255527705 lossA 2.19351602 fraction A 0.0567516424\n",
      "step 811 loss 1.16245711 fisher_loss 0.30380857 triplet loss 0.858648539 l2_loss 3.72425365 fraction B 0.274761736 lossA 2.28211689 fraction A 0.064118214\n",
      "step 812 loss 1.16104341 fisher_loss 0.305943966 triplet loss 0.85509944 l2_loss 3.73190379 fraction B 0.359508187 lossA 2.3161006 fraction A 0.067423135\n",
      "step 813 loss 1.12759304 fisher_loss 0.30714649 triplet loss 0.820446491 l2_loss 3.73961949 fraction B 0.369279385 lossA 2.31141853 fraction A 0.0672633573\n",
      "step 814 loss 1.15629506 fisher_loss 0.306799173 triplet loss 0.849495888 l2_loss 3.74516654 fraction B 0.414393932 lossA 2.26434064 fraction A 0.062944673\n",
      "step 815 loss 1.01770544 fisher_loss 0.304834366 triplet loss 0.712871075 l2_loss 3.74953842 fraction B 0.333976358 lossA 2.17232943 fraction A 0.0545911\n",
      "step 816 loss 1.13566303 fisher_loss 0.30151552 triplet loss 0.834147573 l2_loss 3.75304747 fraction B 0.299919605 lossA 2.0989387 fraction A 0.0483988337\n",
      "step 817 loss 1.26263726 fisher_loss 0.299437582 triplet loss 0.963199675 l2_loss 3.757761 fraction B 0.312644392 lossA 2.001544 fraction A 0.0412528962\n",
      "step 818 loss 1.0589776 fisher_loss 0.297320515 triplet loss 0.761657119 l2_loss 3.76197767 fraction B 0.287099034 lossA 1.91621971 fraction A 0.0355332606\n",
      "step 819 loss 0.976413369 fisher_loss 0.295819104 triplet loss 0.680594265 l2_loss 3.76678586 fraction B 0.293634236 lossA 1.94301462 fraction A 0.0369803943\n",
      "step 820 loss 1.20862758 fisher_loss 0.296700537 triplet loss 0.911927044 l2_loss 3.77537203 fraction B 0.38908872 lossA 2.00433755 fraction A 0.040920224\n",
      "step 821 loss 1.11263514 fisher_loss 0.298116982 triplet loss 0.814518154 l2_loss 3.78435588 fraction B 0.389249593 lossA 2.07030582 fraction A 0.0457411036\n",
      "step 822 loss 1.08554971 fisher_loss 0.299906373 triplet loss 0.785643339 l2_loss 3.7933557 fraction B 0.2910088 lossA 2.13082933 fraction A 0.0503439568\n",
      "step 823 loss 1.12400091 fisher_loss 0.301680446 triplet loss 0.822320461 l2_loss 3.80218506 fraction B 0.26324147 lossA 2.17187071 fraction A 0.054029543\n",
      "step 824 loss 1.04755318 fisher_loss 0.303196847 triplet loss 0.744356334 l2_loss 3.81053567 fraction B 0.303810447 lossA 2.18819618 fraction A 0.0553751439\n",
      "step 825 loss 1.28407431 fisher_loss 0.303716 triplet loss 0.980358362 l2_loss 3.8172791 fraction B 0.245144397 lossA 2.20273137 fraction A 0.0564674363\n",
      "step 826 loss 0.951723 fisher_loss 0.3038809 triplet loss 0.64784205 l2_loss 3.82336521 fraction B 0.254547328 lossA 2.20288634 fraction A 0.0564736314\n",
      "step 827 loss 1.13864553 fisher_loss 0.304051727 triplet loss 0.834593832 l2_loss 3.83061123 fraction B 0.227191597 lossA 2.2002027 fraction A 0.0562626347\n",
      "step 828 loss 1.13083124 fisher_loss 0.304394424 triplet loss 0.826436818 l2_loss 3.83861661 fraction B 0.31014812 lossA 2.19823432 fraction A 0.0560907945\n",
      "step 829 loss 1.51173 fisher_loss 0.304304183 triplet loss 1.20742571 l2_loss 3.84625721 fraction B 0.437140435 lossA 2.19712639 fraction A 0.0558961704\n",
      "step 830 loss 1.16568339 fisher_loss 0.303690434 triplet loss 0.861992955 l2_loss 3.85188484 fraction B 0.259763062 lossA 2.17584014 fraction A 0.054571867\n",
      "step 831 loss 1.08409691 fisher_loss 0.303280652 triplet loss 0.780816317 l2_loss 3.85811234 fraction B 0.336503506 lossA 2.1584034 fraction A 0.0533406213\n",
      "step 832 loss 1.05821609 fisher_loss 0.302614629 triplet loss 0.755601406 l2_loss 3.86434531 fraction B 0.267675906 lossA 2.12219954 fraction A 0.0503509454\n",
      "step 833 loss 1.34256268 fisher_loss 0.301333159 triplet loss 1.04122949 l2_loss 3.8692472 fraction B 0.28178817 lossA 2.08816099 fraction A 0.0477372631\n",
      "step 834 loss 1.31147575 fisher_loss 0.300131708 triplet loss 1.01134408 l2_loss 3.8737545 fraction B 0.289381772 lossA 2.04404926 fraction A 0.0447142981\n",
      "step 835 loss 1.03062725 fisher_loss 0.299151391 triplet loss 0.73147583 l2_loss 3.87849379 fraction B 0.384328187 lossA 2.04774261 fraction A 0.0456696\n",
      "step 836 loss 1.05595124 fisher_loss 0.29972595 triplet loss 0.756225288 l2_loss 3.88480043 fraction B 0.323207706 lossA 2.04966044 fraction A 0.0462806746\n",
      "step 837 loss 0.959386349 fisher_loss 0.300126523 triplet loss 0.659259796 l2_loss 3.89072847 fraction B 0.281403899 lossA 2.06229019 fraction A 0.0474732369\n",
      "step 838 loss 1.07561898 fisher_loss 0.300685197 triplet loss 0.774933815 l2_loss 3.89727 fraction B 0.318100929 lossA 2.06753373 fraction A 0.0482550301\n",
      "step 839 loss 1.03514361 fisher_loss 0.301059812 triplet loss 0.734083831 l2_loss 3.9045074 fraction B 0.317495227 lossA 2.06742597 fraction A 0.0483462401\n",
      "step 840 loss 1.14044082 fisher_loss 0.301057339 triplet loss 0.839383483 l2_loss 3.91125703 fraction B 0.331560582 lossA 2.03955746 fraction A 0.0462355427\n",
      "step 841 loss 1.11034274 fisher_loss 0.300075114 triplet loss 0.810267568 l2_loss 3.91720247 fraction B 0.326417953 lossA 1.9994489 fraction A 0.0433281288\n",
      "step 842 loss 1.10205424 fisher_loss 0.298944265 triplet loss 0.803109944 l2_loss 3.9228518 fraction B 0.319675505 lossA 1.98760331 fraction A 0.0422676355\n",
      "step 843 loss 1.06939411 fisher_loss 0.298492968 triplet loss 0.770901144 l2_loss 3.92932129 fraction B 0.261360735 lossA 1.99311745 fraction A 0.0423104167\n",
      "step 844 loss 0.959728122 fisher_loss 0.298251063 triplet loss 0.661477089 l2_loss 3.93498373 fraction B 0.302899152 lossA 2.02348566 fraction A 0.0442717671\n",
      "step 845 loss 1.07044327 fisher_loss 0.29876247 triplet loss 0.771680772 l2_loss 3.94213152 fraction B 0.29586935 lossA 2.06256437 fraction A 0.0470125936\n",
      "step 846 loss 0.972534835 fisher_loss 0.299769104 triplet loss 0.672765732 l2_loss 3.95052242 fraction B 0.226373136 lossA 2.10508633 fraction A 0.0499509387\n",
      "step 847 loss 1.1709615 fisher_loss 0.301258892 triplet loss 0.869702578 l2_loss 3.95969057 fraction B 0.307563633 lossA 2.13853931 fraction A 0.0528510734\n",
      "step 848 loss 0.968865097 fisher_loss 0.302670419 triplet loss 0.666194677 l2_loss 3.96852374 fraction B 0.316270292 lossA 2.15571737 fraction A 0.0543384142\n",
      "step 849 loss 1.0118525 fisher_loss 0.303590775 triplet loss 0.708261669 l2_loss 3.97788382 fraction B 0.370211899 lossA 2.1490643 fraction A 0.0538849756\n",
      "step 850 loss 1.03252661 fisher_loss 0.303337365 triplet loss 0.729189277 l2_loss 3.98564172 fraction B 0.295979261 lossA 2.10055828 fraction A 0.0497966297\n",
      "step 851 loss 1.32688606 fisher_loss 0.302199841 triplet loss 1.02468622 l2_loss 3.99247789 fraction B 0.328249365 lossA 2.04746866 fraction A 0.0458721556\n",
      "step 852 loss 0.995838821 fisher_loss 0.300661147 triplet loss 0.695177674 l2_loss 3.99791169 fraction B 0.320471823 lossA 2.01218891 fraction A 0.0435694791\n",
      "step 853 loss 0.988256216 fisher_loss 0.300040454 triplet loss 0.688215733 l2_loss 4.00532722 fraction B 0.289205909 lossA 2.00587845 fraction A 0.0433804356\n",
      "step 854 loss 1.11094105 fisher_loss 0.300109148 triplet loss 0.810831904 l2_loss 4.0139308 fraction B 0.327759206 lossA 2.01244974 fraction A 0.0440558046\n",
      "step 855 loss 0.895393312 fisher_loss 0.300579429 triplet loss 0.594813883 l2_loss 4.02273893 fraction B 0.285161465 lossA 2.08760571 fraction A 0.0496848486\n",
      "step 856 loss 0.883827209 fisher_loss 0.303148538 triplet loss 0.580678642 l2_loss 4.03462601 fraction B 0.182815388 lossA 2.1662209 fraction A 0.0570056\n",
      "step 857 loss 1.09487176 fisher_loss 0.306848228 triplet loss 0.788023531 l2_loss 4.04828501 fraction B 0.238555 lossA 2.20179296 fraction A 0.060542345\n",
      "step 858 loss 1.00013065 fisher_loss 0.308848768 triplet loss 0.691281915 l2_loss 4.05965471 fraction B 0.258357167 lossA 2.18096423 fraction A 0.0586168543\n",
      "step 859 loss 1.04964209 fisher_loss 0.308980823 triplet loss 0.740661204 l2_loss 4.06921196 fraction B 0.273929954 lossA 2.13512564 fraction A 0.0543955\n",
      "step 860 loss 0.997759759 fisher_loss 0.308166862 triplet loss 0.689592898 l2_loss 4.07723475 fraction B 0.289433509 lossA 2.08776951 fraction A 0.0500646792\n",
      "step 861 loss 0.955631256 fisher_loss 0.306970656 triplet loss 0.6486606 l2_loss 4.08365345 fraction B 0.213338658 lossA 2.04707217 fraction A 0.0467375517\n",
      "step 862 loss 1.13578081 fisher_loss 0.306102902 triplet loss 0.829677939 l2_loss 4.09089708 fraction B 0.277074158 lossA 2.02390337 fraction A 0.0448178947\n",
      "step 863 loss 1.46161973 fisher_loss 0.305266827 triplet loss 1.15635288 l2_loss 4.09745026 fraction B 0.250521928 lossA 2.02476168 fraction A 0.0446466\n",
      "step 864 loss 0.946236908 fisher_loss 0.305090427 triplet loss 0.641146481 l2_loss 4.10483885 fraction B 0.260163635 lossA 2.05805898 fraction A 0.0467291847\n",
      "step 865 loss 0.993031323 fisher_loss 0.305771172 triplet loss 0.687260151 l2_loss 4.11358 fraction B 0.276986778 lossA 2.06786895 fraction A 0.0471593291\n",
      "step 866 loss 1.09814763 fisher_loss 0.306173742 triplet loss 0.791973829 l2_loss 4.12174 fraction B 0.31321308 lossA 2.11432028 fraction A 0.0506264567\n",
      "step 867 loss 1.16571951 fisher_loss 0.307725102 triplet loss 0.857994437 l2_loss 4.13051653 fraction B 0.337306291 lossA 2.14161754 fraction A 0.0529142097\n",
      "step 868 loss 0.925649524 fisher_loss 0.308940828 triplet loss 0.616708696 l2_loss 4.13972235 fraction B 0.212908074 lossA 2.15688086 fraction A 0.0545795113\n",
      "step 869 loss 0.929283857 fisher_loss 0.310111791 triplet loss 0.619172096 l2_loss 4.1489253 fraction B 0.221867383 lossA 2.16381907 fraction A 0.0556755364\n",
      "step 870 loss 1.2172029 fisher_loss 0.311430782 triplet loss 0.90577209 l2_loss 4.15837288 fraction B 0.263511091 lossA 2.15390635 fraction A 0.0550360978\n",
      "step 871 loss 1.02999592 fisher_loss 0.311685234 triplet loss 0.718310654 l2_loss 4.16666126 fraction B 0.285048664 lossA 2.12841797 fraction A 0.0529025458\n",
      "step 872 loss 1.05288363 fisher_loss 0.311210692 triplet loss 0.741672933 l2_loss 4.17415953 fraction B 0.255449116 lossA 2.08297276 fraction A 0.0491960943\n",
      "step 873 loss 1.17601323 fisher_loss 0.310194552 triplet loss 0.86581862 l2_loss 4.18209028 fraction B 0.313433141 lossA 2.03761 fraction A 0.0460823588\n",
      "step 874 loss 1.15609443 fisher_loss 0.309601218 triplet loss 0.846493244 l2_loss 4.18990707 fraction B 0.273628503 lossA 1.98753083 fraction A 0.0427976102\n",
      "step 875 loss 1.11755276 fisher_loss 0.309070051 triplet loss 0.808482766 l2_loss 4.19653416 fraction B 0.311191261 lossA 1.96854615 fraction A 0.0417134315\n",
      "step 876 loss 0.982530355 fisher_loss 0.309115976 triplet loss 0.673414409 l2_loss 4.2051506 fraction B 0.328136295 lossA 1.9996593 fraction A 0.044035051\n",
      "step 877 loss 1.10759866 fisher_loss 0.310380548 triplet loss 0.797218144 l2_loss 4.21602154 fraction B 0.283693135 lossA 2.01897383 fraction A 0.045484066\n",
      "step 878 loss 0.993148744 fisher_loss 0.310822 triplet loss 0.682326734 l2_loss 4.22496462 fraction B 0.270605177 lossA 2.07454634 fraction A 0.0498243049\n",
      "step 879 loss 0.817215204 fisher_loss 0.312451243 triplet loss 0.504763961 l2_loss 4.23617697 fraction B 0.222748235 lossA 2.11305904 fraction A 0.0540125892\n",
      "step 880 loss 1.16274703 fisher_loss 0.314896584 triplet loss 0.847850442 l2_loss 4.24960232 fraction B 0.265574038 lossA 2.14047956 fraction A 0.0571543649\n",
      "step 881 loss 1.03243148 fisher_loss 0.316579759 triplet loss 0.715851724 l2_loss 4.26120186 fraction B 0.347444147 lossA 2.16370916 fraction A 0.059910804\n",
      "step 882 loss 1.19523692 fisher_loss 0.318042427 triplet loss 0.877194464 l2_loss 4.27191257 fraction B 0.306184143 lossA 2.14378452 fraction A 0.0583668835\n",
      "step 883 loss 1.11578202 fisher_loss 0.317285448 triplet loss 0.798496544 l2_loss 4.28011417 fraction B 0.320807546 lossA 2.08333158 fraction A 0.0525741503\n",
      "step 884 loss 1.07710969 fisher_loss 0.314610243 triplet loss 0.762499452 l2_loss 4.28626776 fraction B 0.30048871 lossA 2.01991558 fraction A 0.046560172\n",
      "step 885 loss 1.17505944 fisher_loss 0.311947584 triplet loss 0.863111854 l2_loss 4.29204941 fraction B 0.276103288 lossA 1.95729613 fraction A 0.0413808338\n",
      "step 886 loss 1.06839681 fisher_loss 0.309859782 triplet loss 0.758537 l2_loss 4.29790878 fraction B 0.327514082 lossA 1.93002534 fraction A 0.0395450331\n",
      "step 887 loss 0.978669167 fisher_loss 0.309105158 triplet loss 0.669564 l2_loss 4.30531025 fraction B 0.284437 lossA 1.96487355 fraction A 0.0420403406\n",
      "step 888 loss 1.10696125 fisher_loss 0.309508294 triplet loss 0.797453 l2_loss 4.31394 fraction B 0.308987558 lossA 2.03541303 fraction A 0.046985317\n",
      "step 889 loss 1.28607476 fisher_loss 0.310893357 triplet loss 0.975181401 l2_loss 4.32402563 fraction B 0.397195101 lossA 2.11788392 fraction A 0.0527869947\n",
      "step 890 loss 0.929506421 fisher_loss 0.312206835 triplet loss 0.617299616 l2_loss 4.33335924 fraction B 0.230034038 lossA 2.2306211 fraction A 0.0617450103\n",
      "step 891 loss 1.06458259 fisher_loss 0.31502527 triplet loss 0.749557257 l2_loss 4.34469891 fraction B 0.298904866 lossA 2.26685333 fraction A 0.0649399236\n",
      "step 892 loss 1.18720484 fisher_loss 0.316006422 triplet loss 0.871198475 l2_loss 4.3543582 fraction B 0.28444925 lossA 2.28796649 fraction A 0.0665293783\n",
      "step 893 loss 1.06749296 fisher_loss 0.315825939 triplet loss 0.751666963 l2_loss 4.36226273 fraction B 0.295347393 lossA 2.28220582 fraction A 0.0653498173\n",
      "step 894 loss 1.08303785 fisher_loss 0.314356506 triplet loss 0.768681347 l2_loss 4.36882067 fraction B 0.355532974 lossA 2.21258926 fraction A 0.0585419126\n",
      "step 895 loss 1.07088411 fisher_loss 0.311104983 triplet loss 0.759779096 l2_loss 4.37341452 fraction B 0.266325265 lossA 2.12238836 fraction A 0.0503002\n",
      "step 896 loss 1.04580784 fisher_loss 0.307536453 triplet loss 0.738271356 l2_loss 4.37631655 fraction B 0.23147884 lossA 1.99011576 fraction A 0.0404516868\n",
      "step 897 loss 1.10236311 fisher_loss 0.304490536 triplet loss 0.797872543 l2_loss 4.38035965 fraction B 0.302939951 lossA 1.87676644 fraction A 0.0332911499\n",
      "step 898 loss 1.01312256 fisher_loss 0.302582502 triplet loss 0.71054 l2_loss 4.386168 fraction B 0.358625293 lossA 1.81069648 fraction A 0.0298079383\n",
      "step 899 loss 1.10465312 fisher_loss 0.301480114 triplet loss 0.803173065 l2_loss 4.39246845 fraction B 0.364903718 lossA 1.83807576 fraction A 0.0311478153\n",
      "step 900 loss 1.05452859 fisher_loss 0.301252604 triplet loss 0.753276 l2_loss 4.39997911 fraction B 0.346421838 lossA 1.91342938 fraction A 0.0352660455\n",
      "step 901 loss 1.1102 fisher_loss 0.301862478 triplet loss 0.808337569 l2_loss 4.40822172 fraction B 0.306432754 lossA 2.03962207 fraction A 0.0430902541\n",
      "step 902 loss 0.936064541 fisher_loss 0.30364424 triplet loss 0.632420301 l2_loss 4.41792822 fraction B 0.175040588 lossA 2.12810493 fraction A 0.0487400889\n",
      "step 903 loss 0.974948049 fisher_loss 0.305419 triplet loss 0.66952908 l2_loss 4.42817163 fraction B 0.277148634 lossA 2.26560092 fraction A 0.0585249588\n",
      "step 904 loss 1.00217581 fisher_loss 0.308818758 triplet loss 0.693357 l2_loss 4.44035 fraction B 0.305880874 lossA 2.32456708 fraction A 0.0634874329\n",
      "step 905 loss 1.11429501 fisher_loss 0.311074734 triplet loss 0.803220332 l2_loss 4.45094824 fraction B 0.281224877 lossA 2.3371675 fraction A 0.0644830465\n",
      "step 906 loss 1.01753926 fisher_loss 0.311672598 triplet loss 0.705866635 l2_loss 4.45911074 fraction B 0.293188393 lossA 2.32164526 fraction A 0.0633456558\n",
      "step 907 loss 1.1577332 fisher_loss 0.311599851 triplet loss 0.846133411 l2_loss 4.46794367 fraction B 0.259245604 lossA 2.29160285 fraction A 0.060708642\n",
      "step 908 loss 1.32924962 fisher_loss 0.31071642 triplet loss 1.01853323 l2_loss 4.47567415 fraction B 0.303599805 lossA 2.2522428 fraction A 0.0568837449\n",
      "step 909 loss 1.30332518 fisher_loss 0.308915883 triplet loss 0.994409323 l2_loss 4.48081446 fraction B 0.286639333 lossA 2.18805766 fraction A 0.0512669086\n",
      "step 910 loss 1.05164552 fisher_loss 0.306837171 triplet loss 0.744808316 l2_loss 4.48585224 fraction B 0.338120937 lossA 2.12863278 fraction A 0.0464503057\n",
      "step 911 loss 1.20235395 fisher_loss 0.305008948 triplet loss 0.897345066 l2_loss 4.49099302 fraction B 0.301573 lossA 2.06079817 fraction A 0.0413644239\n",
      "step 912 loss 1.26475883 fisher_loss 0.303115964 triplet loss 0.961642802 l2_loss 4.49598265 fraction B 0.377737075 lossA 2.00601602 fraction A 0.0377824\n",
      "step 913 loss 1.08126497 fisher_loss 0.301307499 triplet loss 0.779957414 l2_loss 4.50002861 fraction B 0.353079051 lossA 2.02601051 fraction A 0.0390372612\n",
      "step 914 loss 1.07477331 fisher_loss 0.300827235 triplet loss 0.773946106 l2_loss 4.50574207 fraction B 0.317202181 lossA 2.11153436 fraction A 0.044471968\n",
      "step 915 loss 1.14256179 fisher_loss 0.301704317 triplet loss 0.840857446 l2_loss 4.51366329 fraction B 0.214951515 lossA 2.22845483 fraction A 0.0522041\n",
      "step 916 loss 1.06665742 fisher_loss 0.303483605 triplet loss 0.763173819 l2_loss 4.52201509 fraction B 0.357803196 lossA 2.31779861 fraction A 0.0587830096\n",
      "step 917 loss 1.20844483 fisher_loss 0.305094391 triplet loss 0.903350472 l2_loss 4.53054047 fraction B 0.308210492 lossA 2.34437728 fraction A 0.0609898046\n",
      "step 918 loss 1.1724788 fisher_loss 0.30564332 triplet loss 0.866835475 l2_loss 4.53792143 fraction B 0.286420554 lossA 2.32431674 fraction A 0.0594577305\n",
      "step 919 loss 0.994330168 fisher_loss 0.304421753 triplet loss 0.689908385 l2_loss 4.54330492 fraction B 0.230109692 lossA 2.29726934 fraction A 0.0577412434\n",
      "step 920 loss 0.997859 fisher_loss 0.303630322 triplet loss 0.694228709 l2_loss 4.5496273 fraction B 0.29108566 lossA 2.28427529 fraction A 0.0568857752\n",
      "step 921 loss 0.962632775 fisher_loss 0.303054422 triplet loss 0.659578383 l2_loss 4.55625582 fraction B 0.201674581 lossA 2.26318479 fraction A 0.0550535209\n",
      "step 922 loss 0.83699429 fisher_loss 0.302303821 triplet loss 0.534690499 l2_loss 4.56361675 fraction B 0.260110289 lossA 2.24648571 fraction A 0.0538336858\n",
      "step 923 loss 1.09766424 fisher_loss 0.302510589 triplet loss 0.795153677 l2_loss 4.57328892 fraction B 0.298060715 lossA 2.20754313 fraction A 0.0506771319\n",
      "step 924 loss 1.17795658 fisher_loss 0.302377909 triplet loss 0.875578701 l2_loss 4.58239651 fraction B 0.290552229 lossA 2.19442964 fraction A 0.0501773693\n",
      "step 925 loss 1.07925665 fisher_loss 0.303359658 triplet loss 0.775896966 l2_loss 4.59267807 fraction B 0.293598622 lossA 2.16035748 fraction A 0.0480494313\n",
      "step 926 loss 1.06706703 fisher_loss 0.303927869 triplet loss 0.763139129 l2_loss 4.60251856 fraction B 0.363722235 lossA 2.12770557 fraction A 0.0463535935\n",
      "step 927 loss 1.62628698 fisher_loss 0.304479182 triplet loss 1.32180786 l2_loss 4.61128664 fraction B 0.353548 lossA 2.08850241 fraction A 0.0443413481\n",
      "step 928 loss 1.15653753 fisher_loss 0.304483593 triplet loss 0.852053881 l2_loss 4.61763859 fraction B 0.336671084 lossA 2.09572577 fraction A 0.0452008769\n",
      "step 929 loss 1.16805184 fisher_loss 0.305034757 triplet loss 0.863017082 l2_loss 4.62402201 fraction B 0.364440113 lossA 2.08516502 fraction A 0.0449715517\n",
      "step 930 loss 1.10778654 fisher_loss 0.305555 triplet loss 0.80223155 l2_loss 4.63029575 fraction B 0.311669052 lossA 2.12542033 fraction A 0.0478237271\n",
      "step 931 loss 1.10237134 fisher_loss 0.306429416 triplet loss 0.795941949 l2_loss 4.63760853 fraction B 0.308492303 lossA 2.1813252 fraction A 0.0516783632\n",
      "step 932 loss 1.07622278 fisher_loss 0.30761233 triplet loss 0.768610477 l2_loss 4.64523649 fraction B 0.325374633 lossA 2.21226311 fraction A 0.0539002977\n",
      "step 933 loss 1.06468105 fisher_loss 0.30806 triplet loss 0.756621 l2_loss 4.65307379 fraction B 0.334395289 lossA 2.23909807 fraction A 0.0560844541\n",
      "step 934 loss 1.10232103 fisher_loss 0.308431774 triplet loss 0.793889284 l2_loss 4.66067457 fraction B 0.334465414 lossA 2.24530792 fraction A 0.056108\n",
      "step 935 loss 1.24323034 fisher_loss 0.307824075 triplet loss 0.935406327 l2_loss 4.66690111 fraction B 0.350128114 lossA 2.23082066 fraction A 0.0546243563\n",
      "step 936 loss 1.23349619 fisher_loss 0.306908697 triplet loss 0.926587522 l2_loss 4.6720829 fraction B 0.457043558 lossA 2.19528699 fraction A 0.0513532273\n",
      "step 937 loss 0.953924298 fisher_loss 0.305465847 triplet loss 0.648458481 l2_loss 4.67573547 fraction B 0.228161201 lossA 2.18225145 fraction A 0.0502266325\n",
      "step 938 loss 1.0391109 fisher_loss 0.305044532 triplet loss 0.734066367 l2_loss 4.68086338 fraction B 0.291602254 lossA 2.16346645 fraction A 0.0487025604\n",
      "step 939 loss 1.05729687 fisher_loss 0.304421902 triplet loss 0.752875 l2_loss 4.68569279 fraction B 0.228378654 lossA 2.17817354 fraction A 0.0497110374\n",
      "step 940 loss 1.02897906 fisher_loss 0.304524511 triplet loss 0.724454582 l2_loss 4.69252634 fraction B 0.261309773 lossA 2.1854 fraction A 0.0502453595\n",
      "step 941 loss 0.887290657 fisher_loss 0.304444909 triplet loss 0.582845747 l2_loss 4.6993289 fraction B 0.200901806 lossA 2.18078089 fraction A 0.049491819\n",
      "step 942 loss 1.20810544 fisher_loss 0.304290205 triplet loss 0.90381521 l2_loss 4.70574045 fraction B 0.32908681 lossA 2.16158652 fraction A 0.0479026213\n",
      "step 943 loss 1.05209303 fisher_loss 0.303585529 triplet loss 0.7485075 l2_loss 4.71101284 fraction B 0.24241437 lossA 2.15937304 fraction A 0.0474447273\n",
      "step 944 loss 1.06801224 fisher_loss 0.303196669 triplet loss 0.764815629 l2_loss 4.71704531 fraction B 0.253828883 lossA 2.16192579 fraction A 0.0470806547\n",
      "step 945 loss 0.949742556 fisher_loss 0.302862912 triplet loss 0.646879613 l2_loss 4.7238903 fraction B 0.225454837 lossA 2.14431596 fraction A 0.0457399786\n",
      "step 946 loss 1.10557711 fisher_loss 0.302330315 triplet loss 0.803246796 l2_loss 4.73172855 fraction B 0.287598372 lossA 2.11930513 fraction A 0.0440957583\n",
      "step 947 loss 1.16498375 fisher_loss 0.301690102 triplet loss 0.863293648 l2_loss 4.73981 fraction B 0.315743864 lossA 2.14711952 fraction A 0.0456508361\n",
      "step 948 loss 1.12139523 fisher_loss 0.301942319 triplet loss 0.819452941 l2_loss 4.74913073 fraction B 0.31238544 lossA 2.13281083 fraction A 0.0446790531\n",
      "step 949 loss 0.876820445 fisher_loss 0.301128477 triplet loss 0.575692 l2_loss 4.75609303 fraction B 0.262431651 lossA 2.13047934 fraction A 0.0443708\n",
      "step 950 loss 1.01802325 fisher_loss 0.300860941 triplet loss 0.717162371 l2_loss 4.76451159 fraction B 0.239836037 lossA 2.15390539 fraction A 0.0457992405\n",
      "step 951 loss 0.956965923 fisher_loss 0.301494747 triplet loss 0.655471206 l2_loss 4.77524424 fraction B 0.253504127 lossA 2.18245435 fraction A 0.0479122922\n",
      "step 952 loss 0.965828 fisher_loss 0.302538037 triplet loss 0.663289964 l2_loss 4.78604126 fraction B 0.221613884 lossA 2.17569256 fraction A 0.0476476103\n",
      "step 953 loss 1.03310359 fisher_loss 0.302845508 triplet loss 0.730258048 l2_loss 4.79527473 fraction B 0.274653256 lossA 2.16128182 fraction A 0.0465200357\n",
      "step 954 loss 0.918437362 fisher_loss 0.302835196 triplet loss 0.615602136 l2_loss 4.80431747 fraction B 0.198261216 lossA 2.13399625 fraction A 0.0449269637\n",
      "step 955 loss 0.992549896 fisher_loss 0.303102195 triplet loss 0.689447701 l2_loss 4.81426 fraction B 0.239584193 lossA 2.07175016 fraction A 0.0412886478\n",
      "step 956 loss 1.03890181 fisher_loss 0.302538872 triplet loss 0.736362934 l2_loss 4.82277393 fraction B 0.323048264 lossA 2.00265932 fraction A 0.0377604105\n",
      "step 957 loss 1.05592644 fisher_loss 0.302015156 triplet loss 0.753911316 l2_loss 4.83057165 fraction B 0.329156071 lossA 1.92112148 fraction A 0.03377226\n",
      "step 958 loss 1.20641327 fisher_loss 0.301023662 triplet loss 0.905389667 l2_loss 4.83717918 fraction B 0.420386344 lossA 1.87557793 fraction A 0.0314236879\n",
      "step 959 loss 1.10398877 fisher_loss 0.300122172 triplet loss 0.803866565 l2_loss 4.84270954 fraction B 0.360590905 lossA 1.923491 fraction A 0.0339409821\n",
      "step 960 loss 1.41961145 fisher_loss 0.300532758 triplet loss 1.11907864 l2_loss 4.85026693 fraction B 0.338151664 lossA 2.03558803 fraction A 0.0403647162\n",
      "step 961 loss 0.976199389 fisher_loss 0.301838428 triplet loss 0.674360931 l2_loss 4.85889196 fraction B 0.390598506 lossA 2.19589162 fraction A 0.050616134\n",
      "step 962 loss 0.894192517 fisher_loss 0.305167615 triplet loss 0.589024901 l2_loss 4.8703146 fraction B 0.237262174 lossA 2.26604104 fraction A 0.0560385957\n",
      "step 963 loss 1.18369579 fisher_loss 0.307007253 triplet loss 0.8766886 l2_loss 4.88050318 fraction B 0.291890353 lossA 2.29978657 fraction A 0.0594599731\n",
      "step 964 loss 1.32873631 fisher_loss 0.308290392 triplet loss 1.02044594 l2_loss 4.88992071 fraction B 0.311544478 lossA 2.3162179 fraction A 0.0613712296\n",
      "step 965 loss 1.16443253 fisher_loss 0.308685899 triplet loss 0.855746686 l2_loss 4.89654684 fraction B 0.325186461 lossA 2.31323528 fraction A 0.0612170659\n",
      "step 966 loss 1.08443725 fisher_loss 0.30815497 triplet loss 0.776282251 l2_loss 4.9018774 fraction B 0.295877367 lossA 2.31763172 fraction A 0.0615989603\n",
      "step 967 loss 1.0790019 fisher_loss 0.307961404 triplet loss 0.771040499 l2_loss 4.90690041 fraction B 0.2904571 lossA 2.28831244 fraction A 0.0585523434\n",
      "step 968 loss 1.10210276 fisher_loss 0.306294382 triplet loss 0.795808375 l2_loss 4.91031408 fraction B 0.300059915 lossA 2.24807858 fraction A 0.0541621186\n",
      "step 969 loss 1.11638772 fisher_loss 0.303771585 triplet loss 0.812616169 l2_loss 4.91264 fraction B 0.323791087 lossA 2.21528625 fraction A 0.0508867167\n",
      "step 970 loss 1.21715236 fisher_loss 0.301883101 triplet loss 0.915269315 l2_loss 4.91563129 fraction B 0.403258562 lossA 2.15525818 fraction A 0.0461670458\n",
      "step 971 loss 1.02167654 fisher_loss 0.299642265 triplet loss 0.722034335 l2_loss 4.91753054 fraction B 0.263807565 lossA 2.11405563 fraction A 0.0431266204\n",
      "step 972 loss 1.20878386 fisher_loss 0.29856649 triplet loss 0.910217404 l2_loss 4.92168808 fraction B 0.257309347 lossA 2.09840155 fraction A 0.0420659147\n",
      "step 973 loss 1.11469746 fisher_loss 0.297987431 triplet loss 0.81671 l2_loss 4.92668 fraction B 0.29662481 lossA 2.13226986 fraction A 0.0439590178\n",
      "step 974 loss 1.2341944 fisher_loss 0.298061639 triplet loss 0.936132789 l2_loss 4.93275881 fraction B 0.284368753 lossA 2.17404 fraction A 0.0464083962\n",
      "step 975 loss 1.01087117 fisher_loss 0.298449308 triplet loss 0.712421894 l2_loss 4.93901 fraction B 0.354617029 lossA 2.24199557 fraction A 0.0507712029\n",
      "step 976 loss 0.996873736 fisher_loss 0.299369067 triplet loss 0.697504699 l2_loss 4.9462 fraction B 0.266829789 lossA 2.31262398 fraction A 0.0557559878\n",
      "step 977 loss 1.31473517 fisher_loss 0.300673276 triplet loss 1.01406193 l2_loss 4.95479059 fraction B 0.348896652 lossA 2.33213425 fraction A 0.0574156381\n",
      "step 978 loss 1.01168561 fisher_loss 0.301064759 triplet loss 0.710620821 l2_loss 4.96217728 fraction B 0.334343821 lossA 2.28041267 fraction A 0.0539971218\n",
      "step 979 loss 1.01752722 fisher_loss 0.299794525 triplet loss 0.717732728 l2_loss 4.96789503 fraction B 0.325934589 lossA 2.18475866 fraction A 0.0479993336\n",
      "step 980 loss 1.10901904 fisher_loss 0.298329264 triplet loss 0.810689747 l2_loss 4.97395182 fraction B 0.321662426 lossA 2.12394977 fraction A 0.0442204028\n",
      "step 981 loss 1.09474754 fisher_loss 0.297551692 triplet loss 0.797195852 l2_loss 4.9802947 fraction B 0.359550327 lossA 2.09928679 fraction A 0.0425631404\n",
      "step 982 loss 1.11087024 fisher_loss 0.297200233 triplet loss 0.81367 l2_loss 4.98589706 fraction B 0.33422631 lossA 2.13382554 fraction A 0.0449676402\n",
      "step 983 loss 0.995008 fisher_loss 0.297982901 triplet loss 0.697025061 l2_loss 4.99205685 fraction B 0.300147027 lossA 2.1885612 fraction A 0.0493368916\n",
      "step 984 loss 1.17362928 fisher_loss 0.299326122 triplet loss 0.874303102 l2_loss 4.99912453 fraction B 0.32522583 lossA 2.25316167 fraction A 0.0552948378\n",
      "step 985 loss 1.19827878 fisher_loss 0.300948709 triplet loss 0.897330105 l2_loss 5.00616789 fraction B 0.271339655 lossA 2.29214048 fraction A 0.0591077469\n",
      "step 986 loss 0.946907401 fisher_loss 0.30172047 triplet loss 0.645186961 l2_loss 5.01204824 fraction B 0.234953925 lossA 2.29768705 fraction A 0.0600831509\n",
      "step 987 loss 1.18277597 fisher_loss 0.30224812 triplet loss 0.880527854 l2_loss 5.01904 fraction B 0.263506949 lossA 2.27682972 fraction A 0.0584068708\n",
      "step 988 loss 1.13477218 fisher_loss 0.301687866 triplet loss 0.833084345 l2_loss 5.02431679 fraction B 0.320351154 lossA 2.22374749 fraction A 0.0539999828\n",
      "step 989 loss 1.15173507 fisher_loss 0.300108 triplet loss 0.851627052 l2_loss 5.02825 fraction B 0.324227184 lossA 2.16907597 fraction A 0.0495393761\n",
      "step 990 loss 1.18374324 fisher_loss 0.298233598 triplet loss 0.88550967 l2_loss 5.0312109 fraction B 0.259649128 lossA 2.15408731 fraction A 0.04832552\n",
      "step 991 loss 0.915389657 fisher_loss 0.297389209 triplet loss 0.618000448 l2_loss 5.03579426 fraction B 0.140633538 lossA 2.1667695 fraction A 0.0490705855\n",
      "step 992 loss 1.15991449 fisher_loss 0.297539264 triplet loss 0.862375259 l2_loss 5.04291773 fraction B 0.310116529 lossA 2.16803408 fraction A 0.0487280264\n",
      "step 993 loss 1.11813951 fisher_loss 0.297207087 triplet loss 0.820932448 l2_loss 5.04998589 fraction B 0.30488649 lossA 2.18645072 fraction A 0.0498320572\n",
      "step 994 loss 1.07239521 fisher_loss 0.297485054 triplet loss 0.774910152 l2_loss 5.05811596 fraction B 0.245705038 lossA 2.21797657 fraction A 0.0520869903\n",
      "step 995 loss 0.95720458 fisher_loss 0.298249453 triplet loss 0.658955157 l2_loss 5.06715298 fraction B 0.221959248 lossA 2.21440554 fraction A 0.0517710224\n",
      "step 996 loss 1.07078278 fisher_loss 0.298550427 triplet loss 0.772232354 l2_loss 5.0765214 fraction B 0.2218972 lossA 2.19696331 fraction A 0.0503481217\n",
      "step 997 loss 1.1681689 fisher_loss 0.298578829 triplet loss 0.869590044 l2_loss 5.08568192 fraction B 0.303463906 lossA 2.1920886 fraction A 0.0498357527\n",
      "step 998 loss 0.99041307 fisher_loss 0.29873234 triplet loss 0.691680729 l2_loss 5.09562683 fraction B 0.265829057 lossA 2.18514538 fraction A 0.04910025\n",
      "step 999 loss 1.27537584 fisher_loss 0.298754603 triplet loss 0.976621211 l2_loss 5.1047945 fraction B 0.299031824 lossA 2.19459629 fraction A 0.0499588363\n",
      "step 1000 loss 1.16851068 fisher_loss 0.299370736 triplet loss 0.869139969 l2_loss 5.11419344 fraction B 0.279913753 lossA 2.17783737 fraction A 0.0486781113\n",
      "step 1001 loss 1.27375054 fisher_loss 0.299103975 triplet loss 0.974646628 l2_loss 5.12166071 fraction B 0.354377717 lossA 2.18486 fraction A 0.0490563139\n",
      "step 1002 loss 0.926669657 fisher_loss 0.298981965 triplet loss 0.627687693 l2_loss 5.12904692 fraction B 0.154554829 lossA 2.1815412 fraction A 0.0489825271\n",
      "step 1003 loss 1.18012333 fisher_loss 0.299062759 triplet loss 0.881060541 l2_loss 5.13772917 fraction B 0.29515484 lossA 2.15333891 fraction A 0.0470867045\n",
      "step 1004 loss 1.07945895 fisher_loss 0.298479557 triplet loss 0.780979335 l2_loss 5.14433146 fraction B 0.336556166 lossA 2.11457682 fraction A 0.044816047\n",
      "step 1005 loss 1.0729028 fisher_loss 0.298152119 triplet loss 0.77475065 l2_loss 5.15126848 fraction B 0.336346418 lossA 2.08098054 fraction A 0.0427392945\n",
      "step 1006 loss 1.0556761 fisher_loss 0.297668219 triplet loss 0.758007884 l2_loss 5.15797234 fraction B 0.261699468 lossA 2.05875278 fraction A 0.0415173918\n",
      "step 1007 loss 0.989795208 fisher_loss 0.297371358 triplet loss 0.69242382 l2_loss 5.16431284 fraction B 0.217897654 lossA 2.05874538 fraction A 0.0415247083\n",
      "step 1008 loss 1.06705451 fisher_loss 0.297524333 triplet loss 0.769530237 l2_loss 5.17226934 fraction B 0.275582165 lossA 2.03101349 fraction A 0.0398864672\n",
      "step 1009 loss 1.16638064 fisher_loss 0.29727 triplet loss 0.869110703 l2_loss 5.1796093 fraction B 0.237229288 lossA 2.04274607 fraction A 0.0405693\n",
      "step 1010 loss 0.944524765 fisher_loss 0.297522724 triplet loss 0.647002041 l2_loss 5.18734646 fraction B 0.244441703 lossA 2.09838891 fraction A 0.043896392\n",
      "step 1011 loss 1.16920698 fisher_loss 0.298527926 triplet loss 0.870679 l2_loss 5.19716597 fraction B 0.38025409 lossA 2.12296677 fraction A 0.0455777384\n",
      "step 1012 loss 1.19837856 fisher_loss 0.299263835 triplet loss 0.899114668 l2_loss 5.20612431 fraction B 0.331279486 lossA 2.12171245 fraction A 0.0455873385\n",
      "step 1013 loss 1.04382145 fisher_loss 0.299099326 triplet loss 0.744722128 l2_loss 5.21280575 fraction B 0.232859209 lossA 2.1126616 fraction A 0.044966951\n",
      "step 1014 loss 1.08504283 fisher_loss 0.298819095 triplet loss 0.78622371 l2_loss 5.21936464 fraction B 0.303481847 lossA 2.098279 fraction A 0.0437275581\n",
      "step 1015 loss 1.14604235 fisher_loss 0.298092782 triplet loss 0.847949624 l2_loss 5.22531748 fraction B 0.315980643 lossA 2.07235169 fraction A 0.0418162681\n",
      "step 1016 loss 1.08074486 fisher_loss 0.296994865 triplet loss 0.78375 l2_loss 5.23074293 fraction B 0.322767675 lossA 2.04005933 fraction A 0.0395735\n",
      "step 1017 loss 1.16701794 fisher_loss 0.295902342 triplet loss 0.871115565 l2_loss 5.2361064 fraction B 0.324741215 lossA 1.98865449 fraction A 0.0362599976\n",
      "step 1018 loss 1.03314769 fisher_loss 0.294074148 triplet loss 0.739073575 l2_loss 5.2398243 fraction B 0.383763313 lossA 1.9898231 fraction A 0.0357957296\n",
      "step 1019 loss 1.01091802 fisher_loss 0.293059677 triplet loss 0.717858374 l2_loss 5.24547338 fraction B 0.265486538 lossA 2.04519343 fraction A 0.0386746712\n",
      "step 1020 loss 1.08737767 fisher_loss 0.29306671 triplet loss 0.794311 l2_loss 5.25199461 fraction B 0.266515732 lossA 2.11468077 fraction A 0.0426713377\n",
      "step 1021 loss 0.997351527 fisher_loss 0.293603331 triplet loss 0.703748167 l2_loss 5.25878716 fraction B 0.300853282 lossA 2.12639141 fraction A 0.0433325134\n",
      "step 1022 loss 1.08403921 fisher_loss 0.293253 triplet loss 0.790786207 l2_loss 5.26436138 fraction B 0.283072859 lossA 2.14577723 fraction A 0.0442940444\n",
      "step 1023 loss 1.20887828 fisher_loss 0.29298678 triplet loss 0.915891528 l2_loss 5.2702508 fraction B 0.312794417 lossA 2.22774935 fraction A 0.0493463464\n",
      "step 1024 loss 0.954443812 fisher_loss 0.294194251 triplet loss 0.660249531 l2_loss 5.27785969 fraction B 0.209011972 lossA 2.28807211 fraction A 0.0533362404\n",
      "step 1025 loss 1.08629096 fisher_loss 0.295162052 triplet loss 0.791128933 l2_loss 5.28529549 fraction B 0.328352541 lossA 2.28453732 fraction A 0.0529733263\n",
      "step 1026 loss 1.20543373 fisher_loss 0.295029223 triplet loss 0.910404503 l2_loss 5.2915287 fraction B 0.261772692 lossA 2.2914536 fraction A 0.0538841449\n",
      "step 1027 loss 1.17580581 fisher_loss 0.296049654 triplet loss 0.879756153 l2_loss 5.30039263 fraction B 0.306753039 lossA 2.26285243 fraction A 0.0525444113\n",
      "step 1028 loss 0.993782401 fisher_loss 0.296324074 triplet loss 0.697458327 l2_loss 5.30805588 fraction B 0.229107454 lossA 2.20858955 fraction A 0.0493499711\n",
      "step 1029 loss 0.945494056 fisher_loss 0.296316236 triplet loss 0.64917779 l2_loss 5.31606531 fraction B 0.231206909 lossA 2.14946198 fraction A 0.0457474068\n",
      "step 1030 loss 1.04600155 fisher_loss 0.296130627 triplet loss 0.749870896 l2_loss 5.32379866 fraction B 0.255880207 lossA 2.07069683 fraction A 0.0407631285\n",
      "step 1031 loss 0.922550797 fisher_loss 0.295120806 triplet loss 0.627429962 l2_loss 5.33098078 fraction B 0.239935383 lossA 2.03586411 fraction A 0.0383988731\n",
      "step 1032 loss 1.09256482 fisher_loss 0.294888139 triplet loss 0.797676682 l2_loss 5.33983326 fraction B 0.338705599 lossA 2.04098797 fraction A 0.0385796241\n",
      "step 1033 loss 0.999961913 fisher_loss 0.295199633 triplet loss 0.70476228 l2_loss 5.34941721 fraction B 0.231385 lossA 2.05361 fraction A 0.0391510725\n",
      "step 1034 loss 0.90059346 fisher_loss 0.295422375 triplet loss 0.605171084 l2_loss 5.35839844 fraction B 0.220823273 lossA 2.08066773 fraction A 0.0406595655\n",
      "step 1035 loss 0.851932645 fisher_loss 0.296261489 triplet loss 0.555671155 l2_loss 5.36893 fraction B 0.227312595 lossA 2.15473509 fraction A 0.0451790728\n",
      "step 1036 loss 1.14914525 fisher_loss 0.298459679 triplet loss 0.850685537 l2_loss 5.38217449 fraction B 0.351662576 lossA 2.21059442 fraction A 0.0484674759\n",
      "step 1037 loss 1.20110691 fisher_loss 0.300211936 triplet loss 0.900895 l2_loss 5.39412546 fraction B 0.363312572 lossA 2.25760341 fraction A 0.051116731\n",
      "step 1038 loss 1.093889 fisher_loss 0.301301628 triplet loss 0.79258734 l2_loss 5.40476894 fraction B 0.235185966 lossA 2.27288914 fraction A 0.0517023802\n",
      "step 1039 loss 1.03285921 fisher_loss 0.301545739 triplet loss 0.731313467 l2_loss 5.41428614 fraction B 0.408648878 lossA 2.2126174 fraction A 0.0478708893\n",
      "step 1040 loss 0.985377967 fisher_loss 0.300281525 triplet loss 0.685096443 l2_loss 5.42245483 fraction B 0.22185 lossA 2.11475348 fraction A 0.0421596207\n",
      "step 1041 loss 1.06987262 fisher_loss 0.298420668 triplet loss 0.77145195 l2_loss 5.43066216 fraction B 0.281098247 lossA 2.04216123 fraction A 0.0378349237\n",
      "step 1042 loss 0.97404182 fisher_loss 0.297074109 triplet loss 0.67696774 l2_loss 5.43928766 fraction B 0.301891565 lossA 2.01404524 fraction A 0.0361046\n",
      "step 1043 loss 1.07540774 fisher_loss 0.296346813 triplet loss 0.77906096 l2_loss 5.44872284 fraction B 0.396555334 lossA 1.99973023 fraction A 0.0352079086\n",
      "step 1044 loss 1.27843559 fisher_loss 0.295808047 triplet loss 0.982627511 l2_loss 5.45761871 fraction B 0.288212389 lossA 1.99458683 fraction A 0.0347440057\n",
      "step 1045 loss 1.19363832 fisher_loss 0.295110494 triplet loss 0.898527861 l2_loss 5.46526861 fraction B 0.423444778 lossA 2.02518487 fraction A 0.0361851975\n",
      "step 1046 loss 0.996780336 fisher_loss 0.294696391 triplet loss 0.702083945 l2_loss 5.47214 fraction B 0.287055016 lossA 2.12463903 fraction A 0.0419750661\n",
      "step 1047 loss 1.19957972 fisher_loss 0.295929283 triplet loss 0.903650403 l2_loss 5.48184252 fraction B 0.290023148 lossA 2.21695471 fraction A 0.0480588824\n",
      "step 1048 loss 1.00820506 fisher_loss 0.297347069 triplet loss 0.710858 l2_loss 5.49135 fraction B 0.279400617 lossA 2.27675462 fraction A 0.0523992665\n",
      "step 1049 loss 0.920635581 fisher_loss 0.298424751 triplet loss 0.62221086 l2_loss 5.50039768 fraction B 0.178952411 lossA 2.26890731 fraction A 0.051646959\n",
      "step 1050 loss 1.07755089 fisher_loss 0.298084646 triplet loss 0.779466271 l2_loss 5.50834656 fraction B 0.294021666 lossA 2.17811 fraction A 0.0452990048\n",
      "step 1051 loss 1.05502915 fisher_loss 0.296219707 triplet loss 0.758809507 l2_loss 5.51414204 fraction B 0.273534149 lossA 2.10340595 fraction A 0.040639028\n",
      "step 1052 loss 0.980356336 fisher_loss 0.29502371 triplet loss 0.685332596 l2_loss 5.52012587 fraction B 0.27989918 lossA 2.09676313 fraction A 0.040052332\n",
      "step 1053 loss 0.982218444 fisher_loss 0.29489547 triplet loss 0.687323 l2_loss 5.52812767 fraction B 0.322789192 lossA 2.09418 fraction A 0.0399548896\n",
      "step 1054 loss 0.929634273 fisher_loss 0.29499352 triplet loss 0.634640753 l2_loss 5.5356288 fraction B 0.228068799 lossA 2.19237518 fraction A 0.0455432907\n",
      "step 1055 loss 0.881791353 fisher_loss 0.296800107 triplet loss 0.584991276 l2_loss 5.54636669 fraction B 0.189280987 lossA 2.29821038 fraction A 0.0519793741\n",
      "step 1056 loss 0.988630056 fisher_loss 0.299366117 triplet loss 0.68926394 l2_loss 5.55868959 fraction B 0.230326578 lossA 2.34273648 fraction A 0.0548955165\n",
      "step 1057 loss 1.16187572 fisher_loss 0.300620884 triplet loss 0.861254811 l2_loss 5.56848812 fraction B 0.222811565 lossA 2.33975244 fraction A 0.054316137\n",
      "step 1058 loss 1.17367291 fisher_loss 0.300635248 triplet loss 0.873037696 l2_loss 5.57567596 fraction B 0.32913056 lossA 2.30643058 fraction A 0.0517821796\n",
      "step 1059 loss 1.01976895 fisher_loss 0.300207853 triplet loss 0.7195611 l2_loss 5.5819068 fraction B 0.277356595 lossA 2.22047281 fraction A 0.0459186323\n",
      "step 1060 loss 1.09846592 fisher_loss 0.298970193 triplet loss 0.799495697 l2_loss 5.58754683 fraction B 0.257861763 lossA 2.1950829 fraction A 0.0441870764\n",
      "step 1061 loss 1.05369878 fisher_loss 0.299165875 triplet loss 0.754532933 l2_loss 5.59512949 fraction B 0.294438064 lossA 2.16719055 fraction A 0.0424031094\n",
      "step 1062 loss 0.9357844 fisher_loss 0.299106658 triplet loss 0.636677742 l2_loss 5.6023612 fraction B 0.201056495 lossA 2.1575036 fraction A 0.0417065099\n",
      "step 1063 loss 1.14974391 fisher_loss 0.299212873 triplet loss 0.850531042 l2_loss 5.60979128 fraction B 0.323739469 lossA 2.15478373 fraction A 0.0414235033\n",
      "step 1064 loss 1.19882143 fisher_loss 0.299197048 triplet loss 0.899624407 l2_loss 5.6171937 fraction B 0.300611824 lossA 2.16433239 fraction A 0.0420527644\n",
      "step 1065 loss 1.01904666 fisher_loss 0.299146444 triplet loss 0.71990025 l2_loss 5.62400293 fraction B 0.250113368 lossA 2.17044401 fraction A 0.0424049571\n",
      "step 1066 loss 1.20604491 fisher_loss 0.29878515 triplet loss 0.907259762 l2_loss 5.6311636 fraction B 0.334681898 lossA 2.16185951 fraction A 0.0418489389\n",
      "step 1067 loss 1.165434 fisher_loss 0.298095196 triplet loss 0.867338777 l2_loss 5.63753176 fraction B 0.338666141 lossA 2.15580225 fraction A 0.041521702\n",
      "step 1068 loss 0.966267 fisher_loss 0.297239751 triplet loss 0.669027209 l2_loss 5.64404488 fraction B 0.261488229 lossA 2.17672396 fraction A 0.0427301638\n",
      "step 1069 loss 1.08640802 fisher_loss 0.297354341 triplet loss 0.789053679 l2_loss 5.65288067 fraction B 0.246114433 lossA 2.23499298 fraction A 0.0464015529\n",
      "step 1070 loss 0.958189666 fisher_loss 0.298361659 triplet loss 0.659828 l2_loss 5.66358519 fraction B 0.230109304 lossA 2.27574039 fraction A 0.0491994657\n",
      "step 1071 loss 1.09362888 fisher_loss 0.299287677 triplet loss 0.794341147 l2_loss 5.67399073 fraction B 0.289526582 lossA 2.29276228 fraction A 0.0504540019\n",
      "step 1072 loss 1.20803714 fisher_loss 0.29943049 triplet loss 0.908606589 l2_loss 5.68334389 fraction B 0.295052439 lossA 2.27580118 fraction A 0.0493025891\n",
      "step 1073 loss 0.973546326 fisher_loss 0.298567295 triplet loss 0.674979031 l2_loss 5.69083929 fraction B 0.313750267 lossA 2.28090644 fraction A 0.0497629791\n",
      "step 1074 loss 1.01326334 fisher_loss 0.298610359 triplet loss 0.714653 l2_loss 5.69980097 fraction B 0.237880468 lossA 2.2743237 fraction A 0.0493748561\n",
      "step 1075 loss 1.08717275 fisher_loss 0.298484445 triplet loss 0.788688362 l2_loss 5.70886183 fraction B 0.346181065 lossA 2.24515605 fraction A 0.0474290438\n",
      "step 1076 loss 0.849841595 fisher_loss 0.298090786 triplet loss 0.551750839 l2_loss 5.71785736 fraction B 0.168641627 lossA 2.25995612 fraction A 0.0489867292\n",
      "step 1077 loss 1.2694149 fisher_loss 0.299138814 triplet loss 0.970276117 l2_loss 5.72917938 fraction B 0.202547908 lossA 2.29631805 fraction A 0.0523701794\n",
      "step 1078 loss 1.31072128 fisher_loss 0.300670505 triplet loss 1.01005077 l2_loss 5.7402935 fraction B 0.353121251 lossA 2.31688142 fraction A 0.055072464\n",
      "step 1079 loss 1.15550518 fisher_loss 0.301906109 triplet loss 0.853599072 l2_loss 5.75019026 fraction B 0.295649499 lossA 2.30636287 fraction A 0.0549614057\n",
      "step 1080 loss 1.33981931 fisher_loss 0.302100897 triplet loss 1.03771842 l2_loss 5.75886345 fraction B 0.329810351 lossA 2.30153108 fraction A 0.0553864799\n",
      "step 1081 loss 1.10809922 fisher_loss 0.302292436 triplet loss 0.805806816 l2_loss 5.76614189 fraction B 0.265927166 lossA 2.24239588 fraction A 0.051148098\n",
      "step 1082 loss 1.17678893 fisher_loss 0.300663561 triplet loss 0.876125336 l2_loss 5.7698307 fraction B 0.355631471 lossA 2.12360358 fraction A 0.0434072\n",
      "step 1083 loss 0.994621873 fisher_loss 0.298041254 triplet loss 0.696580648 l2_loss 5.77116394 fraction B 0.28495276 lossA 2.00263143 fraction A 0.0366882943\n",
      "step 1084 loss 1.04893875 fisher_loss 0.296019793 triplet loss 0.752918959 l2_loss 5.77293539 fraction B 0.309249789 lossA 1.91461861 fraction A 0.0320491828\n",
      "step 1085 loss 1.04453623 fisher_loss 0.294512123 triplet loss 0.75002408 l2_loss 5.77615547 fraction B 0.362999916 lossA 1.91524267 fraction A 0.031956017\n",
      "step 1086 loss 1.0303911 fisher_loss 0.294237345 triplet loss 0.736153781 l2_loss 5.78184032 fraction B 0.305951446 lossA 2.00956202 fraction A 0.0363575071\n",
      "step 1087 loss 1.04038894 fisher_loss 0.294855386 triplet loss 0.745533586 l2_loss 5.78898716 fraction B 0.295760155 lossA 2.1494143 fraction A 0.0448882394\n",
      "step 1088 loss 1.02444911 fisher_loss 0.296427161 triplet loss 0.728022 l2_loss 5.79765415 fraction B 0.32601887 lossA 2.27388 fraction A 0.0539326109\n",
      "step 1089 loss 1.26392388 fisher_loss 0.298336178 triplet loss 0.965587676 l2_loss 5.80615044 fraction B 0.291249484 lossA 2.37844777 fraction A 0.0622705668\n",
      "step 1090 loss 1.01091623 fisher_loss 0.300157487 triplet loss 0.710758746 l2_loss 5.81398916 fraction B 0.275044471 lossA 2.43399096 fraction A 0.0664263219\n",
      "step 1091 loss 1.16375625 fisher_loss 0.300982922 triplet loss 0.862773299 l2_loss 5.82058239 fraction B 0.266030461 lossA 2.43654847 fraction A 0.0656885728\n",
      "step 1092 loss 1.07331395 fisher_loss 0.300314 triplet loss 0.772999883 l2_loss 5.82499409 fraction B 0.338126391 lossA 2.35371828 fraction A 0.0584060028\n",
      "step 1093 loss 1.0882622 fisher_loss 0.297806054 triplet loss 0.790456176 l2_loss 5.82723951 fraction B 0.327271938 lossA 2.21837306 fraction A 0.0479948781\n",
      "step 1094 loss 1.11799574 fisher_loss 0.294623107 triplet loss 0.823372662 l2_loss 5.82837439 fraction B 0.296294749 lossA 2.0089767 fraction A 0.035640154\n",
      "step 1095 loss 0.972921789 fisher_loss 0.291511893 triplet loss 0.681409895 l2_loss 5.82897234 fraction B 0.234381899 lossA 1.8495909 fraction A 0.0280893892\n",
      "step 1096 loss 1.01014316 fisher_loss 0.289509237 triplet loss 0.720633924 l2_loss 5.8309536 fraction B 0.330826133 lossA 1.81386483 fraction A 0.0266623236\n",
      "step 1097 loss 0.943988562 fisher_loss 0.289134443 triplet loss 0.654854119 l2_loss 5.83553934 fraction B 0.264759392 lossA 1.96811867 fraction A 0.033211641\n",
      "step 1098 loss 1.03776217 fisher_loss 0.290768504 triplet loss 0.746993661 l2_loss 5.84542513 fraction B 0.280675113 lossA 2.11214876 fraction A 0.0408813208\n",
      "step 1099 loss 1.12745595 fisher_loss 0.292871594 triplet loss 0.834584296 l2_loss 5.85565186 fraction B 0.300628036 lossA 2.27426124 fraction A 0.0512900911\n",
      "step 1100 loss 1.03799713 fisher_loss 0.296183795 triplet loss 0.741813302 l2_loss 5.86697674 fraction B 0.320515901 lossA 2.35425544 fraction A 0.0573776066\n",
      "step 1101 loss 1.22806501 fisher_loss 0.298264652 triplet loss 0.929800332 l2_loss 5.87600374 fraction B 0.403913677 lossA 2.38653207 fraction A 0.0600672476\n",
      "step 1102 loss 1.1854924 fisher_loss 0.299385101 triplet loss 0.886107326 l2_loss 5.88297129 fraction B 0.256640345 lossA 2.3433733 fraction A 0.0569192804\n",
      "step 1103 loss 1.06827712 fisher_loss 0.2987324 triplet loss 0.769544721 l2_loss 5.88857508 fraction B 0.267540216 lossA 2.26746082 fraction A 0.0511650145\n",
      "step 1104 loss 1.29354668 fisher_loss 0.297023624 triplet loss 0.996523 l2_loss 5.89364719 fraction B 0.322659701 lossA 2.16818261 fraction A 0.0444480255\n",
      "step 1105 loss 1.44562292 fisher_loss 0.294930458 triplet loss 1.15069246 l2_loss 5.89821815 fraction B 0.331090122 lossA 2.05654883 fraction A 0.0379028395\n",
      "step 1106 loss 0.990657568 fisher_loss 0.29311952 triplet loss 0.697538078 l2_loss 5.90318298 fraction B 0.225090891 lossA 2.00931787 fraction A 0.0361528844\n",
      "step 1107 loss 0.986310363 fisher_loss 0.292823076 triplet loss 0.693487287 l2_loss 5.91074133 fraction B 0.217715427 lossA 2.01792 fraction A 0.0374078192\n",
      "step 1108 loss 0.9352777 fisher_loss 0.293625504 triplet loss 0.641652167 l2_loss 5.92017126 fraction B 0.219942376 lossA 2.12845969 fraction A 0.0442722365\n",
      "step 1109 loss 0.927529573 fisher_loss 0.296317339 triplet loss 0.631212234 l2_loss 5.93265533 fraction B 0.204450592 lossA 2.25216079 fraction A 0.0520188585\n",
      "step 1110 loss 1.0503453 fisher_loss 0.299683 triplet loss 0.750662327 l2_loss 5.94626951 fraction B 0.241071433 lossA 2.32304049 fraction A 0.0570709817\n",
      "step 1111 loss 1.12645352 fisher_loss 0.301894337 triplet loss 0.824559152 l2_loss 5.95815659 fraction B 0.290685326 lossA 2.34880257 fraction A 0.0590810142\n",
      "step 1112 loss 1.09785521 fisher_loss 0.302675366 triplet loss 0.795179844 l2_loss 5.96821117 fraction B 0.328228861 lossA 2.32089591 fraction A 0.0568325967\n",
      "step 1113 loss 1.03279853 fisher_loss 0.301816881 triplet loss 0.730981708 l2_loss 5.97631693 fraction B 0.306650758 lossA 2.19141054 fraction A 0.0482721254\n",
      "step 1114 loss 0.996601224 fisher_loss 0.298718125 triplet loss 0.697883129 l2_loss 5.981884 fraction B 0.291926384 lossA 2.01411414 fraction A 0.0373893827\n",
      "step 1115 loss 1.21393287 fisher_loss 0.295430303 triplet loss 0.918502569 l2_loss 5.98639202 fraction B 0.327353507 lossA 1.85606885 fraction A 0.0289651453\n",
      "step 1116 loss 1.11618948 fisher_loss 0.293240756 triplet loss 0.822948694 l2_loss 5.98995066 fraction B 0.346222937 lossA 1.81043303 fraction A 0.0267047044\n",
      "step 1117 loss 1.11427379 fisher_loss 0.292727739 triplet loss 0.821546078 l2_loss 5.99499559 fraction B 0.350962937 lossA 1.89853871 fraction A 0.029837098\n",
      "step 1118 loss 0.950479627 fisher_loss 0.293279 triplet loss 0.657200634 l2_loss 6.00204897 fraction B 0.29373014 lossA 2.07344103 fraction A 0.0381878\n",
      "step 1119 loss 0.896676898 fisher_loss 0.295387506 triplet loss 0.601289392 l2_loss 6.01264858 fraction B 0.226330861 lossA 2.20716834 fraction A 0.0471761748\n",
      "step 1120 loss 0.839891613 fisher_loss 0.297643602 triplet loss 0.542248 l2_loss 6.02215 fraction B 0.213175893 lossA 2.33588719 fraction A 0.0560438484\n",
      "step 1121 loss 1.22583878 fisher_loss 0.300370127 triplet loss 0.925468683 l2_loss 6.03273392 fraction B 0.353199422 lossA 2.40590572 fraction A 0.0604273751\n",
      "step 1122 loss 1.08630145 fisher_loss 0.301610202 triplet loss 0.784691215 l2_loss 6.0406208 fraction B 0.211120307 lossA 2.386343 fraction A 0.0579206571\n",
      "step 1123 loss 1.01670921 fisher_loss 0.300716072 triplet loss 0.715993106 l2_loss 6.04611063 fraction B 0.254811525 lossA 2.32061744 fraction A 0.0524122\n",
      "step 1124 loss 1.04369402 fisher_loss 0.299278051 triplet loss 0.744415939 l2_loss 6.05162 fraction B 0.282238096 lossA 2.18956017 fraction A 0.0441019163\n",
      "step 1125 loss 1.06677818 fisher_loss 0.297332972 triplet loss 0.76944524 l2_loss 6.05649471 fraction B 0.272723436 lossA 2.0002079 fraction A 0.0339706875\n",
      "step 1126 loss 0.874249935 fisher_loss 0.295295089 triplet loss 0.578954875 l2_loss 6.05974483 fraction B 0.247536227 lossA 1.88486922 fraction A 0.0289916247\n",
      "step 1127 loss 0.975637317 fisher_loss 0.294909477 triplet loss 0.680727839 l2_loss 6.06696224 fraction B 0.314248174 lossA 1.92401516 fraction A 0.0307418685\n",
      "step 1128 loss 1.15086257 fisher_loss 0.295766681 triplet loss 0.855095923 l2_loss 6.07746029 fraction B 0.256781936 lossA 1.99655259 fraction A 0.0341683179\n",
      "step 1129 loss 1.19929588 fisher_loss 0.297156841 triplet loss 0.902139 l2_loss 6.08897161 fraction B 0.265700877 lossA 2.08648157 fraction A 0.03870463\n",
      "step 1130 loss 0.99649018 fisher_loss 0.298933029 triplet loss 0.697557151 l2_loss 6.10061312 fraction B 0.247443601 lossA 2.25037408 fraction A 0.0486059189\n",
      "step 1131 loss 1.23958242 fisher_loss 0.302405089 triplet loss 0.93717736 l2_loss 6.11511087 fraction B 0.307249576 lossA 2.35968661 fraction A 0.0568748713\n",
      "step 1132 loss 1.2080195 fisher_loss 0.30543226 triplet loss 0.902587235 l2_loss 6.12831497 fraction B 0.257609814 lossA 2.40600061 fraction A 0.0607960857\n",
      "step 1133 loss 1.05769706 fisher_loss 0.307197779 triplet loss 0.750499308 l2_loss 6.13952208 fraction B 0.39290005 lossA 2.40616846 fraction A 0.0608165525\n",
      "step 1134 loss 1.02666247 fisher_loss 0.307504594 triplet loss 0.719157875 l2_loss 6.14817762 fraction B 0.265161425 lossA 2.33212161 fraction A 0.0548798703\n",
      "step 1135 loss 1.17701697 fisher_loss 0.305342078 triplet loss 0.871674955 l2_loss 6.15379906 fraction B 0.214045703 lossA 2.22762346 fraction A 0.0472200029\n",
      "step 1136 loss 0.914300859 fisher_loss 0.30253613 triplet loss 0.611764729 l2_loss 6.15842247 fraction B 0.171012729 lossA 2.14981675 fraction A 0.0424147025\n",
      "step 1137 loss 1.13746035 fisher_loss 0.300885856 triplet loss 0.836574495 l2_loss 6.16426086 fraction B 0.327314645 lossA 2.08807135 fraction A 0.038812682\n",
      "step 1138 loss 0.938612819 fisher_loss 0.29939577 triplet loss 0.639217079 l2_loss 6.16958666 fraction B 0.222123891 lossA 2.03998804 fraction A 0.0361833125\n",
      "step 1139 loss 0.955314755 fisher_loss 0.298098207 triplet loss 0.657216549 l2_loss 6.17573786 fraction B 0.275301367 lossA 2.04479122 fraction A 0.0361918956\n",
      "step 1140 loss 1.03579605 fisher_loss 0.297704667 triplet loss 0.738091409 l2_loss 6.18335629 fraction B 0.295168459 lossA 2.09006476 fraction A 0.038288936\n",
      "step 1141 loss 1.12605035 fisher_loss 0.297771543 triplet loss 0.82827884 l2_loss 6.19100761 fraction B 0.296019167 lossA 2.14173794 fraction A 0.0409569182\n",
      "step 1142 loss 1.05755734 fisher_loss 0.298202574 triplet loss 0.75935477 l2_loss 6.19952 fraction B 0.253246754 lossA 2.22735882 fraction A 0.0454482436\n",
      "step 1143 loss 1.20196557 fisher_loss 0.299103439 triplet loss 0.902862191 l2_loss 6.2080822 fraction B 0.223319575 lossA 2.29459929 fraction A 0.0492596664\n",
      "step 1144 loss 1.33447111 fisher_loss 0.299928665 triplet loss 1.03454244 l2_loss 6.21594095 fraction B 0.391447097 lossA 2.33769965 fraction A 0.0519308709\n",
      "step 1145 loss 1.0798744 fisher_loss 0.300136626 triplet loss 0.779737771 l2_loss 6.22254801 fraction B 0.273551255 lossA 2.38540196 fraction A 0.0550795309\n",
      "step 1146 loss 1.11998343 fisher_loss 0.300535142 triplet loss 0.819448292 l2_loss 6.2289691 fraction B 0.318367094 lossA 2.3695004 fraction A 0.0537902564\n",
      "step 1147 loss 1.17989826 fisher_loss 0.299440682 triplet loss 0.88045752 l2_loss 6.23368931 fraction B 0.148449272 lossA 2.34150219 fraction A 0.0517170504\n",
      "step 1148 loss 1.1365962 fisher_loss 0.298421323 triplet loss 0.83817482 l2_loss 6.23952055 fraction B 0.298405319 lossA 2.25950384 fraction A 0.0467009321\n",
      "step 1149 loss 1.09864664 fisher_loss 0.296757 triplet loss 0.801889598 l2_loss 6.24444866 fraction B 0.302084178 lossA 2.17398691 fraction A 0.0419660844\n",
      "step 1150 loss 0.929929495 fisher_loss 0.295391381 triplet loss 0.634538114 l2_loss 6.24998 fraction B 0.296907037 lossA 2.12068033 fraction A 0.0389254801\n",
      "step 1151 loss 1.04241288 fisher_loss 0.294572085 triplet loss 0.747840762 l2_loss 6.25564814 fraction B 0.305530608 lossA 2.10414863 fraction A 0.0379697084\n",
      "step 1152 loss 1.09972274 fisher_loss 0.29430303 triplet loss 0.805419683 l2_loss 6.26223803 fraction B 0.308905661 lossA 2.11354852 fraction A 0.0386345759\n",
      "step 1153 loss 0.911845744 fisher_loss 0.294391751 triplet loss 0.617454 l2_loss 6.26905203 fraction B 0.196407512 lossA 2.13569355 fraction A 0.0401742905\n",
      "step 1154 loss 1.11800194 fisher_loss 0.294551969 triplet loss 0.82344991 l2_loss 6.27611446 fraction B 0.278066874 lossA 2.15644717 fraction A 0.0417545065\n",
      "step 1155 loss 1.15528083 fisher_loss 0.29450056 triplet loss 0.860780239 l2_loss 6.28235674 fraction B 0.285119742 lossA 2.22100186 fraction A 0.0455562212\n",
      "step 1156 loss 1.11956573 fisher_loss 0.294774622 triplet loss 0.824791133 l2_loss 6.28860617 fraction B 0.404818773 lossA 2.26582289 fraction A 0.0481560342\n",
      "step 1157 loss 1.05802333 fisher_loss 0.294800073 triplet loss 0.76322329 l2_loss 6.29343176 fraction B 0.272408038 lossA 2.27062082 fraction A 0.0483506583\n",
      "step 1158 loss 1.1307193 fisher_loss 0.294499665 triplet loss 0.836219668 l2_loss 6.29825449 fraction B 0.228647545 lossA 2.2386651 fraction A 0.0462687574\n",
      "step 1159 loss 1.17940593 fisher_loss 0.293608427 triplet loss 0.88579756 l2_loss 6.30235529 fraction B 0.337149441 lossA 2.18644094 fraction A 0.0432592332\n",
      "step 1160 loss 1.35855865 fisher_loss 0.292498946 triplet loss 1.06605971 l2_loss 6.3058691 fraction B 0.359395742 lossA 2.1203742 fraction A 0.0394316912\n",
      "step 1161 loss 1.0820682 fisher_loss 0.290909141 triplet loss 0.791159034 l2_loss 6.30793047 fraction B 0.239707693 lossA 2.05503345 fraction A 0.0360434577\n",
      "step 1162 loss 1.12543523 fisher_loss 0.289299458 triplet loss 0.836135745 l2_loss 6.30985737 fraction B 0.316669405 lossA 1.98765337 fraction A 0.0326234922\n",
      "step 1163 loss 0.939208865 fisher_loss 0.287816048 triplet loss 0.651392817 l2_loss 6.31183434 fraction B 0.256153345 lossA 2.01424623 fraction A 0.0338504277\n",
      "step 1164 loss 0.945168495 fisher_loss 0.287920237 triplet loss 0.657248259 l2_loss 6.3177104 fraction B 0.3088063 lossA 2.11323 fraction A 0.0389572456\n",
      "step 1165 loss 0.887600541 fisher_loss 0.289412886 triplet loss 0.598187625 l2_loss 6.32620668 fraction B 0.167982027 lossA 2.24253702 fraction A 0.0472275019\n",
      "step 1166 loss 1.25999331 fisher_loss 0.292557776 triplet loss 0.967435539 l2_loss 6.33781052 fraction B 0.470830232 lossA 2.31641126 fraction A 0.0526859351\n",
      "step 1167 loss 1.17907965 fisher_loss 0.294713855 triplet loss 0.884365797 l2_loss 6.34669065 fraction B 0.291842461 lossA 2.35115218 fraction A 0.0556835048\n",
      "step 1168 loss 0.950167716 fisher_loss 0.296197 triplet loss 0.653970718 l2_loss 6.35407877 fraction B 0.200524136 lossA 2.32342052 fraction A 0.0539193526\n",
      "step 1169 loss 1.00640368 fisher_loss 0.295946062 triplet loss 0.710457683 l2_loss 6.35974932 fraction B 0.267669082 lossA 2.25363183 fraction A 0.0491376333\n",
      "step 1170 loss 1.09787071 fisher_loss 0.295113534 triplet loss 0.802757204 l2_loss 6.36449718 fraction B 0.342958808 lossA 2.14086509 fraction A 0.0419105552\n",
      "step 1171 loss 1.01061583 fisher_loss 0.293489814 triplet loss 0.717126071 l2_loss 6.36828756 fraction B 0.219674811 lossA 2.00043154 fraction A 0.0340992399\n",
      "step 1172 loss 0.980801702 fisher_loss 0.291597575 triplet loss 0.689204097 l2_loss 6.37146187 fraction B 0.248567775 lossA 1.92728698 fraction A 0.030624615\n",
      "step 1173 loss 1.08530664 fisher_loss 0.290931791 triplet loss 0.794374883 l2_loss 6.37684727 fraction B 0.26502648 lossA 1.9120599 fraction A 0.0299354792\n",
      "step 1174 loss 0.795031607 fisher_loss 0.291171968 triplet loss 0.503859639 l2_loss 6.38430643 fraction B 0.278730661 lossA 2.04605818 fraction A 0.0362476446\n",
      "step 1175 loss 1.17472327 fisher_loss 0.293784946 triplet loss 0.880938292 l2_loss 6.39701223 fraction B 0.254098028 lossA 2.19430709 fraction A 0.0455272086\n",
      "step 1176 loss 1.03665853 fisher_loss 0.29754588 triplet loss 0.739112616 l2_loss 6.41080046 fraction B 0.29644 lossA 2.27188444 fraction A 0.0511515774\n",
      "step 1177 loss 1.28953195 fisher_loss 0.299916625 triplet loss 0.989615381 l2_loss 6.4219017 fraction B 0.323671907 lossA 2.30895615 fraction A 0.054078225\n",
      "step 1178 loss 1.04531264 fisher_loss 0.301050305 triplet loss 0.744262278 l2_loss 6.43094587 fraction B 0.291773528 lossA 2.28610611 fraction A 0.052539777\n",
      "step 1179 loss 1.13935125 fisher_loss 0.300554 triplet loss 0.838797271 l2_loss 6.43791628 fraction B 0.346224606 lossA 2.22990489 fraction A 0.0485793315\n",
      "step 1180 loss 1.12897658 fisher_loss 0.299190968 triplet loss 0.829785585 l2_loss 6.44300413 fraction B 0.239299461 lossA 2.12090421 fraction A 0.0415067449\n",
      "step 1181 loss 1.07325 fisher_loss 0.296921819 triplet loss 0.776328206 l2_loss 6.44737339 fraction B 0.206095144 lossA 2.02782369 fraction A 0.0361564718\n",
      "step 1182 loss 1.20049715 fisher_loss 0.29549247 triplet loss 0.90500474 l2_loss 6.45264912 fraction B 0.286744952 lossA 2.05105591 fraction A 0.0371565074\n",
      "step 1183 loss 1.02040052 fisher_loss 0.295818299 triplet loss 0.724582195 l2_loss 6.46029282 fraction B 0.30216971 lossA 2.10829544 fraction A 0.0401920788\n",
      "step 1184 loss 0.954354703 fisher_loss 0.29685539 triplet loss 0.657499313 l2_loss 6.46929 fraction B 0.205984637 lossA 2.14177155 fraction A 0.0415985323\n",
      "step 1185 loss 1.18267071 fisher_loss 0.297420651 triplet loss 0.885250032 l2_loss 6.47802 fraction B 0.238464653 lossA 2.20768237 fraction A 0.0455732122\n",
      "step 1186 loss 0.980079591 fisher_loss 0.298565447 triplet loss 0.681514144 l2_loss 6.48748159 fraction B 0.286677331 lossA 2.24486136 fraction A 0.0479030907\n",
      "step 1187 loss 1.20601618 fisher_loss 0.299167782 triplet loss 0.906848431 l2_loss 6.4959178 fraction B 0.229820952 lossA 2.26819348 fraction A 0.0490627959\n",
      "step 1188 loss 1.15542793 fisher_loss 0.299033165 triplet loss 0.856394768 l2_loss 6.50296068 fraction B 0.266168863 lossA 2.27298975 fraction A 0.0489967242\n",
      "step 1189 loss 0.93390131 fisher_loss 0.298167318 triplet loss 0.635733962 l2_loss 6.50798035 fraction B 0.287099749 lossA 2.25007153 fraction A 0.0473164655\n",
      "step 1190 loss 1.06882262 fisher_loss 0.297157675 triplet loss 0.771664917 l2_loss 6.51314 fraction B 0.25317198 lossA 2.19579291 fraction A 0.043994192\n",
      "step 1191 loss 1.02636528 fisher_loss 0.295710742 triplet loss 0.730654478 l2_loss 6.51752138 fraction B 0.218569487 lossA 2.14854383 fraction A 0.0412473157\n",
      "step 1192 loss 1.10600722 fisher_loss 0.294628531 triplet loss 0.811378717 l2_loss 6.52393532 fraction B 0.295849264 lossA 2.08769631 fraction A 0.0378209054\n",
      "step 1193 loss 0.851577878 fisher_loss 0.293366581 triplet loss 0.558211267 l2_loss 6.52950716 fraction B 0.273195207 lossA 2.1412034 fraction A 0.0402713344\n",
      "step 1194 loss 1.10015786 fisher_loss 0.293443292 triplet loss 0.806714535 l2_loss 6.53876114 fraction B 0.26324895 lossA 2.2006526 fraction A 0.0437285341\n",
      "step 1195 loss 1.13632369 fisher_loss 0.294049948 triplet loss 0.842273772 l2_loss 6.54782534 fraction B 0.292970419 lossA 2.2718637 fraction A 0.0484906957\n",
      "step 1196 loss 1.04528272 fisher_loss 0.294940203 triplet loss 0.750342548 l2_loss 6.5569768 fraction B 0.276024371 lossA 2.31447697 fraction A 0.0518404953\n",
      "step 1197 loss 1.02754092 fisher_loss 0.295746386 triplet loss 0.731794477 l2_loss 6.5659318 fraction B 0.259459972 lossA 2.35260534 fraction A 0.0546930693\n",
      "step 1198 loss 0.926135659 fisher_loss 0.296340108 triplet loss 0.629795551 l2_loss 6.57409 fraction B 0.227543429 lossA 2.35515094 fraction A 0.0550017245\n",
      "step 1199 loss 0.989243507 fisher_loss 0.296422929 triplet loss 0.692820549 l2_loss 6.58137465 fraction B 0.286304921 lossA 2.32499766 fraction A 0.0528183654\n",
      "step 1200 loss 1.06466198 fisher_loss 0.296082407 triplet loss 0.768579602 l2_loss 6.58799076 fraction B 0.155308411 lossA 2.28120089 fraction A 0.0499525331\n",
      "step 1201 loss 1.21064389 fisher_loss 0.295821816 triplet loss 0.914822102 l2_loss 6.59505224 fraction B 0.232356742 lossA 2.23169804 fraction A 0.0468447357\n",
      "step 1202 loss 0.878088593 fisher_loss 0.295288146 triplet loss 0.582800448 l2_loss 6.60109091 fraction B 0.219128713 lossA 2.17614269 fraction A 0.0438168459\n",
      "step 1203 loss 1.14562345 fisher_loss 0.295276582 triplet loss 0.850346804 l2_loss 6.60905743 fraction B 0.209764913 lossA 2.13274717 fraction A 0.0414918549\n",
      "step 1204 loss 1.03114438 fisher_loss 0.295384765 triplet loss 0.735759556 l2_loss 6.61685324 fraction B 0.265063882 lossA 2.10559058 fraction A 0.0399141051\n",
      "step 1205 loss 1.1531682 fisher_loss 0.295624554 triplet loss 0.857543588 l2_loss 6.6251874 fraction B 0.253310502 lossA 2.10063052 fraction A 0.0395002589\n",
      "step 1206 loss 1.09143305 fisher_loss 0.296073526 triplet loss 0.795359552 l2_loss 6.63416529 fraction B 0.368784636 lossA 2.08834267 fraction A 0.0387931578\n",
      "step 1207 loss 1.25305641 fisher_loss 0.296447605 triplet loss 0.956608772 l2_loss 6.64222717 fraction B 0.20900543 lossA 2.09155893 fraction A 0.0388047472\n",
      "step 1208 loss 1.16854119 fisher_loss 0.29658094 triplet loss 0.871960282 l2_loss 6.64968681 fraction B 0.276834607 lossA 2.13968229 fraction A 0.0414940305\n",
      "step 1209 loss 0.875587404 fisher_loss 0.297148287 triplet loss 0.578439116 l2_loss 6.65733814 fraction B 0.206570804 lossA 2.20185375 fraction A 0.0455700606\n",
      "step 1210 loss 1.31603634 fisher_loss 0.298282593 triplet loss 1.01775372 l2_loss 6.66610098 fraction B 0.256574482 lossA 2.23412776 fraction A 0.0478384681\n",
      "step 1211 loss 1.06387651 fisher_loss 0.298864335 triplet loss 0.765012145 l2_loss 6.6743679 fraction B 0.208168268 lossA 2.25446224 fraction A 0.0492137372\n",
      "step 1212 loss 0.928786397 fisher_loss 0.299245864 triplet loss 0.629540503 l2_loss 6.68276072 fraction B 0.242676497 lossA 2.26953053 fraction A 0.0502509363\n",
      "step 1213 loss 0.875387788 fisher_loss 0.299221754 triplet loss 0.576166034 l2_loss 6.69133139 fraction B 0.156480357 lossA 2.32595491 fraction A 0.0545815043\n",
      "step 1214 loss 1.14883065 fisher_loss 0.301156938 triplet loss 0.847673655 l2_loss 6.70403337 fraction B 0.25735262 lossA 2.33348799 fraction A 0.0555934943\n",
      "step 1215 loss 1.06032884 fisher_loss 0.301488429 triplet loss 0.758840382 l2_loss 6.71416855 fraction B 0.379853964 lossA 2.27767229 fraction A 0.0517403036\n",
      "step 1216 loss 1.03613806 fisher_loss 0.300151825 triplet loss 0.735986173 l2_loss 6.72146606 fraction B 0.213850588 lossA 2.20010686 fraction A 0.046642106\n",
      "step 1217 loss 1.15463638 fisher_loss 0.298459738 triplet loss 0.856176615 l2_loss 6.72905159 fraction B 0.342174262 lossA 2.10544538 fraction A 0.0405950174\n",
      "step 1218 loss 1.09777701 fisher_loss 0.296352237 triplet loss 0.801424742 l2_loss 6.73551416 fraction B 0.239976808 lossA 2.01730466 fraction A 0.0359320343\n",
      "step 1219 loss 0.919180632 fisher_loss 0.294596583 triplet loss 0.624584 l2_loss 6.74110651 fraction B 0.299272895 lossA 1.99542 fraction A 0.0351642594\n",
      "step 1220 loss 1.10610271 fisher_loss 0.294085324 triplet loss 0.812017322 l2_loss 6.74826431 fraction B 0.237413526 lossA 2.08896732 fraction A 0.0400925726\n",
      "step 1221 loss 1.10800195 fisher_loss 0.294820786 triplet loss 0.813181221 l2_loss 6.75746679 fraction B 0.347969472 lossA 2.18006873 fraction A 0.0457890257\n",
      "step 1222 loss 1.24946058 fisher_loss 0.295749635 triplet loss 0.953710914 l2_loss 6.76626492 fraction B 0.246608138 lossA 2.264781 fraction A 0.0513390303\n",
      "step 1223 loss 1.1698786 fisher_loss 0.29662627 triplet loss 0.873252332 l2_loss 6.7748208 fraction B 0.307362229 lossA 2.3221252 fraction A 0.0553324707\n",
      "step 1224 loss 1.12678075 fisher_loss 0.296943545 triplet loss 0.829837143 l2_loss 6.78138065 fraction B 0.155342 lossA 2.37066793 fraction A 0.0589437671\n",
      "step 1225 loss 1.04794693 fisher_loss 0.297371686 triplet loss 0.750575244 l2_loss 6.78801107 fraction B 0.245931178 lossA 2.3814528 fraction A 0.0599183\n",
      "step 1226 loss 1.15102649 fisher_loss 0.297220647 triplet loss 0.8538059 l2_loss 6.79445219 fraction B 0.206296459 lossA 2.34037113 fraction A 0.0568383597\n",
      "step 1227 loss 1.31930196 fisher_loss 0.296264619 triplet loss 1.02303731 l2_loss 6.79968548 fraction B 0.217276827 lossA 2.32283044 fraction A 0.055735413\n",
      "step 1228 loss 1.3547256 fisher_loss 0.29573971 triplet loss 1.05898595 l2_loss 6.80459642 fraction B 0.33594656 lossA 2.26059222 fraction A 0.0511899367\n",
      "step 1229 loss 0.999466658 fisher_loss 0.294496506 triplet loss 0.704970121 l2_loss 6.80760193 fraction B 0.1995662 lossA 2.22993684 fraction A 0.0489790477\n",
      "step 1230 loss 1.0270766 fisher_loss 0.293879271 triplet loss 0.733197331 l2_loss 6.81210184 fraction B 0.305910736 lossA 2.18456578 fraction A 0.0456594937\n",
      "step 1231 loss 1.13345671 fisher_loss 0.293102086 triplet loss 0.840354562 l2_loss 6.81555367 fraction B 0.301855534 lossA 2.12712955 fraction A 0.0417697579\n",
      "step 1232 loss 1.04455209 fisher_loss 0.292279333 triplet loss 0.752272785 l2_loss 6.81789112 fraction B 0.248385027 lossA 2.11825013 fraction A 0.0413956493\n",
      "step 1233 loss 1.14863706 fisher_loss 0.292411566 triplet loss 0.85622555 l2_loss 6.82169628 fraction B 0.231672451 lossA 2.11075497 fraction A 0.0411971137\n",
      "step 1234 loss 0.985307097 fisher_loss 0.292453647 triplet loss 0.692853451 l2_loss 6.82542 fraction B 0.206576794 lossA 2.15679836 fraction A 0.0444335714\n",
      "step 1235 loss 1.4128027 fisher_loss 0.293415964 triplet loss 1.11938679 l2_loss 6.83122444 fraction B 0.279080033 lossA 2.15305257 fraction A 0.0443758704\n",
      "step 1236 loss 1.00086331 fisher_loss 0.293948859 triplet loss 0.706914425 l2_loss 6.83573389 fraction B 0.276424587 lossA 2.20576763 fraction A 0.0478641875\n",
      "step 1237 loss 1.02622461 fisher_loss 0.295483202 triplet loss 0.730741382 l2_loss 6.84328842 fraction B 0.272356719 lossA 2.20412874 fraction A 0.0477273\n",
      "step 1238 loss 1.19900179 fisher_loss 0.295796633 triplet loss 0.903205216 l2_loss 6.84924173 fraction B 0.34780851 lossA 2.17502642 fraction A 0.0458223149\n",
      "step 1239 loss 0.971591413 fisher_loss 0.295376956 triplet loss 0.676214457 l2_loss 6.85429 fraction B 0.22515282 lossA 2.12795949 fraction A 0.0428373106\n",
      "step 1240 loss 1.04736447 fisher_loss 0.294668704 triplet loss 0.752695799 l2_loss 6.85998201 fraction B 0.314827383 lossA 2.1294 fraction A 0.0428394489\n",
      "step 1241 loss 1.04939425 fisher_loss 0.294706017 triplet loss 0.754688263 l2_loss 6.86739731 fraction B 0.336172909 lossA 2.10566115 fraction A 0.041398149\n",
      "step 1242 loss 1.18275666 fisher_loss 0.294483393 triplet loss 0.888273299 l2_loss 6.8748045 fraction B 0.390279382 lossA 2.05367899 fraction A 0.0382139571\n",
      "step 1243 loss 0.927114844 fisher_loss 0.293406755 triplet loss 0.633708119 l2_loss 6.88036203 fraction B 0.208825856 lossA 2.08004451 fraction A 0.0397152416\n",
      "step 1244 loss 0.881441474 fisher_loss 0.293664545 triplet loss 0.587776959 l2_loss 6.88829803 fraction B 0.255666763 lossA 2.09174609 fraction A 0.0403399393\n",
      "step 1245 loss 0.995795 fisher_loss 0.293661475 triplet loss 0.702133536 l2_loss 6.89634657 fraction B 0.214718834 lossA 2.1180675 fraction A 0.0422185548\n",
      "step 1246 loss 1.06877494 fisher_loss 0.294016153 triplet loss 0.774758816 l2_loss 6.90446472 fraction B 0.255154759 lossA 2.17188382 fraction A 0.0459136665\n",
      "step 1247 loss 1.16772342 fisher_loss 0.294894308 triplet loss 0.87282908 l2_loss 6.91355371 fraction B 0.312654972 lossA 2.21795416 fraction A 0.0490771383\n",
      "step 1248 loss 1.01856375 fisher_loss 0.295639426 triplet loss 0.722924352 l2_loss 6.9221983 fraction B 0.212503076 lossA 2.22125 fraction A 0.0495374203\n",
      "step 1249 loss 1.10868812 fisher_loss 0.295933843 triplet loss 0.812754214 l2_loss 6.92974567 fraction B 0.352714181 lossA 2.18305445 fraction A 0.047350768\n",
      "step 1250 loss 0.817066193 fisher_loss 0.295046538 triplet loss 0.522019684 l2_loss 6.93460274 fraction B 0.185376018 lossA 2.13119626 fraction A 0.0443479419\n",
      "step 1251 loss 0.899365067 fisher_loss 0.293680847 triplet loss 0.605684221 l2_loss 6.94097567 fraction B 0.262046725 lossA 2.07576656 fraction A 0.0409791246\n",
      "step 1252 loss 0.967743754 fisher_loss 0.292381704 triplet loss 0.675362051 l2_loss 6.94718695 fraction B 0.28562361 lossA 2.050354 fraction A 0.0393775366\n",
      "step 1253 loss 1.02108502 fisher_loss 0.291638106 triplet loss 0.729446888 l2_loss 6.9546 fraction B 0.253604293 lossA 2.1190145 fraction A 0.0429789051\n",
      "step 1254 loss 0.959934652 fisher_loss 0.291984081 triplet loss 0.667950571 l2_loss 6.96233559 fraction B 0.197709247 lossA 2.20557 fraction A 0.0482108034\n",
      "step 1255 loss 1.0407654 fisher_loss 0.293017179 triplet loss 0.747748256 l2_loss 6.97178125 fraction B 0.257900834 lossA 2.2668345 fraction A 0.0525306836\n",
      "step 1256 loss 1.05232537 fisher_loss 0.294327587 triplet loss 0.757997811 l2_loss 6.98143291 fraction B 0.227520376 lossA 2.29071641 fraction A 0.0544341505\n",
      "step 1257 loss 1.12182522 fisher_loss 0.295116663 triplet loss 0.826708615 l2_loss 6.99001551 fraction B 0.303447694 lossA 2.31044436 fraction A 0.0559209473\n",
      "step 1258 loss 0.986699343 fisher_loss 0.295431525 triplet loss 0.691267848 l2_loss 6.99778652 fraction B 0.195366368 lossA 2.32584691 fraction A 0.0569502153\n",
      "step 1259 loss 1.16386271 fisher_loss 0.295585573 triplet loss 0.868277192 l2_loss 7.00560188 fraction B 0.339782089 lossA 2.30585647 fraction A 0.055264879\n",
      "step 1260 loss 1.08942366 fisher_loss 0.294838727 triplet loss 0.79458487 l2_loss 7.01157904 fraction B 0.335954666 lossA 2.25240612 fraction A 0.0511148125\n",
      "step 1261 loss 1.05703413 fisher_loss 0.293484628 triplet loss 0.763549507 l2_loss 7.01586294 fraction B 0.224429667 lossA 2.1987555 fraction A 0.0474360362\n",
      "step 1262 loss 1.1834209 fisher_loss 0.292240232 triplet loss 0.891180694 l2_loss 7.01991606 fraction B 0.421089113 lossA 2.14542341 fraction A 0.0439237729\n",
      "step 1263 loss 1.02064395 fisher_loss 0.290737599 triplet loss 0.729906321 l2_loss 7.02304173 fraction B 0.24896118 lossA 2.12694407 fraction A 0.0428238\n",
      "step 1264 loss 1.04976869 fisher_loss 0.290100634 triplet loss 0.759668 l2_loss 7.02809191 fraction B 0.372381955 lossA 2.11244249 fraction A 0.0418824814\n",
      "step 1265 loss 1.02236056 fisher_loss 0.289462119 triplet loss 0.732898474 l2_loss 7.03309 fraction B 0.265776038 lossA 2.11638021 fraction A 0.0419131964\n",
      "step 1266 loss 1.20944953 fisher_loss 0.289167225 triplet loss 0.920282364 l2_loss 7.04015923 fraction B 0.290702671 lossA 2.13558078 fraction A 0.0429686904\n",
      "step 1267 loss 1.08108938 fisher_loss 0.288919 triplet loss 0.792170405 l2_loss 7.04673767 fraction B 0.329946548 lossA 2.16273355 fraction A 0.0444965288\n",
      "step 1268 loss 1.18971026 fisher_loss 0.288951248 triplet loss 0.900759041 l2_loss 7.05319786 fraction B 0.331414431 lossA 2.19646955 fraction A 0.0463421829\n",
      "step 1269 loss 1.11502647 fisher_loss 0.288902462 triplet loss 0.826123953 l2_loss 7.0597949 fraction B 0.24975495 lossA 2.23924351 fraction A 0.049040556\n",
      "step 1270 loss 0.971161544 fisher_loss 0.289324403 triplet loss 0.681837142 l2_loss 7.0671277 fraction B 0.194607809 lossA 2.32760763 fraction A 0.0550556928\n",
      "step 1271 loss 1.20509279 fisher_loss 0.290875196 triplet loss 0.914217591 l2_loss 7.07632637 fraction B 0.334252387 lossA 2.38598537 fraction A 0.0594589226\n",
      "step 1272 loss 1.24236429 fisher_loss 0.292371243 triplet loss 0.949993074 l2_loss 7.08441925 fraction B 0.315764546 lossA 2.38325024 fraction A 0.0593120046\n",
      "step 1273 loss 1.0435853 fisher_loss 0.292792559 triplet loss 0.750792682 l2_loss 7.09126091 fraction B 0.283952981 lossA 2.34636807 fraction A 0.0566747747\n",
      "step 1274 loss 1.03611577 fisher_loss 0.292366564 triplet loss 0.743749201 l2_loss 7.09688854 fraction B 0.270331383 lossA 2.27400088 fraction A 0.0515183322\n",
      "step 1275 loss 1.07955551 fisher_loss 0.291568428 triplet loss 0.787987053 l2_loss 7.10230827 fraction B 0.322011858 lossA 2.19599819 fraction A 0.0464451276\n",
      "step 1276 loss 1.05524266 fisher_loss 0.290647179 triplet loss 0.764595449 l2_loss 7.10691833 fraction B 0.312372804 lossA 2.11180067 fraction A 0.0414906964\n",
      "step 1277 loss 1.26799357 fisher_loss 0.289869815 triplet loss 0.978123784 l2_loss 7.11136675 fraction B 0.249503031 lossA 2.05034161 fraction A 0.0378803834\n",
      "step 1278 loss 0.930623889 fisher_loss 0.289202869 triplet loss 0.641421 l2_loss 7.11579037 fraction B 0.272251755 lossA 2.06903481 fraction A 0.0391047075\n",
      "step 1279 loss 1.0743506 fisher_loss 0.289843261 triplet loss 0.784507394 l2_loss 7.12278652 fraction B 0.246788561 lossA 2.09833 fraction A 0.040984448\n",
      "step 1280 loss 1.06141412 fisher_loss 0.290737659 triplet loss 0.770676434 l2_loss 7.12987614 fraction B 0.280789644 lossA 2.14982867 fraction A 0.0441105403\n",
      "step 1281 loss 0.923051834 fisher_loss 0.292112 triplet loss 0.630939841 l2_loss 7.13822222 fraction B 0.21229355 lossA 2.21023345 fraction A 0.0477492511\n",
      "step 1282 loss 1.06397271 fisher_loss 0.293760657 triplet loss 0.770212114 l2_loss 7.14749527 fraction B 0.30200088 lossA 2.26341081 fraction A 0.0510367863\n",
      "step 1283 loss 1.39428878 fisher_loss 0.295120031 triplet loss 1.09916878 l2_loss 7.15626812 fraction B 0.374709189 lossA 2.28991938 fraction A 0.0526435897\n",
      "step 1284 loss 0.903354883 fisher_loss 0.295769721 triplet loss 0.607585132 l2_loss 7.16302824 fraction B 0.23173371 lossA 2.28908491 fraction A 0.0523510203\n",
      "step 1285 loss 1.32167208 fisher_loss 0.29559207 triplet loss 1.02608 l2_loss 7.16932774 fraction B 0.354791909 lossA 2.26715136 fraction A 0.050567124\n",
      "step 1286 loss 1.11575222 fisher_loss 0.29466483 triplet loss 0.82108742 l2_loss 7.173594 fraction B 0.244642556 lossA 2.19561863 fraction A 0.0458094925\n",
      "step 1287 loss 1.17351186 fisher_loss 0.292845696 triplet loss 0.880666137 l2_loss 7.17670393 fraction B 0.238848746 lossA 2.11180925 fraction A 0.0405340195\n",
      "step 1288 loss 0.953419924 fisher_loss 0.290976554 triplet loss 0.662443399 l2_loss 7.17970705 fraction B 0.242273375 lossA 2.08585358 fraction A 0.0388729908\n",
      "step 1289 loss 0.771700859 fisher_loss 0.290423691 triplet loss 0.481277168 l2_loss 7.18485498 fraction B 0.184143737 lossA 2.11648679 fraction A 0.0403338522\n",
      "step 1290 loss 1.08370471 fisher_loss 0.290908515 triplet loss 0.792796135 l2_loss 7.19236803 fraction B 0.293229908 lossA 2.13813496 fraction A 0.0412936099\n",
      "step 1291 loss 1.10972512 fisher_loss 0.291184187 triplet loss 0.818540931 l2_loss 7.19952297 fraction B 0.224579677 lossA 2.15772867 fraction A 0.0422618762\n",
      "step 1292 loss 0.904413223 fisher_loss 0.291326493 triplet loss 0.6130867 l2_loss 7.20660973 fraction B 0.20867312 lossA 2.18056893 fraction A 0.0435748063\n",
      "step 1293 loss 0.946195722 fisher_loss 0.291572869 triplet loss 0.654622853 l2_loss 7.21335506 fraction B 0.233121544 lossA 2.2331183 fraction A 0.0464499071\n",
      "step 1294 loss 1.13415611 fisher_loss 0.292160243 triplet loss 0.841995835 l2_loss 7.22046518 fraction B 0.316151947 lossA 2.23585367 fraction A 0.0462103672\n",
      "step 1295 loss 0.952725172 fisher_loss 0.291802794 triplet loss 0.660922408 l2_loss 7.22576618 fraction B 0.212776974 lossA 2.23917985 fraction A 0.0460306294\n",
      "step 1296 loss 0.850586295 fisher_loss 0.291915774 triplet loss 0.558670521 l2_loss 7.23256874 fraction B 0.190450683 lossA 2.21861339 fraction A 0.0445006564\n",
      "step 1297 loss 1.40034926 fisher_loss 0.292203933 triplet loss 1.10814536 l2_loss 7.24087334 fraction B 0.331868917 lossA 2.20248413 fraction A 0.0432622023\n",
      "step 1298 loss 1.07114267 fisher_loss 0.292344362 triplet loss 0.778798342 l2_loss 7.24788618 fraction B 0.297678471 lossA 2.17654705 fraction A 0.0417314693\n",
      "step 1299 loss 1.16417563 fisher_loss 0.292245924 triplet loss 0.871929705 l2_loss 7.25449181 fraction B 0.263508976 lossA 2.12973356 fraction A 0.0391190872\n",
      "step 1300 loss 1.14584053 fisher_loss 0.291660905 triplet loss 0.854179621 l2_loss 7.25993776 fraction B 0.275272638 lossA 2.09897494 fraction A 0.0373398662\n",
      "step 1301 loss 1.31679618 fisher_loss 0.291102976 triplet loss 1.02569318 l2_loss 7.26559687 fraction B 0.379614979 lossA 2.08284068 fraction A 0.0365906358\n",
      "step 1302 loss 1.07755446 fisher_loss 0.29046312 triplet loss 0.787091374 l2_loss 7.27101755 fraction B 0.24956052 lossA 2.12209892 fraction A 0.0387979038\n",
      "step 1303 loss 1.28554261 fisher_loss 0.290592521 triplet loss 0.994950056 l2_loss 7.27828741 fraction B 0.354600221 lossA 2.13887548 fraction A 0.0399425738\n",
      "step 1304 loss 1.34762955 fisher_loss 0.29049772 triplet loss 1.05713177 l2_loss 7.28428221 fraction B 0.234868124 lossA 2.14125395 fraction A 0.0402628221\n",
      "step 1305 loss 1.05698538 fisher_loss 0.290500611 triplet loss 0.766484797 l2_loss 7.29059124 fraction B 0.229860887 lossA 2.12695 fraction A 0.03986343\n",
      "step 1306 loss 1.06497979 fisher_loss 0.290548205 triplet loss 0.774431527 l2_loss 7.29728794 fraction B 0.224997863 lossA 2.11286139 fraction A 0.0395251438\n",
      "step 1307 loss 1.07806706 fisher_loss 0.290331751 triplet loss 0.787735343 l2_loss 7.30320215 fraction B 0.319486439 lossA 2.09351516 fraction A 0.0387662426\n",
      "step 1308 loss 1.01593935 fisher_loss 0.290030092 triplet loss 0.725909233 l2_loss 7.30877304 fraction B 0.309973031 lossA 2.09623861 fraction A 0.0393796377\n",
      "step 1309 loss 1.32945728 fisher_loss 0.290439 triplet loss 1.03901827 l2_loss 7.31519842 fraction B 0.26909408 lossA 2.10212302 fraction A 0.0399482623\n",
      "step 1310 loss 0.806857228 fisher_loss 0.290729851 triplet loss 0.516127408 l2_loss 7.32149792 fraction B 0.171490714 lossA 2.11796594 fraction A 0.0409343876\n",
      "step 1311 loss 0.933564723 fisher_loss 0.291022837 triplet loss 0.642541885 l2_loss 7.32871151 fraction B 0.254126966 lossA 2.19477224 fraction A 0.045193743\n",
      "step 1312 loss 0.897576 fisher_loss 0.292036504 triplet loss 0.605539441 l2_loss 7.33765173 fraction B 0.232800469 lossA 2.21503782 fraction A 0.0463873893\n",
      "step 1313 loss 1.00216055 fisher_loss 0.292334557 triplet loss 0.709826052 l2_loss 7.34626102 fraction B 0.224945396 lossA 2.19304228 fraction A 0.0449210927\n",
      "step 1314 loss 1.20311809 fisher_loss 0.291907787 triplet loss 0.911210358 l2_loss 7.35417 fraction B 0.300429374 lossA 2.15249276 fraction A 0.0420890227\n",
      "step 1315 loss 1.0275892 fisher_loss 0.291025072 triplet loss 0.736564159 l2_loss 7.36076832 fraction B 0.25331831 lossA 2.12518287 fraction A 0.0404198468\n",
      "step 1316 loss 1.28314126 fisher_loss 0.290331 triplet loss 0.992810249 l2_loss 7.36681175 fraction B 0.337281585 lossA 2.09133029 fraction A 0.0383938029\n",
      "step 1317 loss 1.19996774 fisher_loss 0.289377809 triplet loss 0.910589933 l2_loss 7.37118864 fraction B 0.28111589 lossA 2.07446527 fraction A 0.037714228\n",
      "step 1318 loss 1.16683722 fisher_loss 0.288613439 triplet loss 0.878223717 l2_loss 7.37535954 fraction B 0.257314831 lossA 2.09159136 fraction A 0.0390095152\n",
      "step 1319 loss 1.26745439 fisher_loss 0.288164735 triplet loss 0.979289591 l2_loss 7.37952 fraction B 0.306456357 lossA 2.13339114 fraction A 0.0416019\n",
      "step 1320 loss 1.1517539 fisher_loss 0.288073689 triplet loss 0.863680243 l2_loss 7.3832593 fraction B 0.294588536 lossA 2.16325212 fraction A 0.043380145\n",
      "step 1321 loss 1.1405561 fisher_loss 0.287567914 triplet loss 0.852988243 l2_loss 7.38611174 fraction B 0.280180246 lossA 2.2208271 fraction A 0.0462487265\n",
      "step 1322 loss 1.03221059 fisher_loss 0.287539274 triplet loss 0.744671285 l2_loss 7.38894892 fraction B 0.329933554 lossA 2.23772955 fraction A 0.0468607098\n",
      "step 1323 loss 0.949099541 fisher_loss 0.286956459 triplet loss 0.662143052 l2_loss 7.39139223 fraction B 0.234782264 lossA 2.18595457 fraction A 0.0439238101\n",
      "step 1324 loss 1.02626145 fisher_loss 0.285467178 triplet loss 0.740794241 l2_loss 7.39313221 fraction B 0.293464541 lossA 2.11919975 fraction A 0.0403177701\n",
      "step 1325 loss 0.85340631 fisher_loss 0.283809334 triplet loss 0.569597 l2_loss 7.39542913 fraction B 0.234008819 lossA 2.07701564 fraction A 0.0378048941\n",
      "step 1326 loss 1.24707115 fisher_loss 0.282936245 triplet loss 0.964134932 l2_loss 7.40061569 fraction B 0.236805022 lossA 2.04924798 fraction A 0.0360518247\n",
      "step 1327 loss 1.02408242 fisher_loss 0.282343507 triplet loss 0.741739 l2_loss 7.40520144 fraction B 0.272547275 lossA 2.06137705 fraction A 0.0364458561\n",
      "step 1328 loss 1.13016129 fisher_loss 0.282324284 triplet loss 0.847837 l2_loss 7.41122961 fraction B 0.386153132 lossA 2.08692074 fraction A 0.0376606546\n",
      "step 1329 loss 1.10705471 fisher_loss 0.282445729 triplet loss 0.824609 l2_loss 7.41645098 fraction B 0.241861448 lossA 2.17716622 fraction A 0.0427165441\n",
      "step 1330 loss 0.97583437 fisher_loss 0.283189923 triplet loss 0.692644477 l2_loss 7.42228746 fraction B 0.230428487 lossA 2.23820353 fraction A 0.0461690389\n",
      "step 1331 loss 0.911199689 fisher_loss 0.283555478 triplet loss 0.627644241 l2_loss 7.42728 fraction B 0.15088819 lossA 2.29069853 fraction A 0.0488888174\n",
      "step 1332 loss 1.17300177 fisher_loss 0.283645719 triplet loss 0.889356077 l2_loss 7.43220091 fraction B 0.302642703 lossA 2.32966089 fraction A 0.0506760813\n",
      "step 1333 loss 0.957135201 fisher_loss 0.283120215 triplet loss 0.674015 l2_loss 7.43581915 fraction B 0.216411307 lossA 2.31770706 fraction A 0.0491580255\n",
      "step 1334 loss 0.908534646 fisher_loss 0.282059401 triplet loss 0.626475275 l2_loss 7.43887281 fraction B 0.193159312 lossA 2.18489671 fraction A 0.0410756581\n",
      "step 1335 loss 1.12612748 fisher_loss 0.279820085 triplet loss 0.846307397 l2_loss 7.44065762 fraction B 0.3013542 lossA 2.07887959 fraction A 0.0355123952\n",
      "step 1336 loss 1.24793935 fisher_loss 0.278402209 triplet loss 0.969537139 l2_loss 7.44288 fraction B 0.242790028 lossA 2.04313922 fraction A 0.0337554142\n",
      "step 1337 loss 1.25117886 fisher_loss 0.277706653 triplet loss 0.973472238 l2_loss 7.44618511 fraction B 0.280208319 lossA 2.05118942 fraction A 0.0340521149\n",
      "step 1338 loss 1.10492241 fisher_loss 0.277284831 triplet loss 0.827637613 l2_loss 7.44919491 fraction B 0.286484182 lossA 2.1373651 fraction A 0.0380175933\n",
      "step 1339 loss 1.09968269 fisher_loss 0.277508587 triplet loss 0.822174072 l2_loss 7.45381832 fraction B 0.179191098 lossA 2.27574015 fraction A 0.0451294109\n",
      "step 1340 loss 1.40458846 fisher_loss 0.278457463 triplet loss 1.12613106 l2_loss 7.45995522 fraction B 0.323845327 lossA 2.38939691 fraction A 0.0520054176\n",
      "step 1341 loss 1.17174911 fisher_loss 0.279704899 triplet loss 0.892044187 l2_loss 7.46615124 fraction B 0.262866706 lossA 2.44281578 fraction A 0.0559496358\n",
      "step 1342 loss 0.970203102 fisher_loss 0.280496836 triplet loss 0.689706266 l2_loss 7.47130823 fraction B 0.254451662 lossA 2.42679834 fraction A 0.0553451516\n",
      "step 1343 loss 1.0067656 fisher_loss 0.280550659 triplet loss 0.726214945 l2_loss 7.47619772 fraction B 0.242109701 lossA 2.38772655 fraction A 0.0530701131\n",
      "step 1344 loss 1.00637579 fisher_loss 0.280225813 triplet loss 0.726149917 l2_loss 7.48092937 fraction B 0.27365005 lossA 2.31940794 fraction A 0.0488880575\n",
      "step 1345 loss 1.16567481 fisher_loss 0.279380798 triplet loss 0.886294 l2_loss 7.48494387 fraction B 0.23962225 lossA 2.27889395 fraction A 0.0464870371\n",
      "step 1346 loss 1.03798103 fisher_loss 0.278887 triplet loss 0.759094059 l2_loss 7.48929501 fraction B 0.200797483 lossA 2.23602533 fraction A 0.0441582426\n",
      "step 1347 loss 0.976789355 fisher_loss 0.278300017 triplet loss 0.698489308 l2_loss 7.49338245 fraction B 0.232255459 lossA 2.22039032 fraction A 0.0434352383\n",
      "step 1348 loss 0.899178 fisher_loss 0.278196186 triplet loss 0.620981872 l2_loss 7.49904299 fraction B 0.15567863 lossA 2.21783471 fraction A 0.0434028916\n",
      "step 1349 loss 1.06383967 fisher_loss 0.278532684 triplet loss 0.785307 l2_loss 7.50625229 fraction B 0.262763321 lossA 2.27306581 fraction A 0.0467928275\n",
      "step 1350 loss 1.03941107 fisher_loss 0.279761821 triplet loss 0.759649277 l2_loss 7.51477623 fraction B 0.260246158 lossA 2.29013538 fraction A 0.0480622537\n",
      "step 1351 loss 1.04988027 fisher_loss 0.280423403 triplet loss 0.769456804 l2_loss 7.52142239 fraction B 0.262089133 lossA 2.26101971 fraction A 0.0467976481\n",
      "step 1352 loss 1.12771845 fisher_loss 0.280912757 triplet loss 0.846805751 l2_loss 7.52871418 fraction B 0.277032822 lossA 2.19441 fraction A 0.0434002839\n",
      "step 1353 loss 1.30967402 fisher_loss 0.28048408 triplet loss 1.02919 l2_loss 7.53439951 fraction B 0.254511565 lossA 2.09336257 fraction A 0.038192369\n",
      "step 1354 loss 1.02403343 fisher_loss 0.279482543 triplet loss 0.744550884 l2_loss 7.53811741 fraction B 0.336885929 lossA 2.02586675 fraction A 0.0348624513\n",
      "step 1355 loss 1.31294954 fisher_loss 0.278641462 triplet loss 1.03430808 l2_loss 7.54214621 fraction B 0.309316 lossA 1.98007727 fraction A 0.0326707624\n",
      "step 1356 loss 0.977221 fisher_loss 0.277890712 triplet loss 0.69933033 l2_loss 7.54543114 fraction B 0.28437525 lossA 2.02954292 fraction A 0.0349157341\n",
      "step 1357 loss 0.954546392 fisher_loss 0.277971685 triplet loss 0.676574707 l2_loss 7.54987574 fraction B 0.215435296 lossA 2.1688869 fraction A 0.0418635\n",
      "step 1358 loss 0.92280972 fisher_loss 0.279142052 triplet loss 0.643667638 l2_loss 7.55621815 fraction B 0.254870743 lossA 2.26913095 fraction A 0.0472940765\n",
      "step 1359 loss 1.02859676 fisher_loss 0.280210972 triplet loss 0.748385787 l2_loss 7.56230783 fraction B 0.25824663 lossA 2.32280564 fraction A 0.0502847321\n",
      "step 1360 loss 1.00863135 fisher_loss 0.28094089 triplet loss 0.727690458 l2_loss 7.56804466 fraction B 0.20970498 lossA 2.32510185 fraction A 0.0500706546\n",
      "step 1361 loss 1.03616095 fisher_loss 0.281007111 triplet loss 0.755153775 l2_loss 7.57309532 fraction B 0.274148941 lossA 2.25581217 fraction A 0.0456323251\n",
      "step 1362 loss 0.891741395 fisher_loss 0.279955119 triplet loss 0.611786306 l2_loss 7.57584095 fraction B 0.199376687 lossA 2.1354816 fraction A 0.0388912857\n",
      "step 1363 loss 0.970216036 fisher_loss 0.27875185 triplet loss 0.691464186 l2_loss 7.57848406 fraction B 0.218214378 lossA 2.0200305 fraction A 0.0332260206\n",
      "step 1364 loss 1.09707332 fisher_loss 0.278018564 triplet loss 0.819054782 l2_loss 7.5828681 fraction B 0.303450912 lossA 1.95607495 fraction A 0.0304368362\n",
      "step 1365 loss 0.97170043 fisher_loss 0.278024554 triplet loss 0.693675876 l2_loss 7.58837605 fraction B 0.233284578 lossA 1.90721631 fraction A 0.02863512\n",
      "step 1366 loss 1.08342218 fisher_loss 0.278383642 triplet loss 0.805038571 l2_loss 7.59470892 fraction B 0.382412285 lossA 1.97078443 fraction A 0.0316055603\n",
      "step 1367 loss 0.935273409 fisher_loss 0.279699385 triplet loss 0.655574 l2_loss 7.60390282 fraction B 0.228862152 lossA 2.0113337 fraction A 0.0338420235\n",
      "step 1368 loss 1.04731822 fisher_loss 0.280817837 triplet loss 0.766500413 l2_loss 7.61239243 fraction B 0.313955903 lossA 2.09438396 fraction A 0.038323205\n",
      "step 1369 loss 1.12486482 fisher_loss 0.282450974 triplet loss 0.842413843 l2_loss 7.62281799 fraction B 0.182393238 lossA 2.21716762 fraction A 0.0460121222\n",
      "step 1370 loss 1.16588545 fisher_loss 0.284906924 triplet loss 0.880978584 l2_loss 7.63505268 fraction B 0.338970035 lossA 2.29844713 fraction A 0.0514663868\n",
      "step 1371 loss 0.869210422 fisher_loss 0.286694944 triplet loss 0.582515478 l2_loss 7.64579 fraction B 0.182919413 lossA 2.32171774 fraction A 0.0527695715\n",
      "step 1372 loss 1.16517067 fisher_loss 0.286868215 triplet loss 0.878302395 l2_loss 7.65591192 fraction B 0.359030187 lossA 2.28087473 fraction A 0.0496049039\n",
      "step 1373 loss 0.974830747 fisher_loss 0.286007375 triplet loss 0.688823342 l2_loss 7.66405964 fraction B 0.261687785 lossA 2.19187355 fraction A 0.0436741263\n",
      "step 1374 loss 0.913732648 fisher_loss 0.284567147 triplet loss 0.629165471 l2_loss 7.67139912 fraction B 0.227574021 lossA 2.09355569 fraction A 0.0378876626\n",
      "step 1375 loss 0.851091444 fisher_loss 0.283740461 triplet loss 0.567351 l2_loss 7.68050432 fraction B 0.279807687 lossA 2.05185056 fraction A 0.0360141508\n",
      "step 1376 loss 1.0134728 fisher_loss 0.284256935 triplet loss 0.72921586 l2_loss 7.69163465 fraction B 0.289126426 lossA 2.02954292 fraction A 0.0351222791\n",
      "step 1377 loss 0.884295 fisher_loss 0.285196513 triplet loss 0.599098444 l2_loss 7.70274734 fraction B 0.26880163 lossA 2.09624028 fraction A 0.0387503058\n",
      "step 1378 loss 1.09707737 fisher_loss 0.287364423 triplet loss 0.809713 l2_loss 7.7150116 fraction B 0.302188694 lossA 2.1720705 fraction A 0.0435858518\n",
      "step 1379 loss 0.993305564 fisher_loss 0.289603859 triplet loss 0.703701675 l2_loss 7.72667837 fraction B 0.267826468 lossA 2.23623323 fraction A 0.0478942506\n",
      "step 1380 loss 1.09435582 fisher_loss 0.291423202 triplet loss 0.80293268 l2_loss 7.73783827 fraction B 0.133672938 lossA 2.29667616 fraction A 0.0520583764\n",
      "step 1381 loss 1.16745496 fisher_loss 0.292838335 triplet loss 0.874616623 l2_loss 7.74852324 fraction B 0.265681177 lossA 2.34803987 fraction A 0.0558307879\n",
      "step 1382 loss 1.18213868 fisher_loss 0.29360491 triplet loss 0.888533711 l2_loss 7.75783777 fraction B 0.337712735 lossA 2.30513573 fraction A 0.0529428236\n",
      "step 1383 loss 0.980037212 fisher_loss 0.292603225 triplet loss 0.687434 l2_loss 7.76497173 fraction B 0.201749772 lossA 2.21484113 fraction A 0.0469214544\n",
      "step 1384 loss 1.13338888 fisher_loss 0.290549964 triplet loss 0.842838883 l2_loss 7.77118111 fraction B 0.273602486 lossA 2.1062839 fraction A 0.0399239585\n",
      "step 1385 loss 0.934832931 fisher_loss 0.288475305 triplet loss 0.646357596 l2_loss 7.77664137 fraction B 0.252143323 lossA 2.01216888 fraction A 0.0349232331\n",
      "step 1386 loss 1.142851 fisher_loss 0.287287325 triplet loss 0.8555637 l2_loss 7.7839241 fraction B 0.235362649 lossA 2.00698376 fraction A 0.0348811783\n",
      "step 1387 loss 1.28421199 fisher_loss 0.287162691 triplet loss 0.997049332 l2_loss 7.79196 fraction B 0.368922234 lossA 2.02968931 fraction A 0.0363194\n",
      "step 1388 loss 1.26766062 fisher_loss 0.287302077 triplet loss 0.980358481 l2_loss 7.80043602 fraction B 0.229614899 lossA 2.06055856 fraction A 0.0380977169\n",
      "step 1389 loss 1.13150883 fisher_loss 0.28746593 triplet loss 0.844042838 l2_loss 7.80818 fraction B 0.33345291 lossA 2.1317904 fraction A 0.0422895513\n",
      "step 1390 loss 1.14643717 fisher_loss 0.287895143 triplet loss 0.858542085 l2_loss 7.81538439 fraction B 0.306454301 lossA 2.19491076 fraction A 0.0468038395\n",
      "step 1391 loss 0.941438735 fisher_loss 0.288487792 triplet loss 0.652950943 l2_loss 7.82193184 fraction B 0.192080766 lossA 2.28646493 fraction A 0.0536327586\n",
      "step 1392 loss 0.984133422 fisher_loss 0.290078938 triplet loss 0.694054484 l2_loss 7.83003092 fraction B 0.217497483 lossA 2.29197788 fraction A 0.0539390221\n",
      "step 1393 loss 1.07510209 fisher_loss 0.290042 triplet loss 0.785060108 l2_loss 7.83610201 fraction B 0.278969705 lossA 2.25363421 fraction A 0.0509326831\n",
      "step 1394 loss 1.02027941 fisher_loss 0.289191037 triplet loss 0.73108834 l2_loss 7.84097624 fraction B 0.225359365 lossA 2.18071175 fraction A 0.0457486026\n",
      "step 1395 loss 0.939956665 fisher_loss 0.287899733 triplet loss 0.652056932 l2_loss 7.84547377 fraction B 0.268316388 lossA 2.13796186 fraction A 0.0430835523\n",
      "step 1396 loss 1.13062096 fisher_loss 0.287506759 triplet loss 0.843114197 l2_loss 7.85101795 fraction B 0.284444064 lossA 2.08935332 fraction A 0.0401915684\n",
      "step 1397 loss 0.958626926 fisher_loss 0.287042737 triplet loss 0.671584189 l2_loss 7.85625935 fraction B 0.239264786 lossA 2.08770275 fraction A 0.0400685593\n",
      "step 1398 loss 1.02873528 fisher_loss 0.28725639 triplet loss 0.74147892 l2_loss 7.86338091 fraction B 0.277128488 lossA 2.14676857 fraction A 0.0436168946\n",
      "step 1399 loss 1.01271701 fisher_loss 0.288306624 triplet loss 0.724410415 l2_loss 7.87130404 fraction B 0.210926875 lossA 2.22000098 fraction A 0.0477893874\n",
      "step 1400 loss 1.27631581 fisher_loss 0.289592057 triplet loss 0.986723721 l2_loss 7.87891579 fraction B 0.299249679 lossA 2.24269271 fraction A 0.0489254743\n",
      "step 1401 loss 0.937071085 fisher_loss 0.289909929 triplet loss 0.647161126 l2_loss 7.88522959 fraction B 0.247529417 lossA 2.2252543 fraction A 0.0476340279\n",
      "step 1402 loss 1.23271263 fisher_loss 0.289262921 triplet loss 0.943449676 l2_loss 7.89138031 fraction B 0.196133494 lossA 2.2741816 fraction A 0.0501436815\n",
      "step 1403 loss 0.973773241 fisher_loss 0.289335251 triplet loss 0.684438 l2_loss 7.89902687 fraction B 0.292541474 lossA 2.30990553 fraction A 0.0520965196\n",
      "step 1404 loss 0.906121552 fisher_loss 0.289336383 triplet loss 0.616785169 l2_loss 7.90750647 fraction B 0.189499572 lossA 2.33439326 fraction A 0.0539522059\n",
      "step 1405 loss 1.14034009 fisher_loss 0.289827257 triplet loss 0.850512862 l2_loss 7.91749048 fraction B 0.214939401 lossA 2.31562257 fraction A 0.0525942557\n",
      "step 1406 loss 1.10999227 fisher_loss 0.2894862 triplet loss 0.820506036 l2_loss 7.92580557 fraction B 0.350283325 lossA 2.27676749 fraction A 0.0496744886\n",
      "step 1407 loss 1.15369964 fisher_loss 0.28849268 triplet loss 0.865206957 l2_loss 7.93211746 fraction B 0.203111604 lossA 2.22416973 fraction A 0.0460226275\n",
      "step 1408 loss 1.13971019 fisher_loss 0.287320495 triplet loss 0.852389634 l2_loss 7.93845892 fraction B 0.280045509 lossA 2.15993619 fraction A 0.0417425521\n",
      "step 1409 loss 1.13704288 fisher_loss 0.285906345 triplet loss 0.851136565 l2_loss 7.94353628 fraction B 0.271988571 lossA 2.08953166 fraction A 0.0378369503\n",
      "step 1410 loss 0.927644372 fisher_loss 0.284558833 triplet loss 0.643085539 l2_loss 7.94795609 fraction B 0.172846332 lossA 2.09835744 fraction A 0.0384024605\n",
      "step 1411 loss 1.02998674 fisher_loss 0.28439194 triplet loss 0.7455948 l2_loss 7.95463085 fraction B 0.209103957 lossA 2.15408683 fraction A 0.0415848754\n",
      "step 1412 loss 1.06855392 fisher_loss 0.285099924 triplet loss 0.783453941 l2_loss 7.96223783 fraction B 0.274260432 lossA 2.27992 fraction A 0.0486554354\n",
      "step 1413 loss 1.39867163 fisher_loss 0.286595 triplet loss 1.11207664 l2_loss 7.97076321 fraction B 0.284581423 lossA 2.35124207 fraction A 0.0527686663\n",
      "step 1414 loss 1.05881572 fisher_loss 0.28732425 triplet loss 0.771491468 l2_loss 7.97700739 fraction B 0.255514026 lossA 2.35745215 fraction A 0.0532783568\n",
      "step 1415 loss 1.03379416 fisher_loss 0.28728348 triplet loss 0.746510744 l2_loss 7.98272228 fraction B 0.230656847 lossA 2.2675159 fraction A 0.0477803685\n",
      "step 1416 loss 1.17718446 fisher_loss 0.285886705 triplet loss 0.891297758 l2_loss 7.98774433 fraction B 0.31043455 lossA 2.1597755 fraction A 0.0412662961\n",
      "step 1417 loss 0.904191732 fisher_loss 0.284753829 triplet loss 0.619437933 l2_loss 7.99188662 fraction B 0.222542122 lossA 2.05272818 fraction A 0.0355184823\n",
      "step 1418 loss 0.985957623 fisher_loss 0.284121603 triplet loss 0.701836 l2_loss 7.99651194 fraction B 0.254308015 lossA 1.98325109 fraction A 0.0322638378\n",
      "step 1419 loss 1.05171955 fisher_loss 0.284064204 triplet loss 0.767655373 l2_loss 8.00046635 fraction B 0.267453611 lossA 2.01034784 fraction A 0.0331376381\n",
      "step 1420 loss 1.17556703 fisher_loss 0.284573942 triplet loss 0.890993059 l2_loss 8.00484562 fraction B 0.284245729 lossA 2.11132455 fraction A 0.0376278\n",
      "step 1421 loss 0.853400826 fisher_loss 0.285644203 triplet loss 0.567756593 l2_loss 8.00938416 fraction B 0.182362631 lossA 2.32015705 fraction A 0.0497544669\n",
      "step 1422 loss 0.925083101 fisher_loss 0.288550377 triplet loss 0.636532724 l2_loss 8.0178833 fraction B 0.193377063 lossA 2.45420027 fraction A 0.0591466129\n",
      "step 1423 loss 1.0777142 fisher_loss 0.29115358 triplet loss 0.786560595 l2_loss 8.02535 fraction B 0.27957055 lossA 2.4873004 fraction A 0.0616057\n",
      "step 1424 loss 1.02907944 fisher_loss 0.292042106 triplet loss 0.737037361 l2_loss 8.03105 fraction B 0.263826 lossA 2.44963169 fraction A 0.0588629171\n",
      "step 1425 loss 1.06371593 fisher_loss 0.291110575 triplet loss 0.77260536 l2_loss 8.0346117 fraction B 0.205626518 lossA 2.37322426 fraction A 0.0533610173\n",
      "step 1426 loss 1.05305111 fisher_loss 0.289558619 triplet loss 0.763492465 l2_loss 8.03745842 fraction B 0.26974079 lossA 2.32594299 fraction A 0.0506067872\n",
      "step 1427 loss 1.20144367 fisher_loss 0.288764149 triplet loss 0.912679553 l2_loss 8.04167461 fraction B 0.188113406 lossA 2.29094505 fraction A 0.0488024652\n",
      "step 1428 loss 0.997879863 fisher_loss 0.287904352 triplet loss 0.709975481 l2_loss 8.04536819 fraction B 0.260507911 lossA 2.29015303 fraction A 0.0492131561\n",
      "step 1429 loss 0.978223801 fisher_loss 0.28775546 triplet loss 0.690468311 l2_loss 8.05028439 fraction B 0.252182096 lossA 2.24543929 fraction A 0.0471984856\n",
      "step 1430 loss 1.05135334 fisher_loss 0.28714186 triplet loss 0.764211476 l2_loss 8.05553818 fraction B 0.204242811 lossA 2.15883613 fraction A 0.0427493639\n",
      "step 1431 loss 1.20793486 fisher_loss 0.286270469 triplet loss 0.921664417 l2_loss 8.06119347 fraction B 0.301366746 lossA 2.1155622 fraction A 0.0405622\n",
      "step 1432 loss 1.15373993 fisher_loss 0.285999477 triplet loss 0.867740512 l2_loss 8.06718063 fraction B 0.266018748 lossA 2.08892941 fraction A 0.0394896455\n",
      "step 1433 loss 1.12000573 fisher_loss 0.285892874 triplet loss 0.834112883 l2_loss 8.07254505 fraction B 0.294858664 lossA 2.03988695 fraction A 0.0375348888\n",
      "step 1434 loss 1.01816618 fisher_loss 0.285875946 triplet loss 0.732290208 l2_loss 8.07859707 fraction B 0.285133541 lossA 2.05261 fraction A 0.0388419144\n",
      "step 1435 loss 1.06558895 fisher_loss 0.286472917 triplet loss 0.779116035 l2_loss 8.08587074 fraction B 0.321887612 lossA 2.08454227 fraction A 0.0409901701\n",
      "step 1436 loss 0.978893399 fisher_loss 0.286895305 triplet loss 0.691998124 l2_loss 8.09266567 fraction B 0.23952581 lossA 2.10288596 fraction A 0.0426281616\n",
      "step 1437 loss 0.931824446 fisher_loss 0.287239105 triplet loss 0.644585371 l2_loss 8.0996542 fraction B 0.285109758 lossA 2.20690036 fraction A 0.0493965894\n",
      "step 1438 loss 0.955485463 fisher_loss 0.288648456 triplet loss 0.666837 l2_loss 8.10884666 fraction B 0.241869912 lossA 2.29502082 fraction A 0.0558064096\n",
      "step 1439 loss 1.19806206 fisher_loss 0.290241331 triplet loss 0.907820761 l2_loss 8.11869049 fraction B 0.291813403 lossA 2.31224823 fraction A 0.0574705154\n",
      "step 1440 loss 0.965474784 fisher_loss 0.290954113 triplet loss 0.674520671 l2_loss 8.1270895 fraction B 0.261826336 lossA 2.30269408 fraction A 0.0568255708\n",
      "step 1441 loss 1.18358505 fisher_loss 0.291093111 triplet loss 0.892491937 l2_loss 8.13487434 fraction B 0.219314933 lossA 2.30092764 fraction A 0.0565266609\n",
      "step 1442 loss 1.07124448 fisher_loss 0.29091233 triplet loss 0.780332208 l2_loss 8.14176655 fraction B 0.342474103 lossA 2.29061174 fraction A 0.055321604\n",
      "step 1443 loss 1.20281959 fisher_loss 0.290259093 triplet loss 0.912560463 l2_loss 8.14810467 fraction B 0.359135509 lossA 2.22968674 fraction A 0.0504968874\n",
      "step 1444 loss 1.03972173 fisher_loss 0.288958281 triplet loss 0.750763476 l2_loss 8.15274906 fraction B 0.296231896 lossA 2.18451905 fraction A 0.0470409915\n",
      "step 1445 loss 1.08414543 fisher_loss 0.288084954 triplet loss 0.796060443 l2_loss 8.15799904 fraction B 0.199719444 lossA 2.13408279 fraction A 0.0436657593\n",
      "step 1446 loss 0.945830703 fisher_loss 0.287400812 triplet loss 0.658429921 l2_loss 8.16340542 fraction B 0.174229518 lossA 2.10143924 fraction A 0.0414192639\n",
      "step 1447 loss 1.0889622 fisher_loss 0.287624091 triplet loss 0.801338136 l2_loss 8.17197514 fraction B 0.23560825 lossA 2.10814476 fraction A 0.0417656265\n",
      "step 1448 loss 0.896108389 fisher_loss 0.288428128 triplet loss 0.607680261 l2_loss 8.18069363 fraction B 0.209112242 lossA 2.25435019 fraction A 0.0504531674\n",
      "step 1449 loss 1.0153321 fisher_loss 0.29150781 triplet loss 0.723824263 l2_loss 8.19463 fraction B 0.241467506 lossA 2.34431434 fraction A 0.0562835\n",
      "step 1450 loss 1.14341807 fisher_loss 0.29395923 triplet loss 0.849458873 l2_loss 8.20723057 fraction B 0.180351347 lossA 2.37248969 fraction A 0.0580713041\n",
      "step 1451 loss 1.17108297 fisher_loss 0.294899762 triplet loss 0.876183271 l2_loss 8.21773815 fraction B 0.388629943 lossA 2.34687448 fraction A 0.0562982783\n",
      "step 1452 loss 1.09662056 fisher_loss 0.294624865 triplet loss 0.801995754 l2_loss 8.2261982 fraction B 0.173219338 lossA 2.25855851 fraction A 0.0503909364\n",
      "step 1453 loss 1.02243114 fisher_loss 0.293428 triplet loss 0.729003191 l2_loss 8.23327541 fraction B 0.198806837 lossA 2.17803574 fraction A 0.044866\n",
      "step 1454 loss 1.10113239 fisher_loss 0.291963518 triplet loss 0.809168875 l2_loss 8.23982716 fraction B 0.328154594 lossA 2.09684563 fraction A 0.0395667665\n",
      "step 1455 loss 1.05339634 fisher_loss 0.290480971 triplet loss 0.762915373 l2_loss 8.24530888 fraction B 0.279815346 lossA 2.03775907 fraction A 0.0362769105\n",
      "step 1456 loss 0.961159587 fisher_loss 0.289232552 triplet loss 0.671927035 l2_loss 8.25039291 fraction B 0.22956875 lossA 1.98362243 fraction A 0.033926677\n",
      "step 1457 loss 0.847642 fisher_loss 0.288546503 triplet loss 0.559095502 l2_loss 8.25647354 fraction B 0.185562253 lossA 2.09377813 fraction A 0.0395171382\n",
      "step 1458 loss 0.902467847 fisher_loss 0.289703041 triplet loss 0.612764776 l2_loss 8.26586819 fraction B 0.250247926 lossA 2.24095702 fraction A 0.0487240776\n",
      "step 1459 loss 0.95004797 fisher_loss 0.292264313 triplet loss 0.657783687 l2_loss 8.27722168 fraction B 0.230588585 lossA 2.35061812 fraction A 0.0560002029\n",
      "step 1460 loss 1.12533367 fisher_loss 0.294655353 triplet loss 0.830678344 l2_loss 8.28871822 fraction B 0.215898082 lossA 2.41947579 fraction A 0.0612529963\n",
      "step 1461 loss 1.05099893 fisher_loss 0.296275526 triplet loss 0.75472343 l2_loss 8.29892635 fraction B 0.227690637 lossA 2.4083097 fraction A 0.0605877303\n",
      "step 1462 loss 1.11462939 fisher_loss 0.296164572 triplet loss 0.818464816 l2_loss 8.30671597 fraction B 0.294065446 lossA 2.32811284 fraction A 0.0551074222\n",
      "step 1463 loss 1.04477382 fisher_loss 0.294538915 triplet loss 0.750234842 l2_loss 8.31250477 fraction B 0.297421843 lossA 2.18380499 fraction A 0.0454573333\n",
      "step 1464 loss 1.10082126 fisher_loss 0.292058915 triplet loss 0.808762312 l2_loss 8.31742477 fraction B 0.242074922 lossA 1.9934063 fraction A 0.0346835107\n",
      "step 1465 loss 1.39487362 fisher_loss 0.289736211 triplet loss 1.10513735 l2_loss 8.32182693 fraction B 0.231088862 lossA 1.77487767 fraction A 0.0256887674\n",
      "step 1466 loss 1.06333148 fisher_loss 0.287934691 triplet loss 0.775396764 l2_loss 8.32555389 fraction B 0.299918294 lossA 1.73297691 fraction A 0.0242012106\n",
      "step 1467 loss 1.02412224 fisher_loss 0.287535101 triplet loss 0.736587107 l2_loss 8.3313179 fraction B 0.342789829 lossA 1.84542954 fraction A 0.0279202294\n",
      "step 1468 loss 1.02593505 fisher_loss 0.287964255 triplet loss 0.737970769 l2_loss 8.33827496 fraction B 0.248216912 lossA 2.03357959 fraction A 0.0358498842\n",
      "step 1469 loss 1.31536674 fisher_loss 0.289600551 triplet loss 1.02576613 l2_loss 8.34688091 fraction B 0.339487493 lossA 2.19879293 fraction A 0.0454378463\n",
      "step 1470 loss 1.19641507 fisher_loss 0.291847497 triplet loss 0.904567599 l2_loss 8.3556118 fraction B 0.293459922 lossA 2.29360485 fraction A 0.051550895\n",
      "step 1471 loss 1.2825706 fisher_loss 0.293340057 triplet loss 0.989230514 l2_loss 8.36234856 fraction B 0.240936771 lossA 2.31575966 fraction A 0.0531495102\n",
      "step 1472 loss 1.00528622 fisher_loss 0.293787599 triplet loss 0.711498618 l2_loss 8.36719799 fraction B 0.135621309 lossA 2.29404092 fraction A 0.0518566892\n",
      "step 1473 loss 0.960626841 fisher_loss 0.29374674 triplet loss 0.666880131 l2_loss 8.3724165 fraction B 0.254297733 lossA 2.22389221 fraction A 0.0474762768\n",
      "step 1474 loss 1.09536743 fisher_loss 0.292799801 triplet loss 0.802567601 l2_loss 8.37593746 fraction B 0.165478155 lossA 2.16792226 fraction A 0.0440724343\n",
      "step 1475 loss 1.4963851 fisher_loss 0.292072147 triplet loss 1.20431292 l2_loss 8.37983322 fraction B 0.324665636 lossA 2.10691929 fraction A 0.0408079363\n",
      "step 1476 loss 0.910615206 fisher_loss 0.291075855 triplet loss 0.61953932 l2_loss 8.38227367 fraction B 0.238113239 lossA 2.11410427 fraction A 0.0410847478\n",
      "step 1477 loss 1.01123118 fisher_loss 0.290858179 triplet loss 0.720373 l2_loss 8.38747215 fraction B 0.179306924 lossA 2.18204761 fraction A 0.044566147\n",
      "step 1478 loss 1.20361662 fisher_loss 0.291240126 triplet loss 0.912376463 l2_loss 8.39483356 fraction B 0.216467768 lossA 2.20524979 fraction A 0.0456651822\n",
      "step 1479 loss 1.00319362 fisher_loss 0.290976077 triplet loss 0.71221751 l2_loss 8.4009409 fraction B 0.297541082 lossA 2.2528882 fraction A 0.0482033752\n",
      "step 1480 loss 1.0084914 fisher_loss 0.290832609 triplet loss 0.717658818 l2_loss 8.40738583 fraction B 0.327319592 lossA 2.27544332 fraction A 0.0490332022\n",
      "step 1481 loss 1.12978268 fisher_loss 0.290223718 triplet loss 0.839559 l2_loss 8.41365719 fraction B 0.306325614 lossA 2.24521875 fraction A 0.0466491692\n",
      "step 1482 loss 1.15726185 fisher_loss 0.289258599 triplet loss 0.868003309 l2_loss 8.4189 fraction B 0.381332487 lossA 2.19287539 fraction A 0.0429287031\n",
      "step 1483 loss 1.06680942 fisher_loss 0.287653565 triplet loss 0.77915591 l2_loss 8.4221859 fraction B 0.417120129 lossA 2.13892841 fraction A 0.0392990075\n",
      "step 1484 loss 1.02903724 fisher_loss 0.286237121 triplet loss 0.742800117 l2_loss 8.42462444 fraction B 0.265446 lossA 2.10210395 fraction A 0.0370245129\n",
      "step 1485 loss 1.14752781 fisher_loss 0.285176188 triplet loss 0.862351596 l2_loss 8.42690659 fraction B 0.294596 lossA 2.09749103 fraction A 0.036588572\n",
      "step 1486 loss 1.17586601 fisher_loss 0.284244329 triplet loss 0.891621649 l2_loss 8.4290123 fraction B 0.350631177 lossA 2.10442686 fraction A 0.0366004892\n",
      "step 1487 loss 1.04094481 fisher_loss 0.283213586 triplet loss 0.757731199 l2_loss 8.43030834 fraction B 0.226154402 lossA 2.16438246 fraction A 0.039498955\n",
      "step 1488 loss 1.07306838 fisher_loss 0.282759875 triplet loss 0.790308475 l2_loss 8.43328 fraction B 0.244371653 lossA 2.21744013 fraction A 0.0422048643\n",
      "step 1489 loss 1.10481334 fisher_loss 0.282247961 triplet loss 0.822565377 l2_loss 8.43625736 fraction B 0.193878829 lossA 2.2875545 fraction A 0.046135243\n",
      "step 1490 loss 1.10366046 fisher_loss 0.28198415 triplet loss 0.821676314 l2_loss 8.43985939 fraction B 0.258906871 lossA 2.33509111 fraction A 0.048604507\n",
      "step 1491 loss 1.19088459 fisher_loss 0.281363606 triplet loss 0.909520924 l2_loss 8.44262 fraction B 0.268036693 lossA 2.38671374 fraction A 0.0516449288\n",
      "step 1492 loss 1.20096755 fisher_loss 0.280936688 triplet loss 0.920030892 l2_loss 8.44608593 fraction B 0.228946701 lossA 2.4153378 fraction A 0.0536761545\n",
      "step 1493 loss 1.27337718 fisher_loss 0.280705601 triplet loss 0.992671609 l2_loss 8.45105171 fraction B 0.22158815 lossA 2.39427638 fraction A 0.0524282083\n",
      "step 1494 loss 1.04274559 fisher_loss 0.280145139 triplet loss 0.762600482 l2_loss 8.45516586 fraction B 0.205899388 lossA 2.39042354 fraction A 0.0522892587\n",
      "step 1495 loss 1.03669453 fisher_loss 0.279744178 triplet loss 0.756950319 l2_loss 8.46047401 fraction B 0.240686268 lossA 2.31892657 fraction A 0.0482694469\n",
      "step 1496 loss 0.859606 fisher_loss 0.27868703 triplet loss 0.580919 l2_loss 8.46548271 fraction B 0.173043296 lossA 2.23857665 fraction A 0.0438856669\n",
      "step 1497 loss 1.03870523 fisher_loss 0.277947068 triplet loss 0.760758162 l2_loss 8.47177792 fraction B 0.321025312 lossA 2.1337862 fraction A 0.0385740809\n",
      "step 1498 loss 0.914672375 fisher_loss 0.277137965 triplet loss 0.63753438 l2_loss 8.47766304 fraction B 0.221338347 lossA 2.12591743 fraction A 0.0385300703\n",
      "step 1499 loss 1.10138333 fisher_loss 0.277583629 triplet loss 0.823799729 l2_loss 8.4864006 fraction B 0.253258258 lossA 2.17279 fraction A 0.0417233929\n",
      "step 1500 loss 1.02286828 fisher_loss 0.278743327 triplet loss 0.744124949 l2_loss 8.49598503 fraction B 0.242268667 lossA 2.24771547 fraction A 0.0462727062\n",
      "step 1501 loss 0.979202032 fisher_loss 0.279784054 triplet loss 0.699417949 l2_loss 8.50446892 fraction B 0.202520564 lossA 2.27945542 fraction A 0.0480332747\n",
      "step 1502 loss 0.965715289 fisher_loss 0.280326247 triplet loss 0.685389042 l2_loss 8.5126543 fraction B 0.228742644 lossA 2.30921078 fraction A 0.049687203\n",
      "step 1503 loss 0.937364459 fisher_loss 0.280741483 triplet loss 0.656623 l2_loss 8.52049446 fraction B 0.250349581 lossA 2.28466105 fraction A 0.0477585979\n",
      "step 1504 loss 1.14293718 fisher_loss 0.280389607 triplet loss 0.862547517 l2_loss 8.52648163 fraction B 0.361736476 lossA 2.26949215 fraction A 0.0463877507\n",
      "step 1505 loss 1.05623257 fisher_loss 0.280136049 triplet loss 0.776096523 l2_loss 8.53192711 fraction B 0.191731751 lossA 2.30858088 fraction A 0.048626963\n",
      "step 1506 loss 0.92007339 fisher_loss 0.280759305 triplet loss 0.639314055 l2_loss 8.53854656 fraction B 0.166956559 lossA 2.31810474 fraction A 0.0490916297\n",
      "step 1507 loss 1.14824748 fisher_loss 0.28115648 triplet loss 0.86709094 l2_loss 8.54488 fraction B 0.284958184 lossA 2.34305286 fraction A 0.0508848689\n",
      "step 1508 loss 1.04825211 fisher_loss 0.282018065 triplet loss 0.7662341 l2_loss 8.55153942 fraction B 0.229056597 lossA 2.32894254 fraction A 0.0502106212\n",
      "step 1509 loss 0.884185791 fisher_loss 0.282329112 triplet loss 0.601856649 l2_loss 8.55829716 fraction B 0.249191627 lossA 2.29191 fraction A 0.0480140783\n",
      "step 1510 loss 1.01861382 fisher_loss 0.282368362 triplet loss 0.736245453 l2_loss 8.56498909 fraction B 0.208772555 lossA 2.21813965 fraction A 0.0435109101\n",
      "step 1511 loss 0.902817309 fisher_loss 0.282057 triplet loss 0.620760322 l2_loss 8.57095528 fraction B 0.189011306 lossA 2.17689371 fraction A 0.0413649678\n",
      "step 1512 loss 0.977600574 fisher_loss 0.282643467 triplet loss 0.694957078 l2_loss 8.57912922 fraction B 0.258383244 lossA 2.14163375 fraction A 0.039696008\n",
      "step 1513 loss 1.06982982 fisher_loss 0.283221632 triplet loss 0.78660816 l2_loss 8.58777714 fraction B 0.209158257 lossA 2.16837263 fraction A 0.0414575152\n",
      "step 1514 loss 1.08197284 fisher_loss 0.284353971 triplet loss 0.797618806 l2_loss 8.59697914 fraction B 0.170139909 lossA 2.23419666 fraction A 0.0459302925\n",
      "step 1515 loss 0.972255111 fisher_loss 0.286056608 triplet loss 0.686198473 l2_loss 8.60741 fraction B 0.238704279 lossA 2.26610255 fraction A 0.0481871143\n",
      "step 1516 loss 0.925941825 fisher_loss 0.286749333 triplet loss 0.639192522 l2_loss 8.61595 fraction B 0.21806626 lossA 2.26186 fraction A 0.0479722023\n",
      "step 1517 loss 1.2220993 fisher_loss 0.287158936 triplet loss 0.934940398 l2_loss 8.624403 fraction B 0.276157767 lossA 2.24886012 fraction A 0.0473941602\n",
      "step 1518 loss 1.21819019 fisher_loss 0.287218869 triplet loss 0.930971384 l2_loss 8.63127232 fraction B 0.188233629 lossA 2.2095778 fraction A 0.0448211543\n",
      "step 1519 loss 1.39882588 fisher_loss 0.286636591 triplet loss 1.11218929 l2_loss 8.63716221 fraction B 0.255000621 lossA 2.15490723 fraction A 0.0419523567\n",
      "step 1520 loss 0.995754182 fisher_loss 0.286082268 triplet loss 0.709671915 l2_loss 8.64290524 fraction B 0.198281676 lossA 2.09019351 fraction A 0.0387771837\n",
      "step 1521 loss 0.919959128 fisher_loss 0.285357296 triplet loss 0.634601831 l2_loss 8.64861298 fraction B 0.203047395 lossA 2.00576019 fraction A 0.0350118726\n",
      "step 1522 loss 1.01624274 fisher_loss 0.284306049 triplet loss 0.731936753 l2_loss 8.65303421 fraction B 0.229076087 lossA 1.94853449 fraction A 0.0328844748\n",
      "step 1523 loss 1.16798925 fisher_loss 0.283787847 triplet loss 0.884201407 l2_loss 8.65726852 fraction B 0.410113961 lossA 1.91885912 fraction A 0.0318162702\n",
      "step 1524 loss 1.07503533 fisher_loss 0.283341914 triplet loss 0.791693389 l2_loss 8.66121578 fraction B 0.265770614 lossA 1.96601784 fraction A 0.0340602286\n",
      "step 1525 loss 1.04633725 fisher_loss 0.283409446 triplet loss 0.76292783 l2_loss 8.66568 fraction B 0.247049183 lossA 2.01136875 fraction A 0.036329\n",
      "step 1526 loss 0.896149039 fisher_loss 0.283628047 triplet loss 0.612521 l2_loss 8.67063236 fraction B 0.150555789 lossA 2.11507583 fraction A 0.0421898663\n",
      "step 1527 loss 1.1918025 fisher_loss 0.284859717 triplet loss 0.906942725 l2_loss 8.67800331 fraction B 0.302943856 lossA 2.23245645 fraction A 0.0501739644\n",
      "step 1528 loss 1.11493218 fisher_loss 0.286549121 triplet loss 0.828383088 l2_loss 8.68645 fraction B 0.295384556 lossA 2.31356478 fraction A 0.0560220778\n",
      "step 1529 loss 0.974641263 fisher_loss 0.287727356 triplet loss 0.686913908 l2_loss 8.69442272 fraction B 0.155045643 lossA 2.35540724 fraction A 0.0592833906\n",
      "step 1530 loss 1.1905539 fisher_loss 0.288279831 triplet loss 0.902274072 l2_loss 8.70115662 fraction B 0.29959482 lossA 2.34880328 fraction A 0.0583868399\n",
      "step 1531 loss 1.19312954 fisher_loss 0.28756389 triplet loss 0.905565619 l2_loss 8.70579147 fraction B 0.253513396 lossA 2.28930926 fraction A 0.0532973\n",
      "step 1532 loss 1.23305798 fisher_loss 0.285966754 triplet loss 0.947091162 l2_loss 8.70910931 fraction B 0.242287576 lossA 2.18582106 fraction A 0.0460426584\n",
      "step 1533 loss 1.09745526 fisher_loss 0.284310073 triplet loss 0.813145161 l2_loss 8.71275806 fraction B 0.201076031 lossA 2.10488987 fraction A 0.0405233689\n",
      "step 1534 loss 0.858428419 fisher_loss 0.283013165 triplet loss 0.575415254 l2_loss 8.71663284 fraction B 0.205964059 lossA 2.0078032 fraction A 0.0353228413\n",
      "step 1535 loss 1.10659182 fisher_loss 0.282180041 triplet loss 0.824411809 l2_loss 8.72286892 fraction B 0.318915963 lossA 2.01208901 fraction A 0.0353816338\n",
      "step 1536 loss 1.03032827 fisher_loss 0.282265067 triplet loss 0.748063266 l2_loss 8.73119 fraction B 0.198505178 lossA 2.15777826 fraction A 0.0430929\n",
      "step 1537 loss 0.991237819 fisher_loss 0.283989668 triplet loss 0.707248151 l2_loss 8.7429924 fraction B 0.228020608 lossA 2.30896235 fraction A 0.0526609756\n",
      "step 1538 loss 1.17441022 fisher_loss 0.286309391 triplet loss 0.888100863 l2_loss 8.75513744 fraction B 0.343298525 lossA 2.38030744 fraction A 0.0575156845\n",
      "step 1539 loss 1.01780343 fisher_loss 0.287530452 triplet loss 0.730273 l2_loss 8.76555157 fraction B 0.239604831 lossA 2.3544054 fraction A 0.0554083213\n",
      "step 1540 loss 1.18391764 fisher_loss 0.287038445 triplet loss 0.896879196 l2_loss 8.77365589 fraction B 0.222561479 lossA 2.31690121 fraction A 0.0529607199\n",
      "step 1541 loss 1.07621777 fisher_loss 0.286442786 triplet loss 0.789774954 l2_loss 8.78139687 fraction B 0.178493366 lossA 2.27544284 fraction A 0.0504058599\n",
      "step 1542 loss 1.01183391 fisher_loss 0.285663843 triplet loss 0.726170123 l2_loss 8.79013157 fraction B 0.202058464 lossA 2.19722772 fraction A 0.0455043167\n",
      "step 1543 loss 1.01730418 fisher_loss 0.284697831 triplet loss 0.732606411 l2_loss 8.79798222 fraction B 0.298669517 lossA 2.08719397 fraction A 0.0390255265\n",
      "step 1544 loss 1.15057492 fisher_loss 0.283622712 triplet loss 0.866952181 l2_loss 8.80617332 fraction B 0.312353194 lossA 2.02296185 fraction A 0.0358244888\n",
      "step 1545 loss 0.960445404 fisher_loss 0.283146262 triplet loss 0.677299142 l2_loss 8.81480217 fraction B 0.280627757 lossA 2.0587039 fraction A 0.0377042294\n",
      "step 1546 loss 0.990993 fisher_loss 0.283736736 triplet loss 0.707256317 l2_loss 8.82442 fraction B 0.276767671 lossA 2.1111598 fraction A 0.0406048335\n",
      "step 1547 loss 1.09927428 fisher_loss 0.284559458 triplet loss 0.814714789 l2_loss 8.83393192 fraction B 0.281177819 lossA 2.16780233 fraction A 0.0442058779\n",
      "step 1548 loss 1.11067796 fisher_loss 0.28550297 triplet loss 0.825174928 l2_loss 8.84292 fraction B 0.345523417 lossA 2.18812418 fraction A 0.0453742743\n",
      "step 1549 loss 1.00114274 fisher_loss 0.285803348 triplet loss 0.715339422 l2_loss 8.85093 fraction B 0.293594301 lossA 2.21844125 fraction A 0.047302518\n",
      "step 1550 loss 1.2868129 fisher_loss 0.286067098 triplet loss 1.00074577 l2_loss 8.85872555 fraction B 0.145766303 lossA 2.24584055 fraction A 0.0490761623\n",
      "step 1551 loss 1.08018148 fisher_loss 0.286159873 triplet loss 0.794021606 l2_loss 8.86605167 fraction B 0.209091261 lossA 2.28395891 fraction A 0.0509192459\n",
      "step 1552 loss 1.17432547 fisher_loss 0.285863429 triplet loss 0.888462067 l2_loss 8.87098503 fraction B 0.272585273 lossA 2.28931594 fraction A 0.0506089628\n",
      "step 1553 loss 1.34597528 fisher_loss 0.284880072 triplet loss 1.06109524 l2_loss 8.87476158 fraction B 0.286140501 lossA 2.25914979 fraction A 0.0479736142\n",
      "step 1554 loss 0.970685 fisher_loss 0.283197314 triplet loss 0.687487662 l2_loss 8.87614632 fraction B 0.291090846 lossA 2.16079736 fraction A 0.0416516326\n",
      "step 1555 loss 0.873391807 fisher_loss 0.281155944 triplet loss 0.592235863 l2_loss 8.87634563 fraction B 0.228974745 lossA 2.08392334 fraction A 0.0372839756\n",
      "step 1556 loss 1.09120846 fisher_loss 0.279952586 triplet loss 0.811255872 l2_loss 8.87811 fraction B 0.265633553 lossA 2.02696085 fraction A 0.034412168\n",
      "step 1557 loss 1.00931704 fisher_loss 0.279075891 triplet loss 0.73024112 l2_loss 8.88033485 fraction B 0.267400831 lossA 2.03073215 fraction A 0.0345889\n",
      "step 1558 loss 1.11395645 fisher_loss 0.279048204 triplet loss 0.834908307 l2_loss 8.88337231 fraction B 0.360435367 lossA 2.07604432 fraction A 0.0368289463\n",
      "step 1559 loss 0.97676456 fisher_loss 0.279392272 triplet loss 0.697372317 l2_loss 8.8871994 fraction B 0.305167735 lossA 2.17285466 fraction A 0.0419588387\n",
      "step 1560 loss 1.04913843 fisher_loss 0.280322462 triplet loss 0.768815935 l2_loss 8.89214611 fraction B 0.216383249 lossA 2.26251149 fraction A 0.0474847555\n",
      "step 1561 loss 1.33101916 fisher_loss 0.281610429 triplet loss 1.04940867 l2_loss 8.89815426 fraction B 0.217632204 lossA 2.35518456 fraction A 0.0536606871\n",
      "step 1562 loss 1.06356668 fisher_loss 0.283031642 triplet loss 0.780535 l2_loss 8.90419102 fraction B 0.322409332 lossA 2.3609283 fraction A 0.0542992577\n",
      "step 1563 loss 1.07160378 fisher_loss 0.283359259 triplet loss 0.788244486 l2_loss 8.9088583 fraction B 0.336256683 lossA 2.34248066 fraction A 0.0532508977\n",
      "step 1564 loss 1.091 fisher_loss 0.283304274 triplet loss 0.807695687 l2_loss 8.91321659 fraction B 0.314465791 lossA 2.31739283 fraction A 0.0517004579\n",
      "step 1565 loss 0.990502 fisher_loss 0.282928973 triplet loss 0.707573056 l2_loss 8.91680336 fraction B 0.220196038 lossA 2.25414038 fraction A 0.0478364043\n",
      "step 1566 loss 1.061584 fisher_loss 0.282119632 triplet loss 0.779464364 l2_loss 8.92110443 fraction B 0.319927782 lossA 2.16733813 fraction A 0.0425954163\n",
      "step 1567 loss 1.03696918 fisher_loss 0.280656129 triplet loss 0.756313 l2_loss 8.92482471 fraction B 0.30097127 lossA 2.18975091 fraction A 0.0440983325\n",
      "step 1568 loss 1.0636543 fisher_loss 0.280617267 triplet loss 0.783037066 l2_loss 8.92972183 fraction B 0.230833113 lossA 2.25039554 fraction A 0.0480800755\n",
      "step 1569 loss 0.911460817 fisher_loss 0.281375408 triplet loss 0.630085409 l2_loss 8.93608856 fraction B 0.135850102 lossA 2.27169824 fraction A 0.0496962927\n",
      "step 1570 loss 0.944416642 fisher_loss 0.2823309 triplet loss 0.662085772 l2_loss 8.94427 fraction B 0.278444409 lossA 2.27730632 fraction A 0.0502315573\n",
      "step 1571 loss 1.01777077 fisher_loss 0.283006102 triplet loss 0.734764695 l2_loss 8.95158291 fraction B 0.232716858 lossA 2.24590969 fraction A 0.0486272536\n",
      "step 1572 loss 0.962501049 fisher_loss 0.283322424 triplet loss 0.679178596 l2_loss 8.95902634 fraction B 0.191581324 lossA 2.2244091 fraction A 0.0476917662\n",
      "step 1573 loss 1.08336163 fisher_loss 0.283861488 triplet loss 0.799500108 l2_loss 8.96788788 fraction B 0.324156523 lossA 2.17202973 fraction A 0.0447551571\n",
      "step 1574 loss 0.988677621 fisher_loss 0.283724755 triplet loss 0.704952836 l2_loss 8.97650909 fraction B 0.282636344 lossA 2.16463041 fraction A 0.0448182933\n",
      "step 1575 loss 1.29113555 fisher_loss 0.284332 triplet loss 1.00680351 l2_loss 8.98573875 fraction B 0.254944205 lossA 2.18326592 fraction A 0.0463639535\n",
      "step 1576 loss 1.12815404 fisher_loss 0.284938425 triplet loss 0.843215644 l2_loss 8.99439812 fraction B 0.213836983 lossA 2.2175827 fraction A 0.0489351824\n",
      "step 1577 loss 1.05105042 fisher_loss 0.285542905 triplet loss 0.765507579 l2_loss 9.0028944 fraction B 0.190632805 lossA 2.22651649 fraction A 0.0498685315\n",
      "step 1578 loss 1.12759924 fisher_loss 0.286042869 triplet loss 0.841556311 l2_loss 9.01181602 fraction B 0.286543757 lossA 2.22368598 fraction A 0.0496864766\n",
      "step 1579 loss 0.996505797 fisher_loss 0.285876632 triplet loss 0.710629165 l2_loss 9.01932 fraction B 0.257122368 lossA 2.20910978 fraction A 0.0487667099\n",
      "step 1580 loss 1.08078301 fisher_loss 0.285751045 triplet loss 0.795031965 l2_loss 9.02709484 fraction B 0.246507689 lossA 2.23238635 fraction A 0.0502906367\n",
      "step 1581 loss 1.02959168 fisher_loss 0.286017239 triplet loss 0.74357444 l2_loss 9.03519917 fraction B 0.206380591 lossA 2.21377349 fraction A 0.0491189063\n",
      "step 1582 loss 1.13041329 fisher_loss 0.285671 triplet loss 0.844742298 l2_loss 9.04269123 fraction B 0.250543237 lossA 2.19897318 fraction A 0.0482247472\n",
      "step 1583 loss 1.36506438 fisher_loss 0.285320818 triplet loss 1.07974362 l2_loss 9.05009 fraction B 0.365485549 lossA 2.17139888 fraction A 0.0465828814\n",
      "step 1584 loss 1.04362822 fisher_loss 0.284617066 triplet loss 0.75901109 l2_loss 9.05609 fraction B 0.251677424 lossA 2.13678098 fraction A 0.0444157869\n",
      "step 1585 loss 1.2966584 fisher_loss 0.283995122 triplet loss 1.01266325 l2_loss 9.06222057 fraction B 0.277485669 lossA 2.09720778 fraction A 0.0420951806\n",
      "step 1586 loss 0.909470677 fisher_loss 0.283197492 triplet loss 0.626273215 l2_loss 9.0675 fraction B 0.185460851 lossA 2.1127274 fraction A 0.0430404469\n",
      "step 1587 loss 1.03030598 fisher_loss 0.283088833 triplet loss 0.747217119 l2_loss 9.07401848 fraction B 0.256899804 lossA 2.1596446 fraction A 0.0462568775\n",
      "step 1588 loss 1.00537038 fisher_loss 0.283464164 triplet loss 0.721906245 l2_loss 9.08093548 fraction B 0.17327787 lossA 2.20571184 fraction A 0.0493379794\n",
      "step 1589 loss 0.90712297 fisher_loss 0.284129709 triplet loss 0.62299329 l2_loss 9.08805 fraction B 0.115896486 lossA 2.20874 fraction A 0.0495787524\n",
      "step 1590 loss 0.802571833 fisher_loss 0.284341156 triplet loss 0.518230677 l2_loss 9.09541512 fraction B 0.196499899 lossA 2.13725019 fraction A 0.0447730161\n",
      "step 1591 loss 0.96363461 fisher_loss 0.283722162 triplet loss 0.679912448 l2_loss 9.10242081 fraction B 0.263561308 lossA 2.03062153 fraction A 0.0379656143\n",
      "step 1592 loss 1.16981363 fisher_loss 0.282949269 triplet loss 0.886864424 l2_loss 9.10874939 fraction B 0.233317867 lossA 1.98472035 fraction A 0.0352919102\n",
      "step 1593 loss 0.955644429 fisher_loss 0.282740951 triplet loss 0.672903478 l2_loss 9.11572933 fraction B 0.316397756 lossA 1.99455976 fraction A 0.0356360972\n",
      "step 1594 loss 0.834629178 fisher_loss 0.283163279 triplet loss 0.551465929 l2_loss 9.12406 fraction B 0.229935348 lossA 2.07082462 fraction A 0.0395312682\n",
      "step 1595 loss 1.03547 fisher_loss 0.284380734 triplet loss 0.751089215 l2_loss 9.13269424 fraction B 0.255343586 lossA 2.18758774 fraction A 0.0466340631\n",
      "step 1596 loss 1.11965489 fisher_loss 0.286002964 triplet loss 0.8336519 l2_loss 9.14189911 fraction B 0.25104335 lossA 2.28865719 fraction A 0.0530163944\n",
      "step 1597 loss 1.23048711 fisher_loss 0.287379026 triplet loss 0.943108 l2_loss 9.15090847 fraction B 0.236337915 lossA 2.34571433 fraction A 0.0568367653\n",
      "step 1598 loss 0.967980802 fisher_loss 0.287975788 triplet loss 0.680005 l2_loss 9.15801048 fraction B 0.178312019 lossA 2.36216235 fraction A 0.0577342175\n",
      "step 1599 loss 1.08372843 fisher_loss 0.287970632 triplet loss 0.795757771 l2_loss 9.16463947 fraction B 0.299641252 lossA 2.36596894 fraction A 0.0578229614\n",
      "step 1600 loss 1.17355514 fisher_loss 0.287792534 triplet loss 0.885762572 l2_loss 9.17153072 fraction B 0.319601357 lossA 2.33363533 fraction A 0.0552933887\n",
      "step 1601 loss 1.11016262 fisher_loss 0.286850125 triplet loss 0.823312461 l2_loss 9.17706776 fraction B 0.213162303 lossA 2.25380611 fraction A 0.0494178869\n",
      "step 1602 loss 1.11434913 fisher_loss 0.285077542 triplet loss 0.829271555 l2_loss 9.182127 fraction B 0.34763 lossA 2.17254949 fraction A 0.043918632\n",
      "step 1603 loss 0.99646312 fisher_loss 0.283385038 triplet loss 0.713078082 l2_loss 9.18644619 fraction B 0.201677442 lossA 2.06968641 fraction A 0.0376610905\n",
      "step 1604 loss 1.17197764 fisher_loss 0.28211081 triplet loss 0.889866829 l2_loss 9.19190121 fraction B 0.30032447 lossA 2.04952836 fraction A 0.0364542231\n",
      "step 1605 loss 1.06646788 fisher_loss 0.28162834 triplet loss 0.784839511 l2_loss 9.19829845 fraction B 0.276707768 lossA 2.10490036 fraction A 0.0392851345\n",
      "step 1606 loss 0.947874129 fisher_loss 0.28192091 triplet loss 0.665953219 l2_loss 9.20569611 fraction B 0.214406535 lossA 2.24139357 fraction A 0.0479185954\n",
      "step 1607 loss 1.04508185 fisher_loss 0.283508778 triplet loss 0.761573136 l2_loss 9.21603584 fraction B 0.294920862 lossA 2.3418591 fraction A 0.0548598021\n",
      "step 1608 loss 1.29504061 fisher_loss 0.284927696 triplet loss 1.01011288 l2_loss 9.22590256 fraction B 0.391340554 lossA 2.41199398 fraction A 0.0602422394\n",
      "step 1609 loss 1.19498456 fisher_loss 0.285877705 triplet loss 0.909106851 l2_loss 9.23390388 fraction B 0.217550665 lossA 2.44038749 fraction A 0.0624606274\n",
      "step 1610 loss 1.05288827 fisher_loss 0.285891145 triplet loss 0.766997159 l2_loss 9.2401228 fraction B 0.269078493 lossA 2.40394568 fraction A 0.0601636022\n",
      "step 1611 loss 0.942170858 fisher_loss 0.284861088 triplet loss 0.657309771 l2_loss 9.24510765 fraction B 0.206848577 lossA 2.33536 fraction A 0.0555321686\n",
      "step 1612 loss 1.09720814 fisher_loss 0.283405513 triplet loss 0.8138026 l2_loss 9.25011921 fraction B 0.30433327 lossA 2.23017883 fraction A 0.0481911711\n",
      "step 1613 loss 1.05450261 fisher_loss 0.281134367 triplet loss 0.773368239 l2_loss 9.25355625 fraction B 0.200431466 lossA 2.1285553 fraction A 0.0417235717\n",
      "step 1614 loss 0.926208496 fisher_loss 0.279401332 triplet loss 0.646807134 l2_loss 9.2573595 fraction B 0.207091466 lossA 2.06916666 fraction A 0.0385243855\n",
      "step 1615 loss 0.817661226 fisher_loss 0.27855283 triplet loss 0.539108396 l2_loss 9.26305676 fraction B 0.2533755 lossA 2.0941 fraction A 0.0399345718\n",
      "step 1616 loss 0.998071551 fisher_loss 0.278610528 triplet loss 0.719461 l2_loss 9.27072239 fraction B 0.250246942 lossA 2.21135926 fraction A 0.047350008\n",
      "step 1617 loss 1.14121783 fisher_loss 0.279968619 triplet loss 0.861249208 l2_loss 9.28021526 fraction B 0.294376254 lossA 2.33284307 fraction A 0.0553859733\n",
      "step 1618 loss 1.28994036 fisher_loss 0.281229854 triplet loss 1.0087105 l2_loss 9.289361 fraction B 0.436947435 lossA 2.40602326 fraction A 0.0602266639\n",
      "step 1619 loss 1.1859442 fisher_loss 0.281569749 triplet loss 0.904374421 l2_loss 9.29606438 fraction B 0.339821696 lossA 2.4408288 fraction A 0.0621467568\n",
      "step 1620 loss 0.970372736 fisher_loss 0.281090915 triplet loss 0.689281821 l2_loss 9.30138397 fraction B 0.210436389 lossA 2.40906405 fraction A 0.0592023246\n",
      "step 1621 loss 1.15461242 fisher_loss 0.279740185 triplet loss 0.874872267 l2_loss 9.3055048 fraction B 0.279167026 lossA 2.36344481 fraction A 0.0553126596\n",
      "step 1622 loss 1.12105203 fisher_loss 0.278175622 triplet loss 0.842876434 l2_loss 9.30888939 fraction B 0.260762781 lossA 2.24266815 fraction A 0.0471541509\n",
      "step 1623 loss 1.07337463 fisher_loss 0.276284784 triplet loss 0.797089875 l2_loss 9.31154537 fraction B 0.19630222 lossA 2.20167899 fraction A 0.0441433936\n",
      "step 1624 loss 1.092224 fisher_loss 0.275494277 triplet loss 0.816729724 l2_loss 9.31605434 fraction B 0.275793731 lossA 2.17436194 fraction A 0.0422112\n",
      "step 1625 loss 0.799809694 fisher_loss 0.27494809 triplet loss 0.524861634 l2_loss 9.32068062 fraction B 0.219840616 lossA 2.27394652 fraction A 0.048504062\n",
      "step 1626 loss 1.24391246 fisher_loss 0.276368 triplet loss 0.967544436 l2_loss 9.32938099 fraction B 0.303228766 lossA 2.36854529 fraction A 0.054467909\n",
      "step 1627 loss 0.974246264 fisher_loss 0.277738661 triplet loss 0.696507633 l2_loss 9.33749104 fraction B 0.210625991 lossA 2.38178611 fraction A 0.0554552674\n",
      "step 1628 loss 1.0562371 fisher_loss 0.278716952 triplet loss 0.77752012 l2_loss 9.34598446 fraction B 0.159234285 lossA 2.32169938 fraction A 0.0515076444\n",
      "step 1629 loss 1.21337116 fisher_loss 0.27852872 triplet loss 0.934842408 l2_loss 9.35267639 fraction B 0.381914675 lossA 2.23856735 fraction A 0.0464009717\n",
      "step 1630 loss 0.913741231 fisher_loss 0.277998298 triplet loss 0.635742962 l2_loss 9.35856438 fraction B 0.182088777 lossA 2.12082982 fraction A 0.0395281874\n",
      "step 1631 loss 0.968021631 fisher_loss 0.277448356 triplet loss 0.690573275 l2_loss 9.36536694 fraction B 0.270545453 lossA 2.07341766 fraction A 0.0371518694\n",
      "step 1632 loss 0.885538161 fisher_loss 0.277522981 triplet loss 0.60801518 l2_loss 9.37366 fraction B 0.236923635 lossA 2.10268855 fraction A 0.0388683192\n",
      "step 1633 loss 1.01232696 fisher_loss 0.278439641 triplet loss 0.733887255 l2_loss 9.3828516 fraction B 0.23597312 lossA 2.13873243 fraction A 0.0410223752\n",
      "step 1634 loss 0.978400111 fisher_loss 0.279116601 triplet loss 0.69928354 l2_loss 9.39124298 fraction B 0.22852385 lossA 2.23814225 fraction A 0.0474394746\n",
      "step 1635 loss 1.20303929 fisher_loss 0.280909896 triplet loss 0.922129393 l2_loss 9.40153122 fraction B 0.302759737 lossA 2.29472613 fraction A 0.0516643822\n",
      "step 1636 loss 1.09700036 fisher_loss 0.282312423 triplet loss 0.814687967 l2_loss 9.41119385 fraction B 0.246653527 lossA 2.23220015 fraction A 0.0482624918\n",
      "step 1637 loss 1.12606275 fisher_loss 0.281942874 triplet loss 0.844119847 l2_loss 9.41845894 fraction B 0.200877696 lossA 2.15276575 fraction A 0.0435284041\n",
      "step 1638 loss 1.1739161 fisher_loss 0.281154752 triplet loss 0.89276129 l2_loss 9.42432 fraction B 0.240396529 lossA 2.04654431 fraction A 0.0380807295\n",
      "step 1639 loss 1.07015979 fisher_loss 0.280424 triplet loss 0.789735794 l2_loss 9.42983341 fraction B 0.275576115 lossA 1.94521856 fraction A 0.0335680358\n",
      "step 1640 loss 0.936261892 fisher_loss 0.279898852 triplet loss 0.656363 l2_loss 9.43532276 fraction B 0.224361092 lossA 1.96331608 fraction A 0.0349704325\n",
      "step 1641 loss 0.983530939 fisher_loss 0.280725658 triplet loss 0.702805281 l2_loss 9.44358 fraction B 0.236448631 lossA 2.01459813 fraction A 0.0379895568\n",
      "step 1642 loss 1.08290291 fisher_loss 0.281834543 triplet loss 0.801068306 l2_loss 9.45205593 fraction B 0.268299848 lossA 2.09513116 fraction A 0.0429641642\n",
      "step 1643 loss 1.17690349 fisher_loss 0.283040941 triplet loss 0.893862486 l2_loss 9.45980263 fraction B 0.305168241 lossA 2.12627149 fraction A 0.0451570116\n",
      "step 1644 loss 1.18866515 fisher_loss 0.283380091 triplet loss 0.90528506 l2_loss 9.46642399 fraction B 0.205484331 lossA 2.14238191 fraction A 0.0462489799\n",
      "step 1645 loss 1.08826 fisher_loss 0.283301026 triplet loss 0.804959059 l2_loss 9.47272 fraction B 0.212915778 lossA 2.16518664 fraction A 0.0474580936\n",
      "step 1646 loss 1.16390014 fisher_loss 0.283063769 triplet loss 0.880836308 l2_loss 9.47897053 fraction B 0.245265871 lossA 2.20596194 fraction A 0.0496139601\n",
      "step 1647 loss 1.16188622 fisher_loss 0.282818258 triplet loss 0.879067898 l2_loss 9.48549461 fraction B 0.333874673 lossA 2.19348788 fraction A 0.0481430292\n",
      "step 1648 loss 1.15554142 fisher_loss 0.281607777 triplet loss 0.873933613 l2_loss 9.48987675 fraction B 0.262083977 lossA 2.09853601 fraction A 0.0415014178\n",
      "step 1649 loss 0.890445709 fisher_loss 0.279597253 triplet loss 0.610848486 l2_loss 9.49315643 fraction B 0.222690329 lossA 1.99375296 fraction A 0.0357864946\n",
      "step 1650 loss 1.23766088 fisher_loss 0.277900308 triplet loss 0.959760606 l2_loss 9.49650574 fraction B 0.283630133 lossA 1.96756 fraction A 0.03439\n",
      "step 1651 loss 1.22790074 fisher_loss 0.277145714 triplet loss 0.950755 l2_loss 9.50120735 fraction B 0.308738321 lossA 2.0306623 fraction A 0.037424013\n",
      "step 1652 loss 1.10236895 fisher_loss 0.277261972 triplet loss 0.825107 l2_loss 9.50689888 fraction B 0.25992772 lossA 2.12984061 fraction A 0.0427583084\n",
      "step 1653 loss 0.985173225 fisher_loss 0.27771154 triplet loss 0.707461715 l2_loss 9.5125351 fraction B 0.29836753 lossA 2.21588 fraction A 0.0479969792\n",
      "step 1654 loss 0.956033587 fisher_loss 0.27796036 triplet loss 0.678073227 l2_loss 9.51723862 fraction B 0.240670562 lossA 2.28725696 fraction A 0.0525316261\n",
      "step 1655 loss 1.02207923 fisher_loss 0.278355181 triplet loss 0.743724048 l2_loss 9.52253056 fraction B 0.22198078 lossA 2.25248766 fraction A 0.0502521694\n",
      "step 1656 loss 0.896734715 fisher_loss 0.277472645 triplet loss 0.61926204 l2_loss 9.52659321 fraction B 0.145601943 lossA 2.26425457 fraction A 0.0508507825\n",
      "step 1657 loss 1.07519078 fisher_loss 0.277501374 triplet loss 0.797689438 l2_loss 9.53234 fraction B 0.31874609 lossA 2.22149944 fraction A 0.0478613973\n",
      "step 1658 loss 0.865403414 fisher_loss 0.27671352 triplet loss 0.588689864 l2_loss 9.53626633 fraction B 0.164379358 lossA 2.20039797 fraction A 0.0465601347\n",
      "step 1659 loss 0.944418 fisher_loss 0.276374459 triplet loss 0.668043554 l2_loss 9.54265404 fraction B 0.187539235 lossA 2.16450191 fraction A 0.0442199297\n",
      "step 1660 loss 0.996483922 fisher_loss 0.275878847 triplet loss 0.720605075 l2_loss 9.54911613 fraction B 0.265819699 lossA 2.14159465 fraction A 0.0425833538\n",
      "step 1661 loss 1.05055928 fisher_loss 0.275465 triplet loss 0.77509433 l2_loss 9.55647469 fraction B 0.267676979 lossA 2.15074015 fraction A 0.0430096947\n",
      "step 1662 loss 1.25840831 fisher_loss 0.275504291 triplet loss 0.982904077 l2_loss 9.56430626 fraction B 0.346431881 lossA 2.136693 fraction A 0.0418795124\n",
      "step 1663 loss 1.00035977 fisher_loss 0.27485165 triplet loss 0.725508094 l2_loss 9.57008 fraction B 0.178332269 lossA 2.09320474 fraction A 0.0394035801\n",
      "step 1664 loss 0.886279583 fisher_loss 0.274379 triplet loss 0.611900568 l2_loss 9.57740402 fraction B 0.191250801 lossA 2.12765 fraction A 0.0413100533\n",
      "step 1665 loss 1.059654 fisher_loss 0.275003195 triplet loss 0.784650743 l2_loss 9.58712101 fraction B 0.279409289 lossA 2.15459299 fraction A 0.0430591404\n",
      "step 1666 loss 1.0414319 fisher_loss 0.275775731 triplet loss 0.765656114 l2_loss 9.59713459 fraction B 0.215693474 lossA 2.17767262 fraction A 0.0447393283\n",
      "step 1667 loss 1.25795007 fisher_loss 0.276607603 triplet loss 0.981342435 l2_loss 9.60765171 fraction B 0.249743089 lossA 2.19483781 fraction A 0.0459169634\n",
      "step 1668 loss 1.16974652 fisher_loss 0.277007252 triplet loss 0.892739296 l2_loss 9.61590195 fraction B 0.265181601 lossA 2.20182252 fraction A 0.0462100059\n",
      "step 1669 loss 1.18868256 fisher_loss 0.276980042 triplet loss 0.911702573 l2_loss 9.62311459 fraction B 0.204699248 lossA 2.1956017 fraction A 0.0458369851\n",
      "step 1670 loss 1.06128693 fisher_loss 0.27689606 triplet loss 0.784390926 l2_loss 9.63019943 fraction B 0.278098851 lossA 2.1572516 fraction A 0.0430329852\n",
      "step 1671 loss 1.10555053 fisher_loss 0.276126146 triplet loss 0.829424441 l2_loss 9.63591194 fraction B 0.314033478 lossA 2.09460282 fraction A 0.0393246524\n",
      "step 1672 loss 1.00836802 fisher_loss 0.275025547 triplet loss 0.733342528 l2_loss 9.64065456 fraction B 0.239036739 lossA 2.04663157 fraction A 0.0368102193\n",
      "step 1673 loss 0.948474884 fisher_loss 0.274400115 triplet loss 0.674074769 l2_loss 9.64540672 fraction B 0.203762755 lossA 2.09145164 fraction A 0.0391859934\n",
      "step 1674 loss 1.08077455 fisher_loss 0.274955601 triplet loss 0.805818915 l2_loss 9.65262413 fraction B 0.19242236 lossA 2.20073676 fraction A 0.046131257\n",
      "step 1675 loss 0.990303874 fisher_loss 0.27647993 triplet loss 0.713823915 l2_loss 9.66128063 fraction B 0.215448499 lossA 2.28836679 fraction A 0.0520044416\n",
      "step 1676 loss 1.08928096 fisher_loss 0.278012663 triplet loss 0.81126827 l2_loss 9.66950226 fraction B 0.22944133 lossA 2.34320569 fraction A 0.0560145453\n",
      "step 1677 loss 0.945067286 fisher_loss 0.279086679 triplet loss 0.665980577 l2_loss 9.67679882 fraction B 0.186744288 lossA 2.31004524 fraction A 0.0539665855\n",
      "step 1678 loss 1.08536637 fisher_loss 0.278925627 triplet loss 0.806440711 l2_loss 9.68333626 fraction B 0.196681455 lossA 2.24525642 fraction A 0.0499672405\n",
      "step 1679 loss 0.863640904 fisher_loss 0.278532177 triplet loss 0.585108757 l2_loss 9.68927765 fraction B 0.176344678 lossA 2.16711402 fraction A 0.0446684062\n",
      "step 1680 loss 1.11204267 fisher_loss 0.278004229 triplet loss 0.834038436 l2_loss 9.69519711 fraction B 0.203750893 lossA 2.06345129 fraction A 0.0387601592\n",
      "step 1681 loss 1.00968647 fisher_loss 0.277667 triplet loss 0.732019484 l2_loss 9.70213699 fraction B 0.258254826 lossA 1.99708498 fraction A 0.0354374163\n",
      "step 1682 loss 1.02310109 fisher_loss 0.277916133 triplet loss 0.745184898 l2_loss 9.70963287 fraction B 0.279091477 lossA 1.95844352 fraction A 0.0334293023\n",
      "step 1683 loss 0.893275619 fisher_loss 0.278061658 triplet loss 0.615213931 l2_loss 9.71655083 fraction B 0.180876449 lossA 1.98491371 fraction A 0.0346703269\n",
      "step 1684 loss 1.0618403 fisher_loss 0.279046506 triplet loss 0.78279382 l2_loss 9.72607231 fraction B 0.297714651 lossA 2.04008818 fraction A 0.0372950248\n",
      "step 1685 loss 1.48224103 fisher_loss 0.280106813 triplet loss 1.20213425 l2_loss 9.73587418 fraction B 0.244779587 lossA 2.11251378 fraction A 0.0410642847\n",
      "step 1686 loss 1.036026 fisher_loss 0.281046033 triplet loss 0.75498 l2_loss 9.74521 fraction B 0.228869304 lossA 2.19505095 fraction A 0.0461973287\n",
      "step 1687 loss 1.17245507 fisher_loss 0.282305539 triplet loss 0.890149534 l2_loss 9.75491524 fraction B 0.180826098 lossA 2.24023652 fraction A 0.0492325723\n",
      "step 1688 loss 1.24382699 fisher_loss 0.282979876 triplet loss 0.96084708 l2_loss 9.7633934 fraction B 0.224425316 lossA 2.24176168 fraction A 0.0492948741\n",
      "step 1689 loss 1.04401827 fisher_loss 0.282681197 triplet loss 0.761337101 l2_loss 9.76927376 fraction B 0.196447104 lossA 2.2181704 fraction A 0.0475882776\n",
      "step 1690 loss 1.0661397 fisher_loss 0.281746835 triplet loss 0.784392893 l2_loss 9.7738533 fraction B 0.20317851 lossA 2.18345928 fraction A 0.0451228544\n",
      "step 1691 loss 1.03008163 fisher_loss 0.280630231 triplet loss 0.749451399 l2_loss 9.77791405 fraction B 0.215667427 lossA 2.17856598 fraction A 0.0446356945\n",
      "step 1692 loss 0.915088654 fisher_loss 0.279740661 triplet loss 0.635347962 l2_loss 9.78193569 fraction B 0.179813862 lossA 2.18081236 fraction A 0.045059029\n",
      "step 1693 loss 1.03158927 fisher_loss 0.279323 triplet loss 0.752266288 l2_loss 9.78778267 fraction B 0.32407406 lossA 2.19663024 fraction A 0.0461513624\n",
      "step 1694 loss 1.06944919 fisher_loss 0.278878063 triplet loss 0.790571094 l2_loss 9.79310226 fraction B 0.233983621 lossA 2.19465518 fraction A 0.0462260172\n",
      "step 1695 loss 0.962103724 fisher_loss 0.278439224 triplet loss 0.683664501 l2_loss 9.79888 fraction B 0.190119207 lossA 2.22758031 fraction A 0.0485281125\n",
      "step 1696 loss 1.08863008 fisher_loss 0.278708249 triplet loss 0.809921861 l2_loss 9.80618572 fraction B 0.241425529 lossA 2.24612737 fraction A 0.0498761386\n",
      "step 1697 loss 1.09689522 fisher_loss 0.278948486 triplet loss 0.817946792 l2_loss 9.81366 fraction B 0.2822 lossA 2.30667901 fraction A 0.0535785332\n",
      "step 1698 loss 1.17341506 fisher_loss 0.279472589 triplet loss 0.893942475 l2_loss 9.82111645 fraction B 0.28420198 lossA 2.36242032 fraction A 0.0571277775\n",
      "step 1699 loss 1.3213203 fisher_loss 0.279873252 triplet loss 1.04144704 l2_loss 9.82778263 fraction B 0.26157847 lossA 2.39626789 fraction A 0.0591486059\n",
      "step 1700 loss 0.944666743 fisher_loss 0.279644519 triplet loss 0.665022194 l2_loss 9.83193207 fraction B 0.218387306 lossA 2.32094 fraction A 0.0533852503\n",
      "step 1701 loss 1.06523979 fisher_loss 0.277881622 triplet loss 0.787358165 l2_loss 9.83457756 fraction B 0.158024937 lossA 2.17927 fraction A 0.0437053517\n",
      "step 1702 loss 1.12782156 fisher_loss 0.275548548 triplet loss 0.852273047 l2_loss 9.83594 fraction B 0.250895679 lossA 2.02961898 fraction A 0.0353885517\n",
      "step 1703 loss 0.965926051 fisher_loss 0.273769736 triplet loss 0.692156315 l2_loss 9.83749199 fraction B 0.160558671 lossA 1.90903986 fraction A 0.0298380032\n",
      "step 1704 loss 1.14263558 fisher_loss 0.272631586 triplet loss 0.870003939 l2_loss 9.83972 fraction B 0.323718756 lossA 1.92851388 fraction A 0.030229969\n",
      "step 1705 loss 1.05366528 fisher_loss 0.272264272 triplet loss 0.781401038 l2_loss 9.84344864 fraction B 0.368482351 lossA 2.02777147 fraction A 0.0342992619\n",
      "step 1706 loss 1.05895591 fisher_loss 0.27241841 triplet loss 0.786537528 l2_loss 9.84830093 fraction B 0.3109667 lossA 2.14427519 fraction A 0.0398049653\n",
      "step 1707 loss 1.16830635 fisher_loss 0.272788674 triplet loss 0.895517707 l2_loss 9.8541069 fraction B 0.315896273 lossA 2.23177814 fraction A 0.0445874482\n",
      "step 1708 loss 0.992336929 fisher_loss 0.272975445 triplet loss 0.719361484 l2_loss 9.85899639 fraction B 0.224069476 lossA 2.32564712 fraction A 0.0501370169\n",
      "step 1709 loss 0.96671772 fisher_loss 0.27325359 triplet loss 0.69346416 l2_loss 9.86432648 fraction B 0.288456798 lossA 2.37289929 fraction A 0.0530446097\n",
      "step 1710 loss 1.07867 fisher_loss 0.273193419 triplet loss 0.805476665 l2_loss 9.86932564 fraction B 0.283316612 lossA 2.39201069 fraction A 0.0540641695\n",
      "step 1711 loss 1.2517035 fisher_loss 0.272633284 triplet loss 0.979070187 l2_loss 9.8734684 fraction B 0.27951476 lossA 2.36573291 fraction A 0.0521381386\n",
      "step 1712 loss 1.24781585 fisher_loss 0.271855235 triplet loss 0.975960672 l2_loss 9.8769474 fraction B 0.306623101 lossA 2.3333478 fraction A 0.0498073548\n",
      "step 1713 loss 1.01310945 fisher_loss 0.270862818 triplet loss 0.742246568 l2_loss 9.87947273 fraction B 0.216004223 lossA 2.27253246 fraction A 0.045798298\n",
      "step 1714 loss 1.1082412 fisher_loss 0.269753844 triplet loss 0.838487387 l2_loss 9.88204098 fraction B 0.195019186 lossA 2.26681685 fraction A 0.0451471582\n",
      "step 1715 loss 1.25081909 fisher_loss 0.269281119 triplet loss 0.981537938 l2_loss 9.8861866 fraction B 0.284950107 lossA 2.25973845 fraction A 0.0447285\n",
      "step 1716 loss 1.14364147 fisher_loss 0.268906265 triplet loss 0.874735236 l2_loss 9.89073467 fraction B 0.229670331 lossA 2.22750354 fraction A 0.0427492186\n",
      "step 1717 loss 1.08567452 fisher_loss 0.268315762 triplet loss 0.817358732 l2_loss 9.8942728 fraction B 0.35731414 lossA 2.20832682 fraction A 0.0415964685\n",
      "step 1718 loss 1.07572436 fisher_loss 0.267872036 triplet loss 0.807852328 l2_loss 9.89848 fraction B 0.267405868 lossA 2.26090717 fraction A 0.0448455699\n",
      "step 1719 loss 1.13303781 fisher_loss 0.268350661 triplet loss 0.864687085 l2_loss 9.90452576 fraction B 0.218595982 lossA 2.32229638 fraction A 0.0487015098\n",
      "step 1720 loss 1.01727033 fisher_loss 0.268562496 triplet loss 0.748707831 l2_loss 9.91055107 fraction B 0.272342771 lossA 2.36722326 fraction A 0.0515702032\n",
      "step 1721 loss 1.12055957 fisher_loss 0.268369943 triplet loss 0.85218966 l2_loss 9.91613579 fraction B 0.173273399 lossA 2.36648464 fraction A 0.0516251884\n",
      "step 1722 loss 0.980970144 fisher_loss 0.267836958 triplet loss 0.713133156 l2_loss 9.92176819 fraction B 0.285800099 lossA 2.33772492 fraction A 0.0497640297\n",
      "step 1723 loss 1.13131559 fisher_loss 0.266957253 triplet loss 0.864358306 l2_loss 9.92663765 fraction B 0.23047559 lossA 2.31127787 fraction A 0.0481686741\n",
      "step 1724 loss 1.07375956 fisher_loss 0.266365588 triplet loss 0.807393909 l2_loss 9.93179703 fraction B 0.304373115 lossA 2.26915312 fraction A 0.0456610136\n",
      "step 1725 loss 0.927614927 fisher_loss 0.26548925 triplet loss 0.662125647 l2_loss 9.93588638 fraction B 0.169143021 lossA 2.23803711 fraction A 0.0440490693\n",
      "step 1726 loss 1.28988099 fisher_loss 0.26502043 triplet loss 1.0248605 l2_loss 9.94067478 fraction B 0.267328233 lossA 2.19652343 fraction A 0.0418777727\n",
      "step 1727 loss 0.939512491 fisher_loss 0.264441371 triplet loss 0.67507112 l2_loss 9.94453144 fraction B 0.176499248 lossA 2.22317219 fraction A 0.0442138836\n",
      "step 1728 loss 1.03575695 fisher_loss 0.264967293 triplet loss 0.770789623 l2_loss 9.9513464 fraction B 0.212369442 lossA 2.30208373 fraction A 0.0500672869\n",
      "step 1729 loss 1.27806342 fisher_loss 0.266223639 triplet loss 1.01183975 l2_loss 9.95882511 fraction B 0.225528583 lossA 2.34831023 fraction A 0.0539302193\n",
      "step 1730 loss 1.14209914 fisher_loss 0.267239273 triplet loss 0.874859929 l2_loss 9.96563148 fraction B 0.260059565 lossA 2.26093102 fraction A 0.0479937568\n",
      "step 1731 loss 1.0714668 fisher_loss 0.265912563 triplet loss 0.805554271 l2_loss 9.96975899 fraction B 0.325278819 lossA 2.14939046 fraction A 0.0405531824\n",
      "step 1732 loss 1.08682907 fisher_loss 0.264202505 triplet loss 0.822626531 l2_loss 9.97294807 fraction B 0.262197703 lossA 2.05993962 fraction A 0.0356971323\n",
      "step 1733 loss 1.03436613 fisher_loss 0.263152778 triplet loss 0.771213412 l2_loss 9.97650814 fraction B 0.264404655 lossA 2.02280498 fraction A 0.03397898\n",
      "step 1734 loss 0.88687849 fisher_loss 0.262753308 triplet loss 0.624125183 l2_loss 9.98106098 fraction B 0.1981031 lossA 2.08822322 fraction A 0.0374874398\n",
      "step 1735 loss 0.99349463 fisher_loss 0.264048666 triplet loss 0.729445934 l2_loss 9.98999596 fraction B 0.247109711 lossA 2.13172126 fraction A 0.0400269367\n",
      "step 1736 loss 1.01997066 fisher_loss 0.265155971 triplet loss 0.754814625 l2_loss 9.99839 fraction B 0.225456879 lossA 2.20474601 fraction A 0.0448594429\n",
      "step 1737 loss 1.10066092 fisher_loss 0.266717792 triplet loss 0.833943129 l2_loss 10.0080986 fraction B 0.266004324 lossA 2.27783728 fraction A 0.0502030514\n",
      "step 1738 loss 1.10802412 fisher_loss 0.268306941 triplet loss 0.839717209 l2_loss 10.0173521 fraction B 0.290130764 lossA 2.36201906 fraction A 0.0561436415\n",
      "step 1739 loss 1.1055119 fisher_loss 0.269861609 triplet loss 0.835650265 l2_loss 10.0258675 fraction B 0.252650857 lossA 2.40746307 fraction A 0.0594496503\n",
      "step 1740 loss 0.997632265 fisher_loss 0.270532221 triplet loss 0.727100074 l2_loss 10.0331078 fraction B 0.176265702 lossA 2.343575 fraction A 0.0547981858\n",
      "step 1741 loss 1.37164474 fisher_loss 0.269724756 triplet loss 1.10192 l2_loss 10.0395947 fraction B 0.308055848 lossA 2.22343516 fraction A 0.0468608923\n",
      "step 1742 loss 1.2810148 fisher_loss 0.268054157 triplet loss 1.01296067 l2_loss 10.0433502 fraction B 0.258051038 lossA 2.07817245 fraction A 0.0382194966\n",
      "step 1743 loss 1.17358184 fisher_loss 0.266425669 triplet loss 0.907156229 l2_loss 10.0462179 fraction B 0.282536894 lossA 1.96227074 fraction A 0.032722272\n",
      "step 1744 loss 1.15143955 fisher_loss 0.265375406 triplet loss 0.886064172 l2_loss 10.0496054 fraction B 0.278398484 lossA 1.93474746 fraction A 0.0317108966\n",
      "step 1745 loss 0.94382 fisher_loss 0.265226334 triplet loss 0.678593695 l2_loss 10.0552359 fraction B 0.291346163 lossA 2.03481436 fraction A 0.0367527343\n",
      "step 1746 loss 1.21854734 fisher_loss 0.266297817 triplet loss 0.952249467 l2_loss 10.0633993 fraction B 0.313848346 lossA 2.13479185 fraction A 0.0424549803\n",
      "step 1747 loss 1.20236111 fisher_loss 0.267391801 triplet loss 0.934969366 l2_loss 10.0710678 fraction B 0.249420285 lossA 2.23937488 fraction A 0.0493728966\n",
      "step 1748 loss 1.0315702 fisher_loss 0.268814951 triplet loss 0.762755215 l2_loss 10.078989 fraction B 0.222942248 lossA 2.26656055 fraction A 0.0513271503\n",
      "step 1749 loss 1.027915 fisher_loss 0.269267797 triplet loss 0.758647144 l2_loss 10.0860252 fraction B 0.226523235 lossA 2.24365306 fraction A 0.0500307381\n",
      "step 1750 loss 0.988900304 fisher_loss 0.268926144 triplet loss 0.71997416 l2_loss 10.0919456 fraction B 0.246928543 lossA 2.18249822 fraction A 0.0462584719\n",
      "step 1751 loss 1.18350494 fisher_loss 0.268067867 triplet loss 0.915437043 l2_loss 10.0972 fraction B 0.215621263 lossA 2.15222716 fraction A 0.0444951504\n",
      "step 1752 loss 1.03435576 fisher_loss 0.267380685 triplet loss 0.766975105 l2_loss 10.1019621 fraction B 0.305841714 lossA 2.09660745 fraction A 0.0411236882\n",
      "step 1753 loss 1.08903229 fisher_loss 0.266141742 triplet loss 0.82289052 l2_loss 10.1054525 fraction B 0.360362053 lossA 2.0270493 fraction A 0.0373953581\n",
      "step 1754 loss 1.16675031 fisher_loss 0.26491347 triplet loss 0.901836872 l2_loss 10.1085205 fraction B 0.323141873 lossA 1.9487586 fraction A 0.033760231\n",
      "step 1755 loss 1.02224863 fisher_loss 0.263783485 triplet loss 0.758465171 l2_loss 10.1113901 fraction B 0.281579494 lossA 1.99844313 fraction A 0.0362373218\n",
      "step 1756 loss 0.987450957 fisher_loss 0.263788253 triplet loss 0.723662734 l2_loss 10.116045 fraction B 0.136619553 lossA 2.12500882 fraction A 0.0429691635\n",
      "step 1757 loss 0.964999 fisher_loss 0.264597297 triplet loss 0.700401723 l2_loss 10.1225 fraction B 0.319309145 lossA 2.17230082 fraction A 0.046139624\n",
      "step 1758 loss 1.07726789 fisher_loss 0.264728099 triplet loss 0.812539816 l2_loss 10.1278553 fraction B 0.169521153 lossA 2.18990278 fraction A 0.0473455153\n",
      "step 1759 loss 0.952070117 fisher_loss 0.264737219 triplet loss 0.687332869 l2_loss 10.1340284 fraction B 0.198934585 lossA 2.14646626 fraction A 0.0445562601\n",
      "step 1760 loss 0.890996575 fisher_loss 0.264103979 triplet loss 0.626892567 l2_loss 10.1389303 fraction B 0.231493 lossA 2.06455684 fraction A 0.0399015\n",
      "step 1761 loss 0.90398705 fisher_loss 0.263502866 triplet loss 0.640484214 l2_loss 10.1441298 fraction B 0.188272879 lossA 1.96615 fraction A 0.03503922\n",
      "step 1762 loss 1.11620378 fisher_loss 0.263046473 triplet loss 0.853157282 l2_loss 10.1498146 fraction B 0.260366231 lossA 1.86975574 fraction A 0.0309078768\n",
      "step 1763 loss 0.930062771 fisher_loss 0.262820452 triplet loss 0.667242289 l2_loss 10.1550646 fraction B 0.231128305 lossA 1.83443272 fraction A 0.0294016283\n",
      "step 1764 loss 1.07266903 fisher_loss 0.262995183 triplet loss 0.809673846 l2_loss 10.160326 fraction B 0.324370444 lossA 1.868855 fraction A 0.0306272954\n",
      "step 1765 loss 1.16029739 fisher_loss 0.26364091 triplet loss 0.896656454 l2_loss 10.1660204 fraction B 0.300666243 lossA 1.98091447 fraction A 0.0354423039\n",
      "step 1766 loss 0.925198078 fisher_loss 0.264453441 triplet loss 0.660744607 l2_loss 10.1712685 fraction B 0.262591869 lossA 2.07338715 fraction A 0.0398637168\n",
      "step 1767 loss 1.23964834 fisher_loss 0.265235662 triplet loss 0.97441262 l2_loss 10.1769714 fraction B 0.237985894 lossA 2.18078566 fraction A 0.0466428325\n",
      "step 1768 loss 1.0665921 fisher_loss 0.266705185 triplet loss 0.799886942 l2_loss 10.1834393 fraction B 0.249523684 lossA 2.24251628 fraction A 0.0502081215\n",
      "step 1769 loss 1.20333934 fisher_loss 0.267584503 triplet loss 0.935754895 l2_loss 10.1893101 fraction B 0.294837356 lossA 2.27081132 fraction A 0.0513481945\n",
      "step 1770 loss 0.935887456 fisher_loss 0.267884582 triplet loss 0.668002844 l2_loss 10.1951418 fraction B 0.202295288 lossA 2.25805521 fraction A 0.0496337\n",
      "step 1771 loss 0.858803034 fisher_loss 0.267416805 triplet loss 0.591386199 l2_loss 10.2001324 fraction B 0.210948601 lossA 2.24251533 fraction A 0.0486816615\n",
      "step 1772 loss 1.11710584 fisher_loss 0.267463237 triplet loss 0.849642634 l2_loss 10.2073154 fraction B 0.190241516 lossA 2.21600676 fraction A 0.0466980711\n",
      "step 1773 loss 0.985799313 fisher_loss 0.267162293 triplet loss 0.718637049 l2_loss 10.213623 fraction B 0.186304435 lossA 2.14596391 fraction A 0.0420438536\n",
      "step 1774 loss 1.04908192 fisher_loss 0.266507268 triplet loss 0.782574654 l2_loss 10.2191305 fraction B 0.177244812 lossA 2.06692553 fraction A 0.037762478\n",
      "step 1775 loss 1.07241642 fisher_loss 0.2658889 triplet loss 0.806527495 l2_loss 10.2254572 fraction B 0.291085631 lossA 2.04995227 fraction A 0.0369082354\n",
      "step 1776 loss 1.2057451 fisher_loss 0.265836447 triplet loss 0.939908624 l2_loss 10.2326469 fraction B 0.40410015 lossA 2.08048773 fraction A 0.0387051739\n",
      "step 1777 loss 0.811470091 fisher_loss 0.266259551 triplet loss 0.54521054 l2_loss 10.2400169 fraction B 0.198304385 lossA 2.17444158 fraction A 0.0446985401\n",
      "step 1778 loss 1.14553452 fisher_loss 0.267981857 triplet loss 0.877552688 l2_loss 10.2518778 fraction B 0.258858889 lossA 2.28692794 fraction A 0.0523473956\n",
      "step 1779 loss 1.13335705 fisher_loss 0.27000773 triplet loss 0.863349378 l2_loss 10.2632504 fraction B 0.197340757 lossA 2.347085 fraction A 0.0566475019\n",
      "step 1780 loss 1.04814649 fisher_loss 0.271531522 triplet loss 0.776614964 l2_loss 10.2735596 fraction B 0.262138605 lossA 2.33550072 fraction A 0.0555450283\n",
      "step 1781 loss 1.10136199 fisher_loss 0.271662652 triplet loss 0.829699397 l2_loss 10.2815838 fraction B 0.217239931 lossA 2.26144767 fraction A 0.0501262955\n",
      "step 1782 loss 1.0618031 fisher_loss 0.27079159 triplet loss 0.791011453 l2_loss 10.2880878 fraction B 0.296406806 lossA 2.16379356 fraction A 0.0435090959\n",
      "step 1783 loss 1.02888417 fisher_loss 0.269489855 triplet loss 0.759394348 l2_loss 10.2934113 fraction B 0.205064833 lossA 2.0839808 fraction A 0.0382643044\n",
      "step 1784 loss 1.04421294 fisher_loss 0.268480808 triplet loss 0.7757321 l2_loss 10.2986717 fraction B 0.288709134 lossA 2.00687242 fraction A 0.0338967554\n",
      "step 1785 loss 1.05458009 fisher_loss 0.267777175 triplet loss 0.786802888 l2_loss 10.3034534 fraction B 0.339948535 lossA 1.99347425 fraction A 0.0328781754\n",
      "step 1786 loss 1.09182513 fisher_loss 0.267571658 triplet loss 0.8242535 l2_loss 10.3082733 fraction B 0.315040559 lossA 2.03273129 fraction A 0.0342273973\n",
      "step 1787 loss 1.08430278 fisher_loss 0.267640084 triplet loss 0.816662729 l2_loss 10.3128939 fraction B 0.27437526 lossA 2.13058615 fraction A 0.03904045\n",
      "step 1788 loss 0.890603304 fisher_loss 0.268266886 triplet loss 0.622336447 l2_loss 10.3193798 fraction B 0.229733139 lossA 2.25429702 fraction A 0.0466069356\n",
      "step 1789 loss 0.913196206 fisher_loss 0.269633085 triplet loss 0.643563151 l2_loss 10.3269205 fraction B 0.196917519 lossA 2.36725545 fraction A 0.0543418899\n",
      "step 1790 loss 1.14559221 fisher_loss 0.271252841 triplet loss 0.874339342 l2_loss 10.3349571 fraction B 0.296182424 lossA 2.4624517 fraction A 0.0613229796\n",
      "step 1791 loss 1.08246827 fisher_loss 0.272806764 triplet loss 0.809661567 l2_loss 10.342186 fraction B 0.304146856 lossA 2.50077939 fraction A 0.0640357658\n",
      "step 1792 loss 1.01828873 fisher_loss 0.273371547 triplet loss 0.744917154 l2_loss 10.3478813 fraction B 0.318507 lossA 2.45888591 fraction A 0.0607371852\n",
      "step 1793 loss 1.14886713 fisher_loss 0.272543669 triplet loss 0.876323462 l2_loss 10.3522034 fraction B 0.332735121 lossA 2.39093661 fraction A 0.0555479228\n",
      "step 1794 loss 1.1447897 fisher_loss 0.271216303 triplet loss 0.873573422 l2_loss 10.3558388 fraction B 0.32965225 lossA 2.29478884 fraction A 0.0483980738\n",
      "step 1795 loss 1.01263821 fisher_loss 0.269515187 triplet loss 0.743123055 l2_loss 10.3582382 fraction B 0.288677335 lossA 2.18185568 fraction A 0.0417460315\n",
      "step 1796 loss 0.869235396 fisher_loss 0.267944813 triplet loss 0.601290584 l2_loss 10.361578 fraction B 0.209155664 lossA 2.15293741 fraction A 0.0402847715\n",
      "step 1797 loss 1.33629584 fisher_loss 0.267776668 triplet loss 1.06851923 l2_loss 10.3690329 fraction B 0.255005419 lossA 2.13547945 fraction A 0.0395118892\n",
      "step 1798 loss 1.0991838 fisher_loss 0.267721474 triplet loss 0.831462383 l2_loss 10.3755474 fraction B 0.224062771 lossA 2.16936255 fraction A 0.0422582924\n",
      "step 1799 loss 1.13037252 fisher_loss 0.268487275 triplet loss 0.86188525 l2_loss 10.3838139 fraction B 0.309739262 lossA 2.20428514 fraction A 0.0454540029\n",
      "step 1800 loss 0.903169036 fisher_loss 0.269399047 triplet loss 0.63377 l2_loss 10.3919868 fraction B 0.186348945 lossA 2.30918908 fraction A 0.0538920425\n",
      "step 1801 loss 1.03123105 fisher_loss 0.271617532 triplet loss 0.759613514 l2_loss 10.402216 fraction B 0.172926515 lossA 2.36215353 fraction A 0.059389919\n",
      "step 1802 loss 1.07672668 fisher_loss 0.273460329 triplet loss 0.803266346 l2_loss 10.4117594 fraction B 0.185987532 lossA 2.3614409 fraction A 0.0602544472\n",
      "step 1803 loss 0.995167732 fisher_loss 0.273968846 triplet loss 0.721198857 l2_loss 10.4192076 fraction B 0.201053649 lossA 2.31219554 fraction A 0.0568252467\n",
      "step 1804 loss 0.927760124 fisher_loss 0.27340731 triplet loss 0.654352784 l2_loss 10.4262543 fraction B 0.244468316 lossA 2.25422359 fraction A 0.0527480207\n",
      "step 1805 loss 1.29700685 fisher_loss 0.272592098 triplet loss 1.02441478 l2_loss 10.4326859 fraction B 0.336792201 lossA 2.16278052 fraction A 0.0466882549\n",
      "step 1806 loss 1.01454031 fisher_loss 0.27157554 triplet loss 0.742964804 l2_loss 10.4374723 fraction B 0.260446548 lossA 2.03440928 fraction A 0.0388177522\n",
      "step 1807 loss 0.98408556 fisher_loss 0.270425081 triplet loss 0.713660479 l2_loss 10.4420013 fraction B 0.202388346 lossA 2.04688644 fraction A 0.0399708636\n",
      "step 1808 loss 1.11267078 fisher_loss 0.270996422 triplet loss 0.841674387 l2_loss 10.4501429 fraction B 0.197326913 lossA 2.16164827 fraction A 0.0478224941\n",
      "step 1809 loss 0.934131265 fisher_loss 0.272409588 triplet loss 0.661721706 l2_loss 10.4596319 fraction B 0.232832447 lossA 2.25658798 fraction A 0.0546375029\n",
      "step 1810 loss 0.955869198 fisher_loss 0.273850113 triplet loss 0.682019055 l2_loss 10.4698114 fraction B 0.126030222 lossA 2.29153514 fraction A 0.0565225668\n",
      "step 1811 loss 0.91448307 fisher_loss 0.273801297 triplet loss 0.640681803 l2_loss 10.4780035 fraction B 0.193730861 lossA 2.29602122 fraction A 0.0562829934\n",
      "step 1812 loss 1.33921587 fisher_loss 0.273759156 triplet loss 1.06545675 l2_loss 10.4865665 fraction B 0.347102791 lossA 2.27861738 fraction A 0.0545517653\n",
      "step 1813 loss 0.931967616 fisher_loss 0.273312598 triplet loss 0.658655 l2_loss 10.4936953 fraction B 0.195092157 lossA 2.26673913 fraction A 0.0531220548\n",
      "step 1814 loss 1.12882924 fisher_loss 0.27322 triplet loss 0.855609179 l2_loss 10.5012627 fraction B 0.206197381 lossA 2.26371455 fraction A 0.0523907915\n",
      "step 1815 loss 1.01433432 fisher_loss 0.273040116 triplet loss 0.741294205 l2_loss 10.508276 fraction B 0.202093437 lossA 2.27745986 fraction A 0.0530809797\n",
      "step 1816 loss 1.34322059 fisher_loss 0.273172945 triplet loss 1.07004762 l2_loss 10.5158157 fraction B 0.265220195 lossA 2.31534076 fraction A 0.0554802231\n",
      "step 1817 loss 1.17622423 fisher_loss 0.273498118 triplet loss 0.902726114 l2_loss 10.5227699 fraction B 0.324881077 lossA 2.32947111 fraction A 0.0562219918\n",
      "step 1818 loss 0.949524879 fisher_loss 0.273396164 triplet loss 0.676128745 l2_loss 10.5280485 fraction B 0.274117112 lossA 2.29773808 fraction A 0.0535759963\n",
      "step 1819 loss 0.935080051 fisher_loss 0.272840947 triplet loss 0.662239075 l2_loss 10.5321093 fraction B 0.216202185 lossA 2.2727 fraction A 0.0515435077\n",
      "step 1820 loss 0.955700815 fisher_loss 0.272723973 triplet loss 0.682976842 l2_loss 10.5361805 fraction B 0.209972173 lossA 2.3064959 fraction A 0.0532725975\n",
      "step 1821 loss 0.931209326 fisher_loss 0.273215801 triplet loss 0.657993495 l2_loss 10.541688 fraction B 0.176377028 lossA 2.33543634 fraction A 0.0546277612\n",
      "step 1822 loss 0.998254657 fisher_loss 0.273454219 triplet loss 0.724800467 l2_loss 10.5475292 fraction B 0.190449327 lossA 2.35364676 fraction A 0.0553608723\n",
      "step 1823 loss 0.953534961 fisher_loss 0.273308396 triplet loss 0.680226564 l2_loss 10.5530882 fraction B 0.170055106 lossA 2.35161328 fraction A 0.0550479442\n",
      "step 1824 loss 1.14387679 fisher_loss 0.273034543 triplet loss 0.870842218 l2_loss 10.5594521 fraction B 0.224003628 lossA 2.3695538 fraction A 0.0560732633\n",
      "step 1825 loss 1.09624362 fisher_loss 0.272800803 triplet loss 0.823442757 l2_loss 10.5656404 fraction B 0.253750056 lossA 2.37877297 fraction A 0.0564958379\n",
      "step 1826 loss 0.976833701 fisher_loss 0.272474468 triplet loss 0.704359233 l2_loss 10.5718126 fraction B 0.211399496 lossA 2.35065413 fraction A 0.054015778\n",
      "step 1827 loss 1.21524525 fisher_loss 0.271639019 triplet loss 0.943606257 l2_loss 10.5765419 fraction B 0.182482213 lossA 2.31641817 fraction A 0.0512964316\n",
      "step 1828 loss 0.979224324 fisher_loss 0.270588785 triplet loss 0.708635509 l2_loss 10.5809069 fraction B 0.195128083 lossA 2.24103737 fraction A 0.0460645705\n",
      "step 1829 loss 1.22973752 fisher_loss 0.269274026 triplet loss 0.960463524 l2_loss 10.5848951 fraction B 0.211510882 lossA 2.18960381 fraction A 0.0426145047\n",
      "step 1830 loss 0.960070372 fisher_loss 0.268642247 triplet loss 0.691428125 l2_loss 10.5895529 fraction B 0.29849115 lossA 2.19190836 fraction A 0.0429836139\n",
      "step 1831 loss 1.20631897 fisher_loss 0.268824607 triplet loss 0.937494338 l2_loss 10.595809 fraction B 0.291091144 lossA 2.22732401 fraction A 0.0453324765\n",
      "step 1832 loss 1.01273167 fisher_loss 0.269142389 triplet loss 0.743589282 l2_loss 10.6019354 fraction B 0.237381637 lossA 2.33348608 fraction A 0.0521385\n",
      "step 1833 loss 0.964422703 fisher_loss 0.270158678 triplet loss 0.694264 l2_loss 10.6086197 fraction B 0.145592079 lossA 2.41259 fraction A 0.0583197586\n",
      "step 1834 loss 1.2148298 fisher_loss 0.271239 triplet loss 0.94359076 l2_loss 10.6163721 fraction B 0.338872343 lossA 2.42053 fraction A 0.0590785481\n",
      "step 1835 loss 0.948273957 fisher_loss 0.271188438 triplet loss 0.677085519 l2_loss 10.6217222 fraction B 0.221288294 lossA 2.348809 fraction A 0.0539699197\n",
      "step 1836 loss 1.11909056 fisher_loss 0.269902796 triplet loss 0.849187732 l2_loss 10.6251793 fraction B 0.367133468 lossA 2.26586938 fraction A 0.0482564047\n",
      "step 1837 loss 0.88065052 fisher_loss 0.268704742 triplet loss 0.611945748 l2_loss 10.6280241 fraction B 0.173414052 lossA 2.254179 fraction A 0.0476265289\n",
      "step 1838 loss 0.78590703 fisher_loss 0.268944293 triplet loss 0.516962707 l2_loss 10.6349697 fraction B 0.162359044 lossA 2.26441073 fraction A 0.0490317531\n",
      "step 1839 loss 1.00478184 fisher_loss 0.270115912 triplet loss 0.73466593 l2_loss 10.6447449 fraction B 0.238609016 lossA 2.23401093 fraction A 0.0475038812\n",
      "step 1840 loss 1.11549568 fisher_loss 0.27073282 triplet loss 0.844762802 l2_loss 10.6524811 fraction B 0.358838767 lossA 2.22992134 fraction A 0.0481083654\n",
      "step 1841 loss 1.15326786 fisher_loss 0.271647543 triplet loss 0.881620347 l2_loss 10.6600571 fraction B 0.241793945 lossA 2.25696921 fraction A 0.050684955\n",
      "step 1842 loss 1.15754294 fisher_loss 0.272853136 triplet loss 0.884689748 l2_loss 10.6677322 fraction B 0.192052841 lossA 2.30255246 fraction A 0.0547275171\n",
      "step 1843 loss 1.14360058 fisher_loss 0.274219185 triplet loss 0.869381368 l2_loss 10.6755877 fraction B 0.271490276 lossA 2.31426072 fraction A 0.0562315919\n",
      "step 1844 loss 0.981835485 fisher_loss 0.275082707 triplet loss 0.706752777 l2_loss 10.6821461 fraction B 0.167304352 lossA 2.31739974 fraction A 0.0565249957\n",
      "step 1845 loss 1.14600527 fisher_loss 0.275473505 triplet loss 0.870531797 l2_loss 10.6889858 fraction B 0.288900644 lossA 2.30869746 fraction A 0.0557838082\n",
      "step 1846 loss 1.24826074 fisher_loss 0.275461912 triplet loss 0.972798884 l2_loss 10.6944818 fraction B 0.332249135 lossA 2.28349161 fraction A 0.0540275127\n",
      "step 1847 loss 1.07645142 fisher_loss 0.27517423 triplet loss 0.801277161 l2_loss 10.6993742 fraction B 0.256578952 lossA 2.22079062 fraction A 0.0498563275\n",
      "step 1848 loss 1.26100755 fisher_loss 0.274359584 triplet loss 0.986648 l2_loss 10.704711 fraction B 0.420767099 lossA 2.1586082 fraction A 0.0457390398\n",
      "step 1849 loss 1.0189302 fisher_loss 0.27332294 triplet loss 0.745607316 l2_loss 10.7092304 fraction B 0.328116149 lossA 2.14757252 fraction A 0.0450077392\n",
      "step 1850 loss 1.15774739 fisher_loss 0.272482753 triplet loss 0.885264635 l2_loss 10.7143517 fraction B 0.253088862 lossA 2.13617 fraction A 0.044197619\n",
      "step 1851 loss 0.95825547 fisher_loss 0.271572292 triplet loss 0.686683178 l2_loss 10.7187834 fraction B 0.177909777 lossA 2.1499517 fraction A 0.0447428077\n",
      "step 1852 loss 1.07611287 fisher_loss 0.270997524 triplet loss 0.805115342 l2_loss 10.7230091 fraction B 0.27101934 lossA 2.22677302 fraction A 0.0497363918\n",
      "step 1853 loss 1.00069666 fisher_loss 0.271246016 triplet loss 0.729450703 l2_loss 10.7284403 fraction B 0.144325152 lossA 2.32481146 fraction A 0.0564650483\n",
      "step 1854 loss 1.20727587 fisher_loss 0.271763682 triplet loss 0.935512185 l2_loss 10.7345629 fraction B 0.248054191 lossA 2.40698457 fraction A 0.0622276776\n",
      "step 1855 loss 1.11972082 fisher_loss 0.2720927 triplet loss 0.847628117 l2_loss 10.7398357 fraction B 0.139261067 lossA 2.45963192 fraction A 0.0658669695\n",
      "step 1856 loss 1.16919255 fisher_loss 0.272044778 triplet loss 0.897147834 l2_loss 10.7446108 fraction B 0.288513243 lossA 2.45842433 fraction A 0.0655198097\n",
      "step 1857 loss 0.919117928 fisher_loss 0.271335036 triplet loss 0.647782862 l2_loss 10.7477932 fraction B 0.199528888 lossA 2.39614701 fraction A 0.061068695\n",
      "step 1858 loss 1.12063086 fisher_loss 0.269982308 triplet loss 0.850648522 l2_loss 10.7504244 fraction B 0.217189014 lossA 2.26119828 fraction A 0.0514398366\n",
      "step 1859 loss 1.03320408 fisher_loss 0.26801309 triplet loss 0.765190959 l2_loss 10.7512054 fraction B 0.199483708 lossA 2.15594244 fraction A 0.0441184714\n",
      "step 1860 loss 1.09943509 fisher_loss 0.266569734 triplet loss 0.832865417 l2_loss 10.7525158 fraction B 0.248048931 lossA 2.09483099 fraction A 0.0401006527\n",
      "step 1861 loss 0.994832039 fisher_loss 0.265744328 triplet loss 0.72908771 l2_loss 10.7542143 fraction B 0.231731623 lossA 2.08629417 fraction A 0.0394550189\n",
      "step 1862 loss 0.813402116 fisher_loss 0.265414536 triplet loss 0.54798758 l2_loss 10.7580481 fraction B 0.216151714 lossA 2.1394608 fraction A 0.0428103618\n",
      "step 1863 loss 0.982365966 fisher_loss 0.265933424 triplet loss 0.716432512 l2_loss 10.7649565 fraction B 0.294246286 lossA 2.28030014 fraction A 0.0525837503\n",
      "step 1864 loss 0.906644106 fisher_loss 0.267493755 triplet loss 0.639150321 l2_loss 10.7745056 fraction B 0.220541462 lossA 2.36264229 fraction A 0.0587235317\n",
      "step 1865 loss 1.00872564 fisher_loss 0.268707931 triplet loss 0.740017653 l2_loss 10.784317 fraction B 0.232795492 lossA 2.39948177 fraction A 0.0613268912\n",
      "step 1866 loss 0.945766926 fisher_loss 0.26936844 triplet loss 0.676398516 l2_loss 10.7929382 fraction B 0.161611214 lossA 2.42617631 fraction A 0.0630388856\n",
      "step 1867 loss 1.32176733 fisher_loss 0.269996375 triplet loss 1.05177093 l2_loss 10.8014517 fraction B 0.395318061 lossA 2.40815091 fraction A 0.0610521436\n",
      "step 1868 loss 0.997546673 fisher_loss 0.269699961 triplet loss 0.727846742 l2_loss 10.8071775 fraction B 0.210691959 lossA 2.36842608 fraction A 0.0573273264\n",
      "step 1869 loss 1.64774179 fisher_loss 0.269106328 triplet loss 1.37863553 l2_loss 10.8123302 fraction B 0.216227308 lossA 2.30004525 fraction A 0.0515172817\n",
      "step 1870 loss 1.00012577 fisher_loss 0.267887443 triplet loss 0.732238293 l2_loss 10.8158617 fraction B 0.209375665 lossA 2.21697879 fraction A 0.0451954082\n",
      "step 1871 loss 1.0395813 fisher_loss 0.267008841 triplet loss 0.772572517 l2_loss 10.8199987 fraction B 0.247085199 lossA 2.09050536 fraction A 0.0368511155\n",
      "step 1872 loss 1.14101958 fisher_loss 0.265886366 triplet loss 0.875133276 l2_loss 10.8235569 fraction B 0.319637805 lossA 2.06414485 fraction A 0.035243731\n",
      "step 1873 loss 0.931239545 fisher_loss 0.265882611 triplet loss 0.665356934 l2_loss 10.8286934 fraction B 0.256771445 lossA 2.11302924 fraction A 0.0379423238\n",
      "step 1874 loss 1.135934 fisher_loss 0.266742289 triplet loss 0.869191706 l2_loss 10.836792 fraction B 0.328775764 lossA 2.1619072 fraction A 0.0411060117\n",
      "step 1875 loss 1.21611106 fisher_loss 0.267517775 triplet loss 0.948593259 l2_loss 10.8434172 fraction B 0.321229279 lossA 2.23288679 fraction A 0.0462544523\n",
      "step 1876 loss 0.982750297 fisher_loss 0.268466145 triplet loss 0.714284122 l2_loss 10.8500433 fraction B 0.181903601 lossA 2.31638312 fraction A 0.0526479371\n",
      "step 1877 loss 1.03299332 fisher_loss 0.269973487 triplet loss 0.76301986 l2_loss 10.8581133 fraction B 0.235377908 lossA 2.39707637 fraction A 0.0592752397\n",
      "step 1878 loss 0.920137107 fisher_loss 0.271971047 triplet loss 0.64816606 l2_loss 10.867156 fraction B 0.161328658 lossA 2.4136827 fraction A 0.0613189973\n",
      "step 1879 loss 0.883714676 fisher_loss 0.273173332 triplet loss 0.610541344 l2_loss 10.8756876 fraction B 0.159543633 lossA 2.39787483 fraction A 0.0607366785\n",
      "step 1880 loss 1.06948292 fisher_loss 0.273780465 triplet loss 0.795702457 l2_loss 10.8843679 fraction B 0.245694339 lossA 2.34106874 fraction A 0.0567606241\n",
      "step 1881 loss 1.32167792 fisher_loss 0.273422182 triplet loss 1.0482558 l2_loss 10.8914537 fraction B 0.297293812 lossA 2.21549129 fraction A 0.0482371375\n",
      "step 1882 loss 1.12895334 fisher_loss 0.271971703 triplet loss 0.856981635 l2_loss 10.8962679 fraction B 0.202571481 lossA 2.12292361 fraction A 0.0421417616\n",
      "step 1883 loss 1.05254197 fisher_loss 0.27080369 triplet loss 0.781738341 l2_loss 10.9009266 fraction B 0.273471117 lossA 2.0756464 fraction A 0.0397001356\n",
      "step 1884 loss 0.830885768 fisher_loss 0.270511061 triplet loss 0.560374737 l2_loss 10.9069376 fraction B 0.135276645 lossA 2.07480979 fraction A 0.0399624631\n",
      "step 1885 loss 1.17103827 fisher_loss 0.271006525 triplet loss 0.900031745 l2_loss 10.9154577 fraction B 0.296274751 lossA 2.10661507 fraction A 0.0421909168\n",
      "step 1886 loss 1.09498477 fisher_loss 0.271781355 triplet loss 0.823203444 l2_loss 10.923604 fraction B 0.200762421 lossA 2.19607759 fraction A 0.0486384481\n",
      "step 1887 loss 1.14098692 fisher_loss 0.273130149 triplet loss 0.867856801 l2_loss 10.9333878 fraction B 0.246758237 lossA 2.24263906 fraction A 0.0522484742\n",
      "step 1888 loss 1.00644195 fisher_loss 0.274143964 triplet loss 0.732297957 l2_loss 10.9419279 fraction B 0.161851704 lossA 2.29116631 fraction A 0.0560559854\n",
      "step 1889 loss 0.97303009 fisher_loss 0.275283396 triplet loss 0.697746694 l2_loss 10.9511251 fraction B 0.2057776 lossA 2.23987 fraction A 0.0526354387\n",
      "step 1890 loss 0.930699825 fisher_loss 0.274791092 triplet loss 0.655908763 l2_loss 10.9589872 fraction B 0.217565462 lossA 2.10817575 fraction A 0.0437605567\n",
      "step 1891 loss 0.917443752 fisher_loss 0.273186535 triplet loss 0.644257188 l2_loss 10.9655333 fraction B 0.180847257 lossA 2.00310445 fraction A 0.0373135693\n",
      "step 1892 loss 1.13379705 fisher_loss 0.272117287 triplet loss 0.861679733 l2_loss 10.9729433 fraction B 0.280850261 lossA 1.9728477 fraction A 0.0355985686\n",
      "step 1893 loss 1.00676525 fisher_loss 0.271983981 triplet loss 0.734781265 l2_loss 10.981205 fraction B 0.280566394 lossA 1.9745295 fraction A 0.0353918485\n",
      "step 1894 loss 1.09479952 fisher_loss 0.271840185 triplet loss 0.822959363 l2_loss 10.988225 fraction B 0.226400569 lossA 2.05600333 fraction A 0.0393348671\n",
      "step 1895 loss 1.0081774 fisher_loss 0.272360712 triplet loss 0.735816658 l2_loss 10.9966793 fraction B 0.237118721 lossA 2.20861864 fraction A 0.0484020561\n",
      "step 1896 loss 1.22941637 fisher_loss 0.274005562 triplet loss 0.955410838 l2_loss 11.0077333 fraction B 0.223355815 lossA 2.34506774 fraction A 0.0568039455\n",
      "step 1897 loss 1.11692524 fisher_loss 0.275443614 triplet loss 0.841481626 l2_loss 11.0170708 fraction B 0.365839928 lossA 2.40082526 fraction A 0.0599894412\n",
      "step 1898 loss 1.46905899 fisher_loss 0.275993735 triplet loss 1.19306529 l2_loss 11.024395 fraction B 0.280738384 lossA 2.39693236 fraction A 0.0590899587\n",
      "step 1899 loss 0.993908644 fisher_loss 0.275548637 triplet loss 0.71836 l2_loss 11.0298672 fraction B 0.148596063 lossA 2.35741019 fraction A 0.0559704639\n",
      "step 1900 loss 1.06166387 fisher_loss 0.274433494 triplet loss 0.787230432 l2_loss 11.0358486 fraction B 0.287383676 lossA 2.2631 fraction A 0.0492940061\n",
      "step 1901 loss 1.26243639 fisher_loss 0.27276504 triplet loss 0.98967135 l2_loss 11.0407906 fraction B 0.283205628 lossA 2.14564371 fraction A 0.0413893834\n",
      "step 1902 loss 1.29110897 fisher_loss 0.271136731 triplet loss 1.01997221 l2_loss 11.0447769 fraction B 0.220234305 lossA 2.03324437 fraction A 0.0350101702\n",
      "step 1903 loss 0.979139447 fisher_loss 0.269739598 triplet loss 0.709399879 l2_loss 11.0484142 fraction B 0.28406918 lossA 2.07739 fraction A 0.0375089906\n",
      "step 1904 loss 1.04835391 fisher_loss 0.269785911 triplet loss 0.778568 l2_loss 11.0548849 fraction B 0.344340026 lossA 2.10300899 fraction A 0.0389041416\n",
      "step 1905 loss 0.911941409 fisher_loss 0.269638687 triplet loss 0.642302752 l2_loss 11.060483 fraction B 0.233139053 lossA 2.21823573 fraction A 0.0463403352\n",
      "step 1906 loss 1.00538826 fisher_loss 0.270748287 triplet loss 0.734639943 l2_loss 11.0695 fraction B 0.309134 lossA 2.27405429 fraction A 0.0502974838\n",
      "step 1907 loss 0.978622139 fisher_loss 0.271413326 triplet loss 0.707208812 l2_loss 11.0773325 fraction B 0.158394605 lossA 2.30853796 fraction A 0.0522390902\n",
      "step 1908 loss 1.11159766 fisher_loss 0.271614641 triplet loss 0.839983046 l2_loss 11.0843306 fraction B 0.320692539 lossA 2.29070163 fraction A 0.0508999\n",
      "step 1909 loss 1.00884032 fisher_loss 0.271267027 triplet loss 0.737573326 l2_loss 11.0901632 fraction B 0.182602 lossA 2.23601556 fraction A 0.046865128\n",
      "step 1910 loss 1.01744866 fisher_loss 0.270377785 triplet loss 0.747070849 l2_loss 11.0946989 fraction B 0.238 lossA 2.19592738 fraction A 0.0436002687\n",
      "step 1911 loss 1.06165934 fisher_loss 0.269667834 triplet loss 0.791991472 l2_loss 11.0993328 fraction B 0.233577266 lossA 2.18964 fraction A 0.0431174561\n",
      "step 1912 loss 1.12386656 fisher_loss 0.269366324 triplet loss 0.854500175 l2_loss 11.1035309 fraction B 0.290969819 lossA 2.2390759 fraction A 0.0462335497\n",
      "step 1913 loss 0.900533497 fisher_loss 0.269783437 triplet loss 0.63075006 l2_loss 11.109458 fraction B 0.166390166 lossA 2.33739829 fraction A 0.0533467084\n",
      "step 1914 loss 1.04048586 fisher_loss 0.271271467 triplet loss 0.769214451 l2_loss 11.1181269 fraction B 0.23936215 lossA 2.38132358 fraction A 0.0571442209\n",
      "step 1915 loss 1.51882863 fisher_loss 0.272118598 triplet loss 1.24671006 l2_loss 11.1255846 fraction B 0.274770945 lossA 2.39328265 fraction A 0.0586739443\n",
      "step 1916 loss 0.97117871 fisher_loss 0.272201061 triplet loss 0.698977649 l2_loss 11.1310501 fraction B 0.215817049 lossA 2.34160829 fraction A 0.0558194481\n",
      "step 1917 loss 1.0253619 fisher_loss 0.271735847 triplet loss 0.753626049 l2_loss 11.1369276 fraction B 0.178302363 lossA 2.28622317 fraction A 0.0525801256\n",
      "step 1918 loss 1.03960037 fisher_loss 0.271437317 triplet loss 0.768163085 l2_loss 11.1423044 fraction B 0.161548615 lossA 2.25710726 fraction A 0.0510696396\n",
      "step 1919 loss 1.1508044 fisher_loss 0.271020085 triplet loss 0.879784286 l2_loss 11.1473246 fraction B 0.234299615 lossA 2.24337244 fraction A 0.0503365323\n",
      "step 1920 loss 1.01508141 fisher_loss 0.270632207 triplet loss 0.744449258 l2_loss 11.1523123 fraction B 0.201239407 lossA 2.18736672 fraction A 0.0462708957\n",
      "step 1921 loss 1.07901216 fisher_loss 0.269826353 triplet loss 0.809185863 l2_loss 11.1563654 fraction B 0.223964185 lossA 2.11519074 fraction A 0.0412912928\n",
      "step 1922 loss 1.12257791 fisher_loss 0.268839359 triplet loss 0.853738546 l2_loss 11.1596022 fraction B 0.297124743 lossA 2.02991629 fraction A 0.0367301665\n",
      "step 1923 loss 1.03512812 fisher_loss 0.26780358 triplet loss 0.767324507 l2_loss 11.162528 fraction B 0.279218853 lossA 2.02373123 fraction A 0.0369433723\n",
      "step 1924 loss 0.922165632 fisher_loss 0.267685056 triplet loss 0.654480577 l2_loss 11.16751 fraction B 0.202149525 lossA 2.05928779 fraction A 0.0393624678\n",
      "step 1925 loss 0.97264719 fisher_loss 0.268044144 triplet loss 0.704603 l2_loss 11.1731215 fraction B 0.236428037 lossA 2.16252327 fraction A 0.0461776219\n",
      "step 1926 loss 1.24291098 fisher_loss 0.269114971 triplet loss 0.973796 l2_loss 11.1801968 fraction B 0.337045044 lossA 2.2163105 fraction A 0.0504319407\n",
      "step 1927 loss 1.00236344 fisher_loss 0.269784242 triplet loss 0.732579231 l2_loss 11.1859417 fraction B 0.224219918 lossA 2.21489978 fraction A 0.0505081192\n",
      "step 1928 loss 1.1509459 fisher_loss 0.269899309 triplet loss 0.881046534 l2_loss 11.1910362 fraction B 0.252529502 lossA 2.23154163 fraction A 0.0516070053\n",
      "step 1929 loss 1.02579844 fisher_loss 0.269890964 triplet loss 0.755907476 l2_loss 11.195425 fraction B 0.161993518 lossA 2.25701952 fraction A 0.0532264858\n",
      "step 1930 loss 1.22653508 fisher_loss 0.26964131 triplet loss 0.956893802 l2_loss 11.2004137 fraction B 0.25738126 lossA 2.26976609 fraction A 0.0536739081\n",
      "step 1931 loss 1.09307408 fisher_loss 0.26909259 triplet loss 0.823981464 l2_loss 11.2058868 fraction B 0.281923652 lossA 2.26155019 fraction A 0.0526250787\n",
      "step 1932 loss 0.905288577 fisher_loss 0.268461913 triplet loss 0.636826694 l2_loss 11.2108364 fraction B 0.232130587 lossA 2.23784566 fraction A 0.0503065027\n",
      "step 1933 loss 0.938786268 fisher_loss 0.267960757 triplet loss 0.670825481 l2_loss 11.2156019 fraction B 0.223898053 lossA 2.18066025 fraction A 0.0453058518\n",
      "step 1934 loss 1.30411851 fisher_loss 0.267037 triplet loss 1.03708148 l2_loss 11.2197123 fraction B 0.203060746 lossA 2.13529873 fraction A 0.0416617058\n",
      "step 1935 loss 1.01574969 fisher_loss 0.26635924 triplet loss 0.749390423 l2_loss 11.2235174 fraction B 0.251356244 lossA 2.16489577 fraction A 0.0432031602\n",
      "step 1936 loss 0.798819 fisher_loss 0.266420543 triplet loss 0.532398462 l2_loss 11.2287865 fraction B 0.172822893 lossA 2.27553487 fraction A 0.0511854813\n",
      "step 1937 loss 1.18760753 fisher_loss 0.267969757 triplet loss 0.91963774 l2_loss 11.2375259 fraction B 0.234742641 lossA 2.36363149 fraction A 0.0576621\n",
      "step 1938 loss 1.06414831 fisher_loss 0.269643813 triplet loss 0.794504464 l2_loss 11.2464437 fraction B 0.201281115 lossA 2.38347101 fraction A 0.0591157153\n",
      "step 1939 loss 1.0266639 fisher_loss 0.270375818 triplet loss 0.756288111 l2_loss 11.2536144 fraction B 0.232922062 lossA 2.37886214 fraction A 0.0585645139\n",
      "step 1940 loss 1.03469825 fisher_loss 0.270601839 triplet loss 0.764096439 l2_loss 11.2599363 fraction B 0.296693832 lossA 2.31543183 fraction A 0.0535286181\n",
      "step 1941 loss 1.14309394 fisher_loss 0.269818455 triplet loss 0.873275518 l2_loss 11.264864 fraction B 0.315319151 lossA 2.19760251 fraction A 0.0453654751\n",
      "step 1942 loss 1.00293982 fisher_loss 0.268415123 triplet loss 0.734524727 l2_loss 11.2691031 fraction B 0.146823943 lossA 2.0878191 fraction A 0.0382279754\n",
      "step 1943 loss 1.05520725 fisher_loss 0.267224759 triplet loss 0.787982523 l2_loss 11.2735424 fraction B 0.269876331 lossA 2.05674934 fraction A 0.0363232046\n",
      "step 1944 loss 1.27888918 fisher_loss 0.266692072 triplet loss 1.01219714 l2_loss 11.2790804 fraction B 0.374382973 lossA 2.05072713 fraction A 0.0357203856\n",
      "step 1945 loss 0.941383719 fisher_loss 0.266077042 triplet loss 0.675306678 l2_loss 11.2832804 fraction B 0.194009066 lossA 2.11615062 fraction A 0.0389455445\n",
      "step 1946 loss 0.889227092 fisher_loss 0.26628077 triplet loss 0.622946322 l2_loss 11.2894545 fraction B 0.184173405 lossA 2.25797844 fraction A 0.0474941358\n",
      "step 1947 loss 1.15348196 fisher_loss 0.267489016 triplet loss 0.885992944 l2_loss 11.2974205 fraction B 0.208672419 lossA 2.41649604 fraction A 0.0587657318\n",
      "step 1948 loss 1.02164173 fisher_loss 0.269592047 triplet loss 0.752049685 l2_loss 11.3066235 fraction B 0.279687762 lossA 2.49998212 fraction A 0.0649593\n",
      "step 1949 loss 1.12514091 fisher_loss 0.270810425 triplet loss 0.85433048 l2_loss 11.3144512 fraction B 0.321609288 lossA 2.51586413 fraction A 0.0661290437\n",
      "step 1950 loss 1.07837605 fisher_loss 0.270936549 triplet loss 0.807439506 l2_loss 11.3200741 fraction B 0.172413796 lossA 2.45496917 fraction A 0.0619210154\n",
      "step 1951 loss 1.23009562 fisher_loss 0.270043373 triplet loss 0.960052311 l2_loss 11.3245897 fraction B 0.318146437 lossA 2.32244158 fraction A 0.0523970574\n",
      "step 1952 loss 0.967630208 fisher_loss 0.268263638 triplet loss 0.69936657 l2_loss 11.3273563 fraction B 0.231871635 lossA 2.1859684 fraction A 0.042633377\n",
      "step 1953 loss 1.05777025 fisher_loss 0.266486228 triplet loss 0.791284 l2_loss 11.3297491 fraction B 0.283536583 lossA 2.08274937 fraction A 0.0366848893\n",
      "step 1954 loss 0.953162909 fisher_loss 0.265488207 triplet loss 0.687674701 l2_loss 11.3325691 fraction B 0.227406025 lossA 2.06220055 fraction A 0.0356519632\n",
      "step 1955 loss 0.997106 fisher_loss 0.265436292 triplet loss 0.731669724 l2_loss 11.3366661 fraction B 0.159984976 lossA 2.14205432 fraction A 0.0399860442\n",
      "step 1956 loss 1.02019846 fisher_loss 0.2663697 triplet loss 0.753828764 l2_loss 11.3431854 fraction B 0.25508973 lossA 2.26646757 fraction A 0.0483658724\n",
      "step 1957 loss 1.08390737 fisher_loss 0.268445551 triplet loss 0.815461755 l2_loss 11.3509579 fraction B 0.257909596 lossA 2.38904643 fraction A 0.057262525\n",
      "step 1958 loss 1.14253855 fisher_loss 0.270985335 triplet loss 0.871553242 l2_loss 11.3596535 fraction B 0.265942931 lossA 2.45852256 fraction A 0.0629534349\n",
      "step 1959 loss 1.07475865 fisher_loss 0.272956163 triplet loss 0.801802516 l2_loss 11.3679075 fraction B 0.318877101 lossA 2.46195149 fraction A 0.0636031255\n",
      "step 1960 loss 1.04992485 fisher_loss 0.273656189 triplet loss 0.776268721 l2_loss 11.3740673 fraction B 0.244939864 lossA 2.40780401 fraction A 0.0601288266\n",
      "step 1961 loss 0.964981675 fisher_loss 0.273329467 triplet loss 0.691652179 l2_loss 11.3784695 fraction B 0.205512419 lossA 2.32623219 fraction A 0.0545840748\n",
      "step 1962 loss 1.07718551 fisher_loss 0.272720426 triplet loss 0.804465115 l2_loss 11.3840942 fraction B 0.212273598 lossA 2.22003865 fraction A 0.0484905876\n",
      "step 1963 loss 1.17808056 fisher_loss 0.271932483 triplet loss 0.906148076 l2_loss 11.3894091 fraction B 0.251032889 lossA 2.17462826 fraction A 0.046346385\n",
      "step 1964 loss 1.1210978 fisher_loss 0.271721125 triplet loss 0.849376678 l2_loss 11.3951674 fraction B 0.369116545 lossA 2.13284492 fraction A 0.0440508798\n",
      "step 1965 loss 1.12634945 fisher_loss 0.271361 triplet loss 0.854988515 l2_loss 11.3998785 fraction B 0.2431501 lossA 2.14665389 fraction A 0.0456623174\n",
      "step 1966 loss 1.13372946 fisher_loss 0.271810353 triplet loss 0.861919045 l2_loss 11.4063234 fraction B 0.231970802 lossA 2.15873265 fraction A 0.0471499488\n",
      "step 1967 loss 1.08044851 fisher_loss 0.272103548 triplet loss 0.80834496 l2_loss 11.4122734 fraction B 0.159389898 lossA 2.16952753 fraction A 0.0482313409\n",
      "step 1968 loss 1.40518212 fisher_loss 0.272270024 triplet loss 1.13291204 l2_loss 11.4178791 fraction B 0.221563488 lossA 2.21253 fraction A 0.051777143\n",
      "step 1969 loss 1.21716285 fisher_loss 0.272556126 triplet loss 0.944606721 l2_loss 11.4234648 fraction B 0.202884302 lossA 2.19686389 fraction A 0.0509040318\n",
      "step 1970 loss 1.20328975 fisher_loss 0.27228725 triplet loss 0.931002438 l2_loss 11.4282627 fraction B 0.292670906 lossA 2.1537962 fraction A 0.0481918566\n",
      "step 1971 loss 1.14080381 fisher_loss 0.271889865 triplet loss 0.868913949 l2_loss 11.4327621 fraction B 0.210199118 lossA 2.12726641 fraction A 0.0466585159\n",
      "step 1972 loss 1.08258462 fisher_loss 0.271644801 triplet loss 0.810939848 l2_loss 11.4375105 fraction B 0.225532547 lossA 2.09133625 fraction A 0.044150494\n",
      "step 1973 loss 1.19962263 fisher_loss 0.271372736 triplet loss 0.928249955 l2_loss 11.4415779 fraction B 0.268916637 lossA 2.08945441 fraction A 0.0438724123\n",
      "step 1974 loss 1.05688834 fisher_loss 0.271244317 triplet loss 0.785644 l2_loss 11.4456558 fraction B 0.251928955 lossA 2.0747323 fraction A 0.0429410897\n",
      "step 1975 loss 1.0401994 fisher_loss 0.270811886 triplet loss 0.769387543 l2_loss 11.449748 fraction B 0.26804167 lossA 2.09537697 fraction A 0.0441896841\n",
      "step 1976 loss 1.23797202 fisher_loss 0.270618469 triplet loss 0.967353582 l2_loss 11.4541769 fraction B 0.205617547 lossA 2.13821673 fraction A 0.0474653021\n",
      "step 1977 loss 1.14637053 fisher_loss 0.27070716 triplet loss 0.8756634 l2_loss 11.4590883 fraction B 0.224181935 lossA 2.15892601 fraction A 0.0489549972\n",
      "step 1978 loss 1.1905086 fisher_loss 0.27038914 triplet loss 0.920119464 l2_loss 11.4637585 fraction B 0.212235749 lossA 2.15448427 fraction A 0.0485419855\n",
      "step 1979 loss 0.898147225 fisher_loss 0.26968345 triplet loss 0.628463745 l2_loss 11.4676199 fraction B 0.202357039 lossA 2.15138364 fraction A 0.0476999544\n",
      "step 1980 loss 0.912484288 fisher_loss 0.268684417 triplet loss 0.643799901 l2_loss 11.471921 fraction B 0.185809135 lossA 2.15046811 fraction A 0.0465048589\n",
      "step 1981 loss 0.933109939 fisher_loss 0.267662048 triplet loss 0.665447891 l2_loss 11.4776297 fraction B 0.259860337 lossA 2.13074112 fraction A 0.0434535667\n",
      "step 1982 loss 1.02522826 fisher_loss 0.266440272 triplet loss 0.75878793 l2_loss 11.481802 fraction B 0.213893965 lossA 2.1093626 fraction A 0.0406694189\n",
      "step 1983 loss 1.05674481 fisher_loss 0.265344977 triplet loss 0.791399896 l2_loss 11.4859495 fraction B 0.204460964 lossA 2.1507535 fraction A 0.0423946343\n",
      "step 1984 loss 1.19821262 fisher_loss 0.265064329 triplet loss 0.933148324 l2_loss 11.4921217 fraction B 0.254805326 lossA 2.21726251 fraction A 0.045751173\n",
      "step 1985 loss 1.01087058 fisher_loss 0.26493457 triplet loss 0.745936 l2_loss 11.4974184 fraction B 0.218776017 lossA 2.23226929 fraction A 0.0456438102\n",
      "step 1986 loss 0.965470076 fisher_loss 0.264525086 triplet loss 0.700945 l2_loss 11.5019207 fraction B 0.240820646 lossA 2.23844314 fraction A 0.0451557077\n",
      "step 1987 loss 0.801272631 fisher_loss 0.263869137 triplet loss 0.537403464 l2_loss 11.506897 fraction B 0.169482008 lossA 2.27618051 fraction A 0.04700955\n",
      "step 1988 loss 1.16543269 fisher_loss 0.263549566 triplet loss 0.901883125 l2_loss 11.5118828 fraction B 0.326381892 lossA 2.30976796 fraction A 0.0488281809\n",
      "step 1989 loss 1.15193832 fisher_loss 0.263154715 triplet loss 0.888783574 l2_loss 11.5164528 fraction B 0.272993296 lossA 2.30012608 fraction A 0.0473781861\n",
      "step 1990 loss 0.923768342 fisher_loss 0.26231277 triplet loss 0.661455572 l2_loss 11.5196505 fraction B 0.278074741 lossA 2.28388953 fraction A 0.0457016937\n",
      "step 1991 loss 1.2579298 fisher_loss 0.261614472 triplet loss 0.99631536 l2_loss 11.523612 fraction B 0.273811072 lossA 2.26488495 fraction A 0.0439415947\n",
      "step 1992 loss 1.14713383 fisher_loss 0.261045069 triplet loss 0.886088729 l2_loss 11.5268965 fraction B 0.302014709 lossA 2.23609853 fraction A 0.0416777134\n",
      "step 1993 loss 0.969333 fisher_loss 0.26025939 triplet loss 0.709073603 l2_loss 11.5293007 fraction B 0.186805263 lossA 2.228338 fraction A 0.0411520153\n",
      "step 1994 loss 0.928385377 fisher_loss 0.260000527 triplet loss 0.66838485 l2_loss 11.5333061 fraction B 0.224259496 lossA 2.19617081 fraction A 0.0392949879\n",
      "step 1995 loss 0.871696413 fisher_loss 0.259642 triplet loss 0.612054408 l2_loss 11.537879 fraction B 0.174653739 lossA 2.21506429 fraction A 0.040726617\n",
      "step 1996 loss 1.19034529 fisher_loss 0.260202676 triplet loss 0.930142581 l2_loss 11.545105 fraction B 0.201852441 lossA 2.25158477 fraction A 0.0440542847\n",
      "step 1997 loss 1.09748113 fisher_loss 0.261283547 triplet loss 0.836197555 l2_loss 11.5547676 fraction B 0.229836836 lossA 2.2706213 fraction A 0.0465629958\n",
      "step 1998 loss 1.03107715 fisher_loss 0.262420982 triplet loss 0.768656194 l2_loss 11.5644054 fraction B 0.190139651 lossA 2.25031781 fraction A 0.0467750058\n",
      "step 1999 loss 1.16812992 fisher_loss 0.263343304 triplet loss 0.904786646 l2_loss 11.5736685 fraction B 0.310512722 lossA 2.20414948 fraction A 0.0447569676\n",
      "step 2000 loss 1.02530909 fisher_loss 0.263757467 triplet loss 0.761551619 l2_loss 11.5817356 fraction B 0.24018909 lossA 2.16425 fraction A 0.0428473465\n",
      "step 2001 loss 1.12149477 fisher_loss 0.264309645 triplet loss 0.857185185 l2_loss 11.5893564 fraction B 0.260081828 lossA 2.12078547 fraction A 0.040803697\n",
      "step 2002 loss 1.02990198 fisher_loss 0.264896899 triplet loss 0.765005112 l2_loss 11.5967264 fraction B 0.216073245 lossA 2.09146976 fraction A 0.0395672731\n",
      "step 2003 loss 1.01221538 fisher_loss 0.265372396 triplet loss 0.746842921 l2_loss 11.6042271 fraction B 0.226520121 lossA 2.09213686 fraction A 0.040230982\n",
      "step 2004 loss 1.28602123 fisher_loss 0.266262174 triplet loss 1.01975906 l2_loss 11.6129942 fraction B 0.255895972 lossA 2.11856198 fraction A 0.0424041599\n",
      "step 2005 loss 1.07509947 fisher_loss 0.267098278 triplet loss 0.808001161 l2_loss 11.6203613 fraction B 0.182279378 lossA 2.14418817 fraction A 0.0444510318\n",
      "step 2006 loss 1.18370628 fisher_loss 0.267589957 triplet loss 0.916116357 l2_loss 11.6272097 fraction B 0.241309673 lossA 2.15474439 fraction A 0.045679234\n",
      "step 2007 loss 1.13336217 fisher_loss 0.267717242 triplet loss 0.865644932 l2_loss 11.6341105 fraction B 0.275164962 lossA 2.22225213 fraction A 0.0507146604\n",
      "step 2008 loss 1.14161789 fisher_loss 0.268219918 triplet loss 0.873398 l2_loss 11.6408777 fraction B 0.227974474 lossA 2.29450345 fraction A 0.0564224869\n",
      "step 2009 loss 1.10335243 fisher_loss 0.269007802 triplet loss 0.834344625 l2_loss 11.6481409 fraction B 0.238713667 lossA 2.30481482 fraction A 0.0570880398\n",
      "step 2010 loss 1.11857724 fisher_loss 0.268543273 triplet loss 0.850033939 l2_loss 11.6532583 fraction B 0.262173384 lossA 2.23312473 fraction A 0.0515922606\n",
      "step 2011 loss 0.952644 fisher_loss 0.266991258 triplet loss 0.685652733 l2_loss 11.6565332 fraction B 0.159804866 lossA 2.15206242 fraction A 0.0455608964\n",
      "step 2012 loss 1.03012908 fisher_loss 0.265515119 triplet loss 0.764614 l2_loss 11.6596546 fraction B 0.295877337 lossA 2.06163883 fraction A 0.0389991552\n",
      "step 2013 loss 1.07641745 fisher_loss 0.263904899 triplet loss 0.812512577 l2_loss 11.6618948 fraction B 0.176768094 lossA 1.97035813 fraction A 0.0337452739\n",
      "step 2014 loss 1.01043963 fisher_loss 0.26252529 triplet loss 0.747914374 l2_loss 11.6651974 fraction B 0.291780591 lossA 1.96102118 fraction A 0.0330799706\n",
      "step 2015 loss 0.996971607 fisher_loss 0.261885196 triplet loss 0.735086441 l2_loss 11.6700926 fraction B 0.260677397 lossA 2.06878972 fraction A 0.038771823\n",
      "step 2016 loss 1.11271858 fisher_loss 0.262361974 triplet loss 0.850356638 l2_loss 11.6777334 fraction B 0.238549456 lossA 2.15419483 fraction A 0.0444902256\n",
      "step 2017 loss 1.14645147 fisher_loss 0.26295498 triplet loss 0.883496463 l2_loss 11.6860371 fraction B 0.310589701 lossA 2.22144198 fraction A 0.0492072888\n",
      "step 2018 loss 1.21727729 fisher_loss 0.263287246 triplet loss 0.953990042 l2_loss 11.6930876 fraction B 0.194006711 lossA 2.28525305 fraction A 0.0532897674\n",
      "step 2019 loss 1.27979958 fisher_loss 0.263281941 triplet loss 1.01651764 l2_loss 11.6985483 fraction B 0.239274681 lossA 2.31978154 fraction A 0.0553596392\n",
      "step 2020 loss 1.06095934 fisher_loss 0.262878299 triplet loss 0.79808104 l2_loss 11.7029543 fraction B 0.264156193 lossA 2.31844711 fraction A 0.0546159856\n",
      "step 2021 loss 1.09607434 fisher_loss 0.262123913 triplet loss 0.83395046 l2_loss 11.7063818 fraction B 0.168302283 lossA 2.30113721 fraction A 0.0526653975\n",
      "step 2022 loss 1.03502798 fisher_loss 0.26096946 triplet loss 0.774058521 l2_loss 11.7092638 fraction B 0.151546761 lossA 2.28706813 fraction A 0.0506411642\n",
      "step 2023 loss 1.0063237 fisher_loss 0.259987772 triplet loss 0.746335924 l2_loss 11.7124138 fraction B 0.279816508 lossA 2.22951794 fraction A 0.0459563732\n",
      "step 2024 loss 1.08074534 fisher_loss 0.25850758 triplet loss 0.82223779 l2_loss 11.7144384 fraction B 0.244165286 lossA 2.22104406 fraction A 0.0452917591\n",
      "step 2025 loss 1.28402138 fisher_loss 0.257859647 triplet loss 1.02616179 l2_loss 11.7185526 fraction B 0.328300178 lossA 2.22645545 fraction A 0.0457559526\n",
      "step 2026 loss 1.06715631 fisher_loss 0.257193327 triplet loss 0.809962928 l2_loss 11.7225027 fraction B 0.241008252 lossA 2.27614546 fraction A 0.0498450957\n",
      "step 2027 loss 1.15922093 fisher_loss 0.25749144 triplet loss 0.901729524 l2_loss 11.7291927 fraction B 0.268576503 lossA 2.30074525 fraction A 0.0520276949\n",
      "step 2028 loss 1.15056837 fisher_loss 0.257538229 triplet loss 0.893030107 l2_loss 11.7353487 fraction B 0.292699963 lossA 2.29690266 fraction A 0.0519419573\n",
      "step 2029 loss 0.916233659 fisher_loss 0.256999075 triplet loss 0.659234583 l2_loss 11.7398701 fraction B 0.291880637 lossA 2.30294013 fraction A 0.0522122122\n",
      "step 2030 loss 0.999748886 fisher_loss 0.256474674 triplet loss 0.743274212 l2_loss 11.7446737 fraction B 0.209770799 lossA 2.27643538 fraction A 0.0499496721\n",
      "step 2031 loss 1.03939342 fisher_loss 0.25557521 triplet loss 0.783818185 l2_loss 11.7494478 fraction B 0.302334398 lossA 2.21450663 fraction A 0.0450621806\n",
      "step 2032 loss 0.938221395 fisher_loss 0.25426656 triplet loss 0.683954835 l2_loss 11.7530241 fraction B 0.197467327 lossA 2.15337396 fraction A 0.0404941738\n",
      "step 2033 loss 1.00998 fisher_loss 0.253403 triplet loss 0.756576896 l2_loss 11.7577181 fraction B 0.256631285 lossA 2.10349369 fraction A 0.0377106778\n",
      "step 2034 loss 1.01850533 fisher_loss 0.253247589 triplet loss 0.765257776 l2_loss 11.7637777 fraction B 0.232136831 lossA 2.08238864 fraction A 0.0366158485\n",
      "step 2035 loss 0.923981786 fisher_loss 0.25346598 triplet loss 0.670515776 l2_loss 11.7697802 fraction B 0.226123363 lossA 2.13527942 fraction A 0.0397666432\n",
      "step 2036 loss 1.26733732 fisher_loss 0.254391521 triplet loss 1.01294577 l2_loss 11.7772074 fraction B 0.248092905 lossA 2.1821034 fraction A 0.0427399091\n",
      "step 2037 loss 1.03560817 fisher_loss 0.255199522 triplet loss 0.78040868 l2_loss 11.7842197 fraction B 0.283712417 lossA 2.23811507 fraction A 0.046457734\n",
      "step 2038 loss 1.01284599 fisher_loss 0.256046027 triplet loss 0.7568 l2_loss 11.7914066 fraction B 0.263046324 lossA 2.28388762 fraction A 0.0492735021\n",
      "step 2039 loss 0.869760513 fisher_loss 0.256736308 triplet loss 0.613024235 l2_loss 11.7986612 fraction B 0.168823898 lossA 2.40881658 fraction A 0.0588337556\n",
      "step 2040 loss 1.23653841 fisher_loss 0.258597851 triplet loss 0.9779405 l2_loss 11.8092356 fraction B 0.210358292 lossA 2.5053544 fraction A 0.0661813095\n",
      "step 2041 loss 1.01220596 fisher_loss 0.260228842 triplet loss 0.751977146 l2_loss 11.8191547 fraction B 0.16510345 lossA 2.51534605 fraction A 0.06665764\n",
      "step 2042 loss 1.26059186 fisher_loss 0.260489434 triplet loss 1.0001024 l2_loss 11.8265781 fraction B 0.23148261 lossA 2.40641022 fraction A 0.0587222278\n",
      "step 2043 loss 0.961365 fisher_loss 0.259278655 triplet loss 0.702086329 l2_loss 11.8314981 fraction B 0.19028388 lossA 2.26131701 fraction A 0.0481668636\n",
      "step 2044 loss 0.954999328 fisher_loss 0.257809967 triplet loss 0.697189391 l2_loss 11.8357239 fraction B 0.241848513 lossA 2.0815618 fraction A 0.0359660499\n",
      "step 2045 loss 1.06487179 fisher_loss 0.256365 triplet loss 0.808506727 l2_loss 11.8380184 fraction B 0.283215016 lossA 1.95979142 fraction A 0.0300662797\n",
      "step 2046 loss 0.998635054 fisher_loss 0.255695134 triplet loss 0.742939949 l2_loss 11.8410559 fraction B 0.271083146 lossA 1.97465837 fraction A 0.0304021\n",
      "step 2047 loss 0.981703758 fisher_loss 0.256059706 triplet loss 0.725644052 l2_loss 11.8458853 fraction B 0.363195062 lossA 2.07171655 fraction A 0.0346079879\n",
      "step 2048 loss 0.998473763 fisher_loss 0.256980389 triplet loss 0.741493344 l2_loss 11.8523254 fraction B 0.224747479 lossA 2.24031711 fraction A 0.045261696\n",
      "step 2049 loss 1.15244722 fisher_loss 0.259098917 triplet loss 0.893348336 l2_loss 11.8616648 fraction B 0.280806184 lossA 2.37029958 fraction A 0.0546759367\n",
      "step 2050 loss 1.05498493 fisher_loss 0.261298865 triplet loss 0.793686092 l2_loss 11.8703985 fraction B 0.188135594 lossA 2.45750642 fraction A 0.0619817972\n",
      "step 2051 loss 1.17328942 fisher_loss 0.262957186 triplet loss 0.910332203 l2_loss 11.8784943 fraction B 0.268005818 lossA 2.46453428 fraction A 0.0630041137\n",
      "step 2052 loss 0.955464423 fisher_loss 0.263668716 triplet loss 0.691795707 l2_loss 11.8857603 fraction B 0.210596621 lossA 2.390903 fraction A 0.0576669872\n",
      "step 2053 loss 1.16272306 fisher_loss 0.26299715 triplet loss 0.899725914 l2_loss 11.8919687 fraction B 0.236040398 lossA 2.28051043 fraction A 0.0499242432\n",
      "step 2054 loss 1.16755843 fisher_loss 0.261516273 triplet loss 0.906042099 l2_loss 11.8970089 fraction B 0.205285221 lossA 2.15334439 fraction A 0.0409615189\n",
      "step 2055 loss 1.13257897 fisher_loss 0.259979516 triplet loss 0.872599483 l2_loss 11.9012423 fraction B 0.341192514 lossA 2.04443693 fraction A 0.03472488\n",
      "step 2056 loss 1.0161 fisher_loss 0.259103239 triplet loss 0.75699681 l2_loss 11.9055033 fraction B 0.205964237 lossA 2.0042007 fraction A 0.0328389443\n",
      "step 2057 loss 1.12656891 fisher_loss 0.259039074 triplet loss 0.867529809 l2_loss 11.9118862 fraction B 0.353482783 lossA 2.02294469 fraction A 0.0335578211\n",
      "step 2058 loss 0.948828101 fisher_loss 0.259191215 triplet loss 0.689636886 l2_loss 11.9181499 fraction B 0.270559967 lossA 2.10045433 fraction A 0.0376775339\n",
      "step 2059 loss 1.06031144 fisher_loss 0.259713084 triplet loss 0.800598383 l2_loss 11.9258537 fraction B 0.163494423 lossA 2.25757384 fraction A 0.0489918366\n",
      "step 2060 loss 1.06469953 fisher_loss 0.261370361 triplet loss 0.80332917 l2_loss 11.9365273 fraction B 0.259877354 lossA 2.46422529 fraction A 0.0639492\n",
      "step 2061 loss 1.02505648 fisher_loss 0.263761252 triplet loss 0.761295259 l2_loss 11.9466448 fraction B 0.307723969 lossA 2.6070981 fraction A 0.073814556\n",
      "step 2062 loss 1.0007422 fisher_loss 0.265684128 triplet loss 0.735058129 l2_loss 11.9555826 fraction B 0.220044032 lossA 2.65601 fraction A 0.0768331736\n",
      "step 2063 loss 1.02715707 fisher_loss 0.265990645 triplet loss 0.761166394 l2_loss 11.9622307 fraction B 0.223798305 lossA 2.63906097 fraction A 0.0751763135\n",
      "step 2064 loss 1.09421504 fisher_loss 0.265189 triplet loss 0.829026043 l2_loss 11.9671259 fraction B 0.265855372 lossA 2.51864552 fraction A 0.0666769817\n",
      "step 2065 loss 1.17460442 fisher_loss 0.263129085 triplet loss 0.91147536 l2_loss 11.9693708 fraction B 0.246831506 lossA 2.35016584 fraction A 0.0541307107\n",
      "step 2066 loss 1.07338059 fisher_loss 0.260681182 triplet loss 0.812699378 l2_loss 11.9708986 fraction B 0.227154329 lossA 2.21965694 fraction A 0.0442564785\n",
      "step 2067 loss 0.971362829 fisher_loss 0.25896585 triplet loss 0.712397 l2_loss 11.9729462 fraction B 0.24726446 lossA 2.17417049 fraction A 0.0405867249\n",
      "step 2068 loss 1.00482893 fisher_loss 0.258434474 triplet loss 0.746394515 l2_loss 11.9779205 fraction B 0.181159705 lossA 2.16713524 fraction A 0.040045049\n",
      "step 2069 loss 1.04081404 fisher_loss 0.258369595 triplet loss 0.782444417 l2_loss 11.9839945 fraction B 0.259455 lossA 2.19715309 fraction A 0.0422736853\n",
      "step 2070 loss 1.21104777 fisher_loss 0.258604199 triplet loss 0.95244354 l2_loss 11.9911385 fraction B 0.29264456 lossA 2.24335742 fraction A 0.0457903668\n",
      "step 2071 loss 1.07668829 fisher_loss 0.258726507 triplet loss 0.817961812 l2_loss 11.9976377 fraction B 0.23016201 lossA 2.38130903 fraction A 0.0558633506\n",
      "step 2072 loss 1.10470486 fisher_loss 0.260008603 triplet loss 0.844696283 l2_loss 12.0057659 fraction B 0.205973268 lossA 2.499295 fraction A 0.0645307899\n",
      "step 2073 loss 1.24675894 fisher_loss 0.26118356 triplet loss 0.985575438 l2_loss 12.013588 fraction B 0.147118285 lossA 2.55184865 fraction A 0.0686020702\n",
      "step 2074 loss 1.11621022 fisher_loss 0.262125701 triplet loss 0.854084492 l2_loss 12.0208731 fraction B 0.249079674 lossA 2.51273489 fraction A 0.066054821\n",
      "step 2075 loss 1.00806212 fisher_loss 0.261683077 triplet loss 0.746379077 l2_loss 12.0261459 fraction B 0.222991467 lossA 2.39557409 fraction A 0.0576245338\n",
      "step 2076 loss 1.12869978 fisher_loss 0.260396898 triplet loss 0.868302882 l2_loss 12.0295734 fraction B 0.216200829 lossA 2.24038029 fraction A 0.0466856807\n",
      "step 2077 loss 0.873046637 fisher_loss 0.258848369 triplet loss 0.614198267 l2_loss 12.0322046 fraction B 0.172822416 lossA 2.15828586 fraction A 0.0418196\n",
      "step 2078 loss 0.992745399 fisher_loss 0.258371502 triplet loss 0.734373868 l2_loss 12.0382385 fraction B 0.234115109 lossA 2.12196636 fraction A 0.0402185917\n",
      "step 2079 loss 1.10341311 fisher_loss 0.2583 triplet loss 0.845113158 l2_loss 12.0449486 fraction B 0.245359853 lossA 2.10922408 fraction A 0.0399308391\n",
      "step 2080 loss 1.36930442 fisher_loss 0.258441389 triplet loss 1.11086297 l2_loss 12.0516367 fraction B 0.32384795 lossA 2.13144851 fraction A 0.0417426974\n",
      "step 2081 loss 0.974962354 fisher_loss 0.258660585 triplet loss 0.716301739 l2_loss 12.0573511 fraction B 0.202188253 lossA 2.20174193 fraction A 0.0471120961\n",
      "step 2082 loss 0.827016711 fisher_loss 0.259729207 triplet loss 0.567287505 l2_loss 12.0652037 fraction B 0.162159413 lossA 2.27008367 fraction A 0.052994661\n",
      "step 2083 loss 0.945067644 fisher_loss 0.2611911 triplet loss 0.683876574 l2_loss 12.0748644 fraction B 0.21348989 lossA 2.29767895 fraction A 0.05600328\n",
      "step 2084 loss 1.16368389 fisher_loss 0.262373418 triplet loss 0.901310444 l2_loss 12.0848398 fraction B 0.218481526 lossA 2.32470131 fraction A 0.0588417612\n",
      "step 2085 loss 1.07511294 fisher_loss 0.263288796 triplet loss 0.811824143 l2_loss 12.0934048 fraction B 0.233404353 lossA 2.32403612 fraction A 0.0592266284\n",
      "step 2086 loss 1.03114676 fisher_loss 0.263553321 triplet loss 0.767593443 l2_loss 12.1003008 fraction B 0.275732756 lossA 2.27655697 fraction A 0.0556422845\n",
      "step 2087 loss 1.07624173 fisher_loss 0.262916714 triplet loss 0.813325047 l2_loss 12.1048527 fraction B 0.229695112 lossA 2.23022509 fraction A 0.0521866418\n",
      "step 2088 loss 1.08923328 fisher_loss 0.26210013 triplet loss 0.827133119 l2_loss 12.1092558 fraction B 0.213076919 lossA 2.23027325 fraction A 0.051915694\n",
      "step 2089 loss 1.51063526 fisher_loss 0.261799425 triplet loss 1.2488358 l2_loss 12.1146955 fraction B 0.281066477 lossA 2.21363616 fraction A 0.0503874235\n",
      "step 2090 loss 0.944084048 fisher_loss 0.261386245 triplet loss 0.682697833 l2_loss 12.1188583 fraction B 0.177555114 lossA 2.17784357 fraction A 0.0473507307\n",
      "step 2091 loss 1.01238072 fisher_loss 0.26094982 triplet loss 0.751430869 l2_loss 12.1237917 fraction B 0.206102625 lossA 2.10546303 fraction A 0.0419162773\n",
      "step 2092 loss 0.918612599 fisher_loss 0.260245591 triplet loss 0.658367038 l2_loss 12.1279459 fraction B 0.153677478 lossA 2.11921024 fraction A 0.0428433232\n",
      "step 2093 loss 1.08109713 fisher_loss 0.260734141 triplet loss 0.820363045 l2_loss 12.1354933 fraction B 0.270740271 lossA 2.17032862 fraction A 0.0463295765\n",
      "step 2094 loss 1.07253754 fisher_loss 0.261509567 triplet loss 0.811028 l2_loss 12.1429367 fraction B 0.253477335 lossA 2.19437909 fraction A 0.0478323102\n",
      "step 2095 loss 0.850878835 fisher_loss 0.261939168 triplet loss 0.588939667 l2_loss 12.1494579 fraction B 0.184342593 lossA 2.25551987 fraction A 0.0523162819\n",
      "step 2096 loss 1.04880297 fisher_loss 0.263118476 triplet loss 0.785684526 l2_loss 12.1586657 fraction B 0.23213774 lossA 2.31047773 fraction A 0.0563674644\n",
      "step 2097 loss 1.11978686 fisher_loss 0.264328808 triplet loss 0.855458 l2_loss 12.1677637 fraction B 0.315360308 lossA 2.35119987 fraction A 0.0591612831\n",
      "step 2098 loss 1.06024885 fisher_loss 0.264981449 triplet loss 0.795267403 l2_loss 12.1760235 fraction B 0.195207044 lossA 2.30979371 fraction A 0.0558736734\n",
      "step 2099 loss 0.860880256 fisher_loss 0.264278144 triplet loss 0.596602142 l2_loss 12.1822948 fraction B 0.182482198 lossA 2.2069881 fraction A 0.0481157526\n",
      "step 2100 loss 0.889009356 fisher_loss 0.262756079 triplet loss 0.626253307 l2_loss 12.1880245 fraction B 0.124523401 lossA 2.09806275 fraction A 0.0399571732\n",
      "step 2101 loss 1.09772873 fisher_loss 0.261474669 triplet loss 0.83625412 l2_loss 12.1940813 fraction B 0.205343857 lossA 2.05761218 fraction A 0.0370788835\n",
      "step 2102 loss 1.11184812 fisher_loss 0.261069953 triplet loss 0.850778162 l2_loss 12.2015076 fraction B 0.289553195 lossA 2.07731175 fraction A 0.0378408618\n",
      "step 2103 loss 1.13723993 fisher_loss 0.26118 triplet loss 0.87605989 l2_loss 12.2092886 fraction B 0.315194279 lossA 2.13757396 fraction A 0.0410297625\n",
      "step 2104 loss 1.01322913 fisher_loss 0.261552781 triplet loss 0.751676381 l2_loss 12.216938 fraction B 0.247221991 lossA 2.20504498 fraction A 0.0453375466\n",
      "step 2105 loss 1.54149699 fisher_loss 0.262111247 triplet loss 1.27938581 l2_loss 12.2242813 fraction B 0.296882927 lossA 2.26686287 fraction A 0.0489962548\n",
      "step 2106 loss 1.08532143 fisher_loss 0.262303472 triplet loss 0.823017895 l2_loss 12.2295637 fraction B 0.244673118 lossA 2.32179356 fraction A 0.0519246794\n",
      "step 2107 loss 0.957850337 fisher_loss 0.262521505 triplet loss 0.695328832 l2_loss 12.2341566 fraction B 0.175621554 lossA 2.41295481 fraction A 0.0577261038\n",
      "step 2108 loss 1.22515559 fisher_loss 0.263601363 triplet loss 0.96155417 l2_loss 12.2406931 fraction B 0.222321197 lossA 2.46798 fraction A 0.0612937473\n",
      "step 2109 loss 1.1293987 fisher_loss 0.26462391 triplet loss 0.864774764 l2_loss 12.2467937 fraction B 0.245910242 lossA 2.44091368 fraction A 0.0591372661\n",
      "step 2110 loss 1.03363991 fisher_loss 0.264416605 triplet loss 0.769223332 l2_loss 12.2513218 fraction B 0.212261558 lossA 2.36924052 fraction A 0.0534003191\n",
      "step 2111 loss 1.05851412 fisher_loss 0.263441533 triplet loss 0.795072615 l2_loss 12.2551575 fraction B 0.224111408 lossA 2.29774833 fraction A 0.0487921387\n",
      "step 2112 loss 1.1792084 fisher_loss 0.262427121 triplet loss 0.916781306 l2_loss 12.2604561 fraction B 0.22516723 lossA 2.20827985 fraction A 0.0434122756\n",
      "step 2113 loss 1.0662061 fisher_loss 0.261200339 triplet loss 0.805005729 l2_loss 12.2647972 fraction B 0.270942867 lossA 2.1648221 fraction A 0.0411377437\n",
      "step 2114 loss 1.05787253 fisher_loss 0.260397851 triplet loss 0.797474682 l2_loss 12.2712336 fraction B 0.280383825 lossA 2.13232589 fraction A 0.03954456\n",
      "step 2115 loss 1.26704645 fisher_loss 0.259726346 triplet loss 1.00732017 l2_loss 12.277997 fraction B 0.176208705 lossA 2.09718585 fraction A 0.0378955603\n",
      "step 2116 loss 0.937285662 fisher_loss 0.259183317 triplet loss 0.678102374 l2_loss 12.2846842 fraction B 0.193978488 lossA 2.1700995 fraction A 0.043888893\n",
      "step 2117 loss 1.15267 fisher_loss 0.260020405 triplet loss 0.892649591 l2_loss 12.2943888 fraction B 0.221781775 lossA 2.30758333 fraction A 0.0546182692\n",
      "step 2118 loss 1.03276789 fisher_loss 0.261697918 triplet loss 0.77107 l2_loss 12.3042841 fraction B 0.170622081 lossA 2.37278318 fraction A 0.0599494502\n",
      "step 2119 loss 1.17538822 fisher_loss 0.262718469 triplet loss 0.912669778 l2_loss 12.3134441 fraction B 0.386421829 lossA 2.39207888 fraction A 0.0620055608\n",
      "step 2120 loss 1.16406035 fisher_loss 0.263089061 triplet loss 0.900971234 l2_loss 12.3208523 fraction B 0.277030289 lossA 2.34687042 fraction A 0.0591015145\n",
      "step 2121 loss 0.857966542 fisher_loss 0.262126207 triplet loss 0.595840335 l2_loss 12.3258057 fraction B 0.139174178 lossA 2.23739195 fraction A 0.0512703508\n",
      "step 2122 loss 1.01286423 fisher_loss 0.260235488 triplet loss 0.752628744 l2_loss 12.3287287 fraction B 0.205023482 lossA 2.12017751 fraction A 0.0422467738\n",
      "step 2123 loss 0.850408792 fisher_loss 0.258263737 triplet loss 0.592145085 l2_loss 12.3303995 fraction B 0.179621711 lossA 2.03491783 fraction A 0.0367500521\n",
      "step 2124 loss 1.07509756 fisher_loss 0.257549882 triplet loss 0.817547739 l2_loss 12.334466 fraction B 0.285971612 lossA 1.98256981 fraction A 0.0335029773\n",
      "step 2125 loss 1.01087427 fisher_loss 0.25717029 triplet loss 0.753703952 l2_loss 12.3386488 fraction B 0.241847157 lossA 2.03323603 fraction A 0.0360024162\n",
      "step 2126 loss 1.00652671 fisher_loss 0.257571191 triplet loss 0.748955488 l2_loss 12.3453846 fraction B 0.218934432 lossA 2.11604667 fraction A 0.0407987349\n",
      "step 2127 loss 1.1138 fisher_loss 0.258267641 triplet loss 0.855532348 l2_loss 12.3531256 fraction B 0.282261878 lossA 2.1657691 fraction A 0.0437687039\n",
      "step 2128 loss 1.11902761 fisher_loss 0.258657038 triplet loss 0.860370517 l2_loss 12.3595314 fraction B 0.265547514 lossA 2.21762085 fraction A 0.0476124398\n",
      "step 2129 loss 1.17679048 fisher_loss 0.25913173 triplet loss 0.917658806 l2_loss 12.3672371 fraction B 0.296877682 lossA 2.27090049 fraction A 0.051242929\n",
      "step 2130 loss 1.10980082 fisher_loss 0.259352326 triplet loss 0.850448489 l2_loss 12.3740139 fraction B 0.283755 lossA 2.26378798 fraction A 0.0506097227\n",
      "step 2131 loss 1.04960775 fisher_loss 0.258743912 triplet loss 0.790863872 l2_loss 12.3791685 fraction B 0.290990978 lossA 2.23883 fraction A 0.0488236882\n",
      "step 2132 loss 0.947569489 fisher_loss 0.258145362 triplet loss 0.689424098 l2_loss 12.3846264 fraction B 0.278360695 lossA 2.21547294 fraction A 0.0469063856\n",
      "step 2133 loss 1.12350416 fisher_loss 0.257546693 triplet loss 0.865957439 l2_loss 12.390275 fraction B 0.396119833 lossA 2.21364832 fraction A 0.0465087\n",
      "step 2134 loss 0.836282969 fisher_loss 0.257037461 triplet loss 0.579245508 l2_loss 12.395175 fraction B 0.137332797 lossA 2.22077322 fraction A 0.0469258\n",
      "step 2135 loss 0.980343938 fisher_loss 0.256881595 triplet loss 0.723462343 l2_loss 12.402133 fraction B 0.28580308 lossA 2.23645449 fraction A 0.0480431654\n",
      "step 2136 loss 1.02089846 fisher_loss 0.25699982 triplet loss 0.763898671 l2_loss 12.408886 fraction B 0.297905505 lossA 2.25453234 fraction A 0.0492221043\n",
      "step 2137 loss 1.03160417 fisher_loss 0.256976604 triplet loss 0.774627566 l2_loss 12.4144773 fraction B 0.257077426 lossA 2.29808664 fraction A 0.0519389138\n",
      "step 2138 loss 1.04927921 fisher_loss 0.257139564 triplet loss 0.792139709 l2_loss 12.4202175 fraction B 0.258029 lossA 2.30603957 fraction A 0.0518461093\n",
      "step 2139 loss 1.10959315 fisher_loss 0.256907374 triplet loss 0.85268575 l2_loss 12.4248352 fraction B 0.276233405 lossA 2.31299496 fraction A 0.0513523966\n",
      "step 2140 loss 1.23065114 fisher_loss 0.256574154 triplet loss 0.974077 l2_loss 12.4282932 fraction B 0.193107083 lossA 2.3049612 fraction A 0.0502640866\n",
      "step 2141 loss 1.01765096 fisher_loss 0.25617516 triplet loss 0.761475801 l2_loss 12.4320431 fraction B 0.237696186 lossA 2.34225941 fraction A 0.0523753613\n",
      "step 2142 loss 1.10195196 fisher_loss 0.256200552 triplet loss 0.845751405 l2_loss 12.4369106 fraction B 0.314856917 lossA 2.35926533 fraction A 0.0535576344\n",
      "step 2143 loss 0.866527855 fisher_loss 0.256075025 triplet loss 0.610452831 l2_loss 12.4420881 fraction B 0.173792168 lossA 2.37814093 fraction A 0.0550391786\n",
      "step 2144 loss 0.926443934 fisher_loss 0.256270409 triplet loss 0.670173526 l2_loss 12.4489136 fraction B 0.198928058 lossA 2.4250083 fraction A 0.0587494671\n",
      "step 2145 loss 1.29218984 fisher_loss 0.257057846 triplet loss 1.03513205 l2_loss 12.4575052 fraction B 0.225145191 lossA 2.45767665 fraction A 0.0610659793\n",
      "step 2146 loss 1.04321837 fisher_loss 0.257366568 triplet loss 0.785851777 l2_loss 12.4649982 fraction B 0.256658494 lossA 2.43400574 fraction A 0.0594630167\n",
      "step 2147 loss 1.03635395 fisher_loss 0.2568205 triplet loss 0.779533446 l2_loss 12.4711809 fraction B 0.220287934 lossA 2.38738728 fraction A 0.0556857511\n",
      "step 2148 loss 1.02516234 fisher_loss 0.255726 triplet loss 0.769436359 l2_loss 12.4757948 fraction B 0.307993233 lossA 2.33328271 fraction A 0.0514418297\n",
      "step 2149 loss 1.10578072 fisher_loss 0.254949182 triplet loss 0.850831568 l2_loss 12.4806309 fraction B 0.387159288 lossA 2.26113296 fraction A 0.0457125232\n",
      "step 2150 loss 0.914404452 fisher_loss 0.254032969 triplet loss 0.660371482 l2_loss 12.48458 fraction B 0.208591446 lossA 2.20535517 fraction A 0.0411167704\n",
      "step 2151 loss 0.999002814 fisher_loss 0.253482044 triplet loss 0.745520771 l2_loss 12.4893169 fraction B 0.300590634 lossA 2.24145412 fraction A 0.0437820703\n",
      "step 2152 loss 1.4090569 fisher_loss 0.253921777 triplet loss 1.15513515 l2_loss 12.4955521 fraction B 0.371564478 lossA 2.28816938 fraction A 0.0472633615\n",
      "step 2153 loss 1.19599366 fisher_loss 0.254352421 triplet loss 0.941641212 l2_loss 12.5004015 fraction B 0.219650954 lossA 2.36989307 fraction A 0.0532053672\n",
      "step 2154 loss 1.07375264 fisher_loss 0.255301952 triplet loss 0.818450689 l2_loss 12.5057812 fraction B 0.237467706 lossA 2.4373529 fraction A 0.0577582344\n",
      "step 2155 loss 1.06807637 fisher_loss 0.256258637 triplet loss 0.811817765 l2_loss 12.5104456 fraction B 0.242701307 lossA 2.4600966 fraction A 0.0590248667\n",
      "step 2156 loss 1.06207573 fisher_loss 0.2566742 triplet loss 0.805401564 l2_loss 12.5141363 fraction B 0.236933216 lossA 2.48109937 fraction A 0.0605217\n",
      "step 2157 loss 1.08517098 fisher_loss 0.257142216 triplet loss 0.828028738 l2_loss 12.5189562 fraction B 0.26385656 lossA 2.43452168 fraction A 0.0566433\n",
      "step 2158 loss 0.984066725 fisher_loss 0.256590396 triplet loss 0.727476358 l2_loss 12.5220156 fraction B 0.175558627 lossA 2.3919003 fraction A 0.0534031056\n",
      "step 2159 loss 1.10079312 fisher_loss 0.255754679 triplet loss 0.845038414 l2_loss 12.5262756 fraction B 0.24523367 lossA 2.38501692 fraction A 0.0533048362\n",
      "step 2160 loss 1.00689089 fisher_loss 0.255099267 triplet loss 0.751791656 l2_loss 12.5309029 fraction B 0.222224489 lossA 2.37772322 fraction A 0.0541813523\n",
      "step 2161 loss 0.963564634 fisher_loss 0.254694045 triplet loss 0.70887059 l2_loss 12.537426 fraction B 0.176174492 lossA 2.40052605 fraction A 0.0567101277\n",
      "step 2162 loss 1.00737369 fisher_loss 0.254520833 triplet loss 0.752852857 l2_loss 12.5440531 fraction B 0.180141419 lossA 2.40669489 fraction A 0.0577503368\n",
      "step 2163 loss 1.00540137 fisher_loss 0.254312545 triplet loss 0.751088798 l2_loss 12.5500879 fraction B 0.217359528 lossA 2.38763523 fraction A 0.0565924421\n",
      "step 2164 loss 0.987046838 fisher_loss 0.253941536 triplet loss 0.733105302 l2_loss 12.5551958 fraction B 0.22279565 lossA 2.3622489 fraction A 0.0545285456\n",
      "step 2165 loss 0.911572218 fisher_loss 0.253556758 triplet loss 0.65801543 l2_loss 12.5606499 fraction B 0.199278504 lossA 2.32801962 fraction A 0.0519627854\n",
      "step 2166 loss 0.870982707 fisher_loss 0.25310427 triplet loss 0.617878437 l2_loss 12.5662012 fraction B 0.14245382 lossA 2.34707117 fraction A 0.0535624139\n",
      "step 2167 loss 1.24516463 fisher_loss 0.253317803 triplet loss 0.9918468 l2_loss 12.5728855 fraction B 0.238691658 lossA 2.3605926 fraction A 0.0545535386\n",
      "step 2168 loss 1.09581852 fisher_loss 0.253417134 triplet loss 0.842401385 l2_loss 12.5785589 fraction B 0.188186362 lossA 2.39661288 fraction A 0.0572592653\n",
      "step 2169 loss 1.04454589 fisher_loss 0.253852427 triplet loss 0.790693462 l2_loss 12.5850544 fraction B 0.232915014 lossA 2.43338609 fraction A 0.0601804443\n",
      "step 2170 loss 0.93709743 fisher_loss 0.254296601 triplet loss 0.682800829 l2_loss 12.5914297 fraction B 0.119021 lossA 2.42890811 fraction A 0.0597720705\n",
      "step 2171 loss 1.42825413 fisher_loss 0.254144847 triplet loss 1.17410922 l2_loss 12.5973501 fraction B 0.175722271 lossA 2.40364385 fraction A 0.0575937107\n",
      "step 2172 loss 1.24276602 fisher_loss 0.253722221 triplet loss 0.989043772 l2_loss 12.6013432 fraction B 0.272490799 lossA 2.35778761 fraction A 0.0537481271\n",
      "step 2173 loss 1.0032115 fisher_loss 0.252949089 triplet loss 0.75026238 l2_loss 12.6046829 fraction B 0.285262018 lossA 2.31999326 fraction A 0.0503634065\n",
      "step 2174 loss 0.9488433 fisher_loss 0.252289414 triplet loss 0.696553886 l2_loss 12.6078787 fraction B 0.168196529 lossA 2.31726265 fraction A 0.0498084761\n",
      "step 2175 loss 1.15466475 fisher_loss 0.251895219 triplet loss 0.902769506 l2_loss 12.6132545 fraction B 0.204964042 lossA 2.35236287 fraction A 0.0522870123\n",
      "step 2176 loss 1.09343636 fisher_loss 0.251940757 triplet loss 0.841495574 l2_loss 12.6201477 fraction B 0.244435042 lossA 2.38584661 fraction A 0.0544696823\n",
      "step 2177 loss 0.921221435 fisher_loss 0.25174886 triplet loss 0.669472575 l2_loss 12.6264648 fraction B 0.156184405 lossA 2.41881633 fraction A 0.0567756929\n",
      "step 2178 loss 0.981200337 fisher_loss 0.251805872 triplet loss 0.729394495 l2_loss 12.63375 fraction B 0.230398402 lossA 2.45744777 fraction A 0.0594981536\n",
      "step 2179 loss 0.936856508 fisher_loss 0.252157331 triplet loss 0.684699178 l2_loss 12.6408901 fraction B 0.260994792 lossA 2.48056269 fraction A 0.0613799207\n",
      "step 2180 loss 1.10163426 fisher_loss 0.252567381 triplet loss 0.849066854 l2_loss 12.6486101 fraction B 0.235611364 lossA 2.47926641 fraction A 0.0612947643\n",
      "step 2181 loss 0.82903564 fisher_loss 0.252621651 triplet loss 0.576414 l2_loss 12.6557674 fraction B 0.175112739 lossA 2.54820108 fraction A 0.0671313256\n",
      "step 2182 loss 0.979228854 fisher_loss 0.253653 triplet loss 0.725575864 l2_loss 12.665432 fraction B 0.157942384 lossA 2.55782175 fraction A 0.068668142\n",
      "step 2183 loss 1.19531238 fisher_loss 0.254215389 triplet loss 0.941096961 l2_loss 12.6741858 fraction B 0.22662966 lossA 2.5000639 fraction A 0.0646770224\n",
      "step 2184 loss 1.07467175 fisher_loss 0.253654122 triplet loss 0.821017623 l2_loss 12.6798697 fraction B 0.240598008 lossA 2.40905976 fraction A 0.0577921\n",
      "step 2185 loss 1.40133321 fisher_loss 0.252824187 triplet loss 1.14850903 l2_loss 12.6846275 fraction B 0.200729027 lossA 2.2869575 fraction A 0.0482791178\n",
      "step 2186 loss 0.985689521 fisher_loss 0.251666248 triplet loss 0.734023273 l2_loss 12.6877365 fraction B 0.218807608 lossA 2.1928091 fraction A 0.0412658639\n",
      "step 2187 loss 0.859623 fisher_loss 0.250929654 triplet loss 0.608693361 l2_loss 12.6918516 fraction B 0.178238198 lossA 2.17780924 fraction A 0.0405959971\n",
      "step 2188 loss 1.00502992 fisher_loss 0.250974357 triplet loss 0.75405556 l2_loss 12.6981926 fraction B 0.278660417 lossA 2.28450727 fraction A 0.0492371\n",
      "step 2189 loss 1.06447411 fisher_loss 0.252246916 triplet loss 0.812227249 l2_loss 12.707139 fraction B 0.239287257 lossA 2.41252351 fraction A 0.0591139048\n",
      "step 2190 loss 1.09776938 fisher_loss 0.25374493 triplet loss 0.84402442 l2_loss 12.7156181 fraction B 0.249939248 lossA 2.46731353 fraction A 0.0629440174\n",
      "step 2191 loss 1.04701126 fisher_loss 0.254459769 triplet loss 0.792551458 l2_loss 12.7215137 fraction B 0.251347423 lossA 2.5009346 fraction A 0.0653512329\n",
      "step 2192 loss 1.08898771 fisher_loss 0.254918784 triplet loss 0.834068894 l2_loss 12.7281809 fraction B 0.313931018 lossA 2.50655317 fraction A 0.0650832206\n",
      "step 2193 loss 1.01148367 fisher_loss 0.254703671 triplet loss 0.75678 l2_loss 12.7334347 fraction B 0.282888114 lossA 2.43727016 fraction A 0.0590103418\n",
      "step 2194 loss 1.08968556 fisher_loss 0.253460675 triplet loss 0.836224914 l2_loss 12.7368431 fraction B 0.216581777 lossA 2.36523294 fraction A 0.0525278226\n",
      "step 2195 loss 0.859610319 fisher_loss 0.252287418 triplet loss 0.607322872 l2_loss 12.7393017 fraction B 0.144388735 lossA 2.32645559 fraction A 0.049137596\n",
      "step 2196 loss 1.12134326 fisher_loss 0.252075821 triplet loss 0.869267464 l2_loss 12.7451696 fraction B 0.200703651 lossA 2.29369354 fraction A 0.0459187403\n",
      "step 2197 loss 1.17004776 fisher_loss 0.251933724 triplet loss 0.918114066 l2_loss 12.7511005 fraction B 0.281617641 lossA 2.27026534 fraction A 0.0434224159\n",
      "step 2198 loss 1.07507229 fisher_loss 0.251954556 triplet loss 0.823117733 l2_loss 12.7568474 fraction B 0.199921831 lossA 2.33890438 fraction A 0.0478836745\n",
      "step 2199 loss 1.11268 fisher_loss 0.253203213 triplet loss 0.859476745 l2_loss 12.7651892 fraction B 0.185849056 lossA 2.45351911 fraction A 0.0560424738\n",
      "step 2200 loss 0.921715736 fisher_loss 0.255195767 triplet loss 0.66652 l2_loss 12.7751141 fraction B 0.143737942 lossA 2.52176809 fraction A 0.061874941\n",
      "step 2201 loss 1.30597293 fisher_loss 0.256732613 triplet loss 1.04924035 l2_loss 12.7858686 fraction B 0.241857991 lossA 2.50385809 fraction A 0.0611733794\n",
      "step 2202 loss 1.21029449 fisher_loss 0.256994605 triplet loss 0.95329982 l2_loss 12.7937164 fraction B 0.181606248 lossA 2.43538785 fraction A 0.056559626\n",
      "step 2203 loss 0.977558494 fisher_loss 0.256455213 triplet loss 0.721103251 l2_loss 12.8007269 fraction B 0.23440665 lossA 2.34609723 fraction A 0.0501779467\n",
      "step 2204 loss 1.03721547 fisher_loss 0.255566925 triplet loss 0.781648517 l2_loss 12.807148 fraction B 0.342895031 lossA 2.23392391 fraction A 0.0423317514\n",
      "step 2205 loss 0.924781084 fisher_loss 0.254375786 triplet loss 0.670405269 l2_loss 12.8123026 fraction B 0.262544513 lossA 2.16848874 fraction A 0.0385517329\n",
      "step 2206 loss 0.983678 fisher_loss 0.254166842 triplet loss 0.729511142 l2_loss 12.8198442 fraction B 0.217728451 lossA 2.16237855 fraction A 0.0387254581\n",
      "step 2207 loss 0.953552127 fisher_loss 0.254648417 triplet loss 0.69890368 l2_loss 12.8291407 fraction B 0.188247249 lossA 2.22021914 fraction A 0.0430056751\n",
      "step 2208 loss 1.07495546 fisher_loss 0.255593777 triplet loss 0.819361687 l2_loss 12.8388691 fraction B 0.272155344 lossA 2.29690385 fraction A 0.0492345653\n",
      "step 2209 loss 1.21490562 fisher_loss 0.256950706 triplet loss 0.957954884 l2_loss 12.8486643 fraction B 0.262969553 lossA 2.34688234 fraction A 0.0533846691\n",
      "step 2210 loss 1.19071662 fisher_loss 0.257720053 triplet loss 0.932996571 l2_loss 12.856802 fraction B 0.32630384 lossA 2.36994863 fraction A 0.0552831367\n",
      "step 2211 loss 1.11302149 fisher_loss 0.25791496 triplet loss 0.855106533 l2_loss 12.8632097 fraction B 0.195636332 lossA 2.36618733 fraction A 0.0550759807\n",
      "step 2212 loss 1.22478521 fisher_loss 0.257574588 triplet loss 0.967210591 l2_loss 12.8681078 fraction B 0.187973663 lossA 2.32030487 fraction A 0.05140597\n",
      "step 2213 loss 1.0954926 fisher_loss 0.256751597 triplet loss 0.838741064 l2_loss 12.8715134 fraction B 0.230755225 lossA 2.25436 fraction A 0.0460402668\n",
      "step 2214 loss 1.06947446 fisher_loss 0.255446523 triplet loss 0.814027965 l2_loss 12.8735151 fraction B 0.269818127 lossA 2.19834 fraction A 0.041736722\n",
      "step 2215 loss 0.919139206 fisher_loss 0.25436306 triplet loss 0.664776146 l2_loss 12.875782 fraction B 0.12616 lossA 2.17701817 fraction A 0.0400383137\n",
      "step 2216 loss 0.973840296 fisher_loss 0.253725708 triplet loss 0.720114589 l2_loss 12.8802929 fraction B 0.269534767 lossA 2.15264034 fraction A 0.0383513123\n",
      "step 2217 loss 1.01539183 fisher_loss 0.253142476 triplet loss 0.762249351 l2_loss 12.8852882 fraction B 0.195075989 lossA 2.17562032 fraction A 0.0391375273\n",
      "step 2218 loss 0.994948387 fisher_loss 0.252948433 triplet loss 0.742 l2_loss 12.8907614 fraction B 0.243076235 lossA 2.23713732 fraction A 0.0428101793\n",
      "step 2219 loss 1.01210761 fisher_loss 0.253121912 triplet loss 0.758985639 l2_loss 12.896965 fraction B 0.188742131 lossA 2.25860071 fraction A 0.0438554585\n",
      "step 2220 loss 1.08222842 fisher_loss 0.252901077 triplet loss 0.829327404 l2_loss 12.9020386 fraction B 0.239201441 lossA 2.25759 fraction A 0.0438409708\n",
      "step 2221 loss 0.96133256 fisher_loss 0.252939969 triplet loss 0.70839256 l2_loss 12.9080744 fraction B 0.297041416 lossA 2.25761652 fraction A 0.0439816937\n",
      "step 2222 loss 1.26582456 fisher_loss 0.25312385 triplet loss 1.01270068 l2_loss 12.9143152 fraction B 0.288030595 lossA 2.2502408 fraction A 0.0433464199\n",
      "step 2223 loss 1.28596318 fisher_loss 0.252898842 triplet loss 1.03306437 l2_loss 12.9182243 fraction B 0.253279 lossA 2.24913096 fraction A 0.0438655652\n",
      "step 2224 loss 1.01062214 fisher_loss 0.252968043 triplet loss 0.757654071 l2_loss 12.9230633 fraction B 0.225681871 lossA 2.2554903 fraction A 0.0451662838\n",
      "step 2225 loss 1.28495479 fisher_loss 0.253192 triplet loss 1.03176272 l2_loss 12.9288015 fraction B 0.199039757 lossA 2.27292299 fraction A 0.0475381836\n",
      "step 2226 loss 1.12225556 fisher_loss 0.253627598 triplet loss 0.868627965 l2_loss 12.9348469 fraction B 0.405206859 lossA 2.28873658 fraction A 0.049421\n",
      "step 2227 loss 1.05378735 fisher_loss 0.253956676 triplet loss 0.799830675 l2_loss 12.9404917 fraction B 0.199178383 lossA 2.30467796 fraction A 0.0508287624\n",
      "step 2228 loss 1.03685939 fisher_loss 0.254076481 triplet loss 0.782782912 l2_loss 12.9460077 fraction B 0.192271516 lossA 2.25400472 fraction A 0.0472896956\n",
      "step 2229 loss 0.937247396 fisher_loss 0.253485799 triplet loss 0.683761597 l2_loss 12.9502897 fraction B 0.144299284 lossA 2.25208712 fraction A 0.0469697751\n",
      "step 2230 loss 0.959567428 fisher_loss 0.253526807 triplet loss 0.706040621 l2_loss 12.9565802 fraction B 0.16709803 lossA 2.22545695 fraction A 0.045498956\n",
      "step 2231 loss 0.975781441 fisher_loss 0.253111839 triplet loss 0.722669601 l2_loss 12.9631119 fraction B 0.181046471 lossA 2.22020102 fraction A 0.0447253846\n",
      "step 2232 loss 1.01078081 fisher_loss 0.252757728 triplet loss 0.758023083 l2_loss 12.9695415 fraction B 0.182984948 lossA 2.25335455 fraction A 0.0475129709\n",
      "step 2233 loss 1.05818367 fisher_loss 0.253122747 triplet loss 0.805060923 l2_loss 12.9781027 fraction B 0.245514601 lossA 2.29878283 fraction A 0.0511774383\n",
      "step 2234 loss 1.07896566 fisher_loss 0.253653914 triplet loss 0.82531178 l2_loss 12.9866266 fraction B 0.223643899 lossA 2.28105283 fraction A 0.0506438091\n",
      "step 2235 loss 1.08980489 fisher_loss 0.253466398 triplet loss 0.83633852 l2_loss 12.9951572 fraction B 0.247190773 lossA 2.25742269 fraction A 0.0494726934\n",
      "step 2236 loss 1.12615609 fisher_loss 0.253205299 triplet loss 0.872950733 l2_loss 13.0038795 fraction B 0.355817884 lossA 2.20697212 fraction A 0.0461431034\n",
      "step 2237 loss 0.991948 fisher_loss 0.252453595 triplet loss 0.739494383 l2_loss 13.0109 fraction B 0.194396943 lossA 2.16482377 fraction A 0.0435474589\n",
      "step 2238 loss 1.12880313 fisher_loss 0.251885116 triplet loss 0.876918 l2_loss 13.0182819 fraction B 0.246523842 lossA 2.10898876 fraction A 0.0400242209\n",
      "step 2239 loss 0.996689081 fisher_loss 0.251106471 triplet loss 0.745582581 l2_loss 13.0245981 fraction B 0.208778143 lossA 2.14037538 fraction A 0.0423636287\n",
      "step 2240 loss 1.22847533 fisher_loss 0.251300663 triplet loss 0.977174699 l2_loss 13.0318174 fraction B 0.239774987 lossA 2.20690656 fraction A 0.0476168208\n",
      "step 2241 loss 1.1843071 fisher_loss 0.251920342 triplet loss 0.932386756 l2_loss 13.0384283 fraction B 0.23512508 lossA 2.26326847 fraction A 0.0518741459\n",
      "step 2242 loss 1.04763842 fisher_loss 0.252306044 triplet loss 0.795332313 l2_loss 13.0445404 fraction B 0.220483 lossA 2.31711 fraction A 0.0554951839\n",
      "step 2243 loss 1.05338049 fisher_loss 0.252361178 triplet loss 0.801019311 l2_loss 13.0496149 fraction B 0.213174015 lossA 2.29194832 fraction A 0.0530563481\n",
      "step 2244 loss 0.924258947 fisher_loss 0.251902282 triplet loss 0.672356665 l2_loss 13.053977 fraction B 0.14755787 lossA 2.2358532 fraction A 0.0473674275\n",
      "step 2245 loss 1.04352272 fisher_loss 0.251047939 triplet loss 0.792474747 l2_loss 13.057538 fraction B 0.299371064 lossA 2.16621137 fraction A 0.0412778147\n",
      "step 2246 loss 0.874617636 fisher_loss 0.250079393 triplet loss 0.624538243 l2_loss 13.0602732 fraction B 0.206522182 lossA 2.16599202 fraction A 0.0406514518\n",
      "step 2247 loss 1.28588223 fisher_loss 0.249834388 triplet loss 1.03604782 l2_loss 13.065588 fraction B 0.321117222 lossA 2.21131444 fraction A 0.0432073623\n",
      "step 2248 loss 0.952966809 fisher_loss 0.249765933 triplet loss 0.703200877 l2_loss 13.070816 fraction B 0.209067911 lossA 2.27846503 fraction A 0.0474206395\n",
      "step 2249 loss 0.947581053 fisher_loss 0.250082463 triplet loss 0.69749862 l2_loss 13.0769434 fraction B 0.194396123 lossA 2.37078023 fraction A 0.0538659245\n",
      "step 2250 loss 1.12152827 fisher_loss 0.250781268 triplet loss 0.870747 l2_loss 13.0844822 fraction B 0.238614708 lossA 2.47655034 fraction A 0.0613903552\n",
      "step 2251 loss 0.921336234 fisher_loss 0.251485109 triplet loss 0.669851124 l2_loss 13.0918808 fraction B 0.143104419 lossA 2.50142193 fraction A 0.0625898689\n",
      "step 2252 loss 0.97264421 fisher_loss 0.251471251 triplet loss 0.721172929 l2_loss 13.0978785 fraction B 0.119269654 lossA 2.47730136 fraction A 0.0603531525\n",
      "step 2253 loss 1.13542318 fisher_loss 0.251331508 triplet loss 0.884091735 l2_loss 13.1036587 fraction B 0.46580708 lossA 2.41984677 fraction A 0.0552613661\n",
      "step 2254 loss 1.105708 fisher_loss 0.250715822 triplet loss 0.854992211 l2_loss 13.1079092 fraction B 0.224670351 lossA 2.33296347 fraction A 0.0484710634\n",
      "step 2255 loss 0.964214206 fisher_loss 0.249957666 triplet loss 0.714256525 l2_loss 13.1122675 fraction B 0.137273505 lossA 2.26868272 fraction A 0.0437244065\n",
      "step 2256 loss 1.13473189 fisher_loss 0.249598637 triplet loss 0.885133207 l2_loss 13.1180563 fraction B 0.293656737 lossA 2.24112558 fraction A 0.0414410718\n",
      "step 2257 loss 1.00929224 fisher_loss 0.249413654 triplet loss 0.759878576 l2_loss 13.1236763 fraction B 0.280428797 lossA 2.23166537 fraction A 0.0409511961\n",
      "step 2258 loss 0.935694933 fisher_loss 0.249477834 triplet loss 0.68621707 l2_loss 13.1290512 fraction B 0.139542609 lossA 2.24997282 fraction A 0.0428623036\n",
      "step 2259 loss 0.997971892 fisher_loss 0.250068694 triplet loss 0.747903168 l2_loss 13.1362 fraction B 0.220234856 lossA 2.33723426 fraction A 0.0502937138\n",
      "step 2260 loss 1.39811873 fisher_loss 0.251579702 triplet loss 1.14653897 l2_loss 13.1446524 fraction B 0.251017481 lossA 2.39125061 fraction A 0.055773519\n",
      "step 2261 loss 0.924033761 fisher_loss 0.253003091 triplet loss 0.671030641 l2_loss 13.1521912 fraction B 0.199529752 lossA 2.40555716 fraction A 0.0584970303\n",
      "step 2262 loss 1.06373322 fisher_loss 0.254383653 triplet loss 0.809349537 l2_loss 13.1602077 fraction B 0.251187444 lossA 2.40719151 fraction A 0.0601103157\n",
      "step 2263 loss 1.00595951 fisher_loss 0.255400032 triplet loss 0.750559449 l2_loss 13.1671391 fraction B 0.273870379 lossA 2.35084748 fraction A 0.0562951639\n",
      "step 2264 loss 0.956254542 fisher_loss 0.255361617 triplet loss 0.700892925 l2_loss 13.1734419 fraction B 0.219341472 lossA 2.27711082 fraction A 0.0506428666\n",
      "step 2265 loss 0.952286601 fisher_loss 0.254803091 triplet loss 0.69748354 l2_loss 13.1791315 fraction B 0.133648902 lossA 2.22460556 fraction A 0.0463431962\n",
      "step 2266 loss 0.977219701 fisher_loss 0.254153579 triplet loss 0.723066092 l2_loss 13.1854172 fraction B 0.22026138 lossA 2.22809434 fraction A 0.0462762937\n",
      "step 2267 loss 0.968492568 fisher_loss 0.254272223 triplet loss 0.714220345 l2_loss 13.1935196 fraction B 0.173063144 lossA 2.25484204 fraction A 0.0474275947\n",
      "step 2268 loss 1.03020489 fisher_loss 0.25429213 triplet loss 0.775912762 l2_loss 13.2014275 fraction B 0.187115923 lossA 2.27608204 fraction A 0.0483431593\n",
      "step 2269 loss 1.01095891 fisher_loss 0.254259706 triplet loss 0.756699145 l2_loss 13.2090445 fraction B 0.218307927 lossA 2.33577633 fraction A 0.0523734763\n",
      "step 2270 loss 0.942436695 fisher_loss 0.254597753 triplet loss 0.687838912 l2_loss 13.2169828 fraction B 0.233023778 lossA 2.35889125 fraction A 0.0534743927\n",
      "step 2271 loss 0.862275481 fisher_loss 0.254341125 triplet loss 0.607934356 l2_loss 13.2230396 fraction B 0.169885918 lossA 2.4107275 fraction A 0.05678837\n",
      "step 2272 loss 0.987602949 fisher_loss 0.254474163 triplet loss 0.733128786 l2_loss 13.2294683 fraction B 0.22180292 lossA 2.44923258 fraction A 0.0592701323\n",
      "step 2273 loss 0.966052413 fisher_loss 0.254784554 triplet loss 0.711267829 l2_loss 13.2356586 fraction B 0.190749273 lossA 2.50339079 fraction A 0.0630694926\n",
      "step 2274 loss 1.24280179 fisher_loss 0.255417138 triplet loss 0.987384677 l2_loss 13.242898 fraction B 0.358495027 lossA 2.53981137 fraction A 0.0651094094\n",
      "step 2275 loss 1.4830482 fisher_loss 0.255686343 triplet loss 1.22736192 l2_loss 13.2492 fraction B 0.217713386 lossA 2.51076651 fraction A 0.0628556684\n",
      "step 2276 loss 1.11860371 fisher_loss 0.255403191 triplet loss 0.863200545 l2_loss 13.2542315 fraction B 0.282091975 lossA 2.45044136 fraction A 0.0581315793\n",
      "step 2277 loss 1.13219607 fisher_loss 0.254796267 triplet loss 0.877399802 l2_loss 13.2589769 fraction B 0.3651838 lossA 2.35152388 fraction A 0.0498800166\n",
      "step 2278 loss 0.795769 fisher_loss 0.253472537 triplet loss 0.54229641 l2_loss 13.2612581 fraction B 0.179513603 lossA 2.41222453 fraction A 0.0553352237\n",
      "step 2279 loss 0.90494138 fisher_loss 0.254148662 triplet loss 0.650792718 l2_loss 13.2699375 fraction B 0.215262055 lossA 2.46716762 fraction A 0.0606406517\n",
      "step 2280 loss 1.25272274 fisher_loss 0.255246669 triplet loss 0.997476101 l2_loss 13.2796268 fraction B 0.214886025 lossA 2.47257328 fraction A 0.0617356636\n",
      "step 2281 loss 0.992249489 fisher_loss 0.25586915 triplet loss 0.736380339 l2_loss 13.2880774 fraction B 0.230854318 lossA 2.43386197 fraction A 0.0594518967\n",
      "step 2282 loss 1.06741214 fisher_loss 0.256113738 triplet loss 0.81129837 l2_loss 13.2959509 fraction B 0.216569766 lossA 2.36494517 fraction A 0.0547224805\n",
      "step 2283 loss 0.996296644 fisher_loss 0.255852103 triplet loss 0.740444541 l2_loss 13.3024168 fraction B 0.278600514 lossA 2.30364442 fraction A 0.0502413027\n",
      "step 2284 loss 0.933595657 fisher_loss 0.255779326 triplet loss 0.677816331 l2_loss 13.3088398 fraction B 0.14563483 lossA 2.2523241 fraction A 0.0466155559\n",
      "step 2285 loss 1.06749845 fisher_loss 0.255941838 triplet loss 0.811556637 l2_loss 13.315959 fraction B 0.243525654 lossA 2.22853732 fraction A 0.045142632\n",
      "step 2286 loss 0.89590466 fisher_loss 0.25639 triplet loss 0.639514625 l2_loss 13.3232527 fraction B 0.206723645 lossA 2.23635 fraction A 0.0460633412\n",
      "step 2287 loss 1.1074388 fisher_loss 0.257146388 triplet loss 0.850292444 l2_loss 13.3307447 fraction B 0.233024165 lossA 2.27000976 fraction A 0.0487049893\n",
      "step 2288 loss 1.05771458 fisher_loss 0.258211672 triplet loss 0.799502909 l2_loss 13.3374472 fraction B 0.192901298 lossA 2.30963802 fraction A 0.0512293838\n",
      "step 2289 loss 1.11713481 fisher_loss 0.259036601 triplet loss 0.858098149 l2_loss 13.3441725 fraction B 0.264378279 lossA 2.37529302 fraction A 0.0564174131\n",
      "step 2290 loss 0.864956796 fisher_loss 0.260310531 triplet loss 0.604646266 l2_loss 13.3528433 fraction B 0.188566267 lossA 2.38791394 fraction A 0.0574446879\n",
      "step 2291 loss 1.11201859 fisher_loss 0.260885924 triplet loss 0.851132631 l2_loss 13.3612242 fraction B 0.295372635 lossA 2.38195419 fraction A 0.0567095131\n",
      "step 2292 loss 1.32082379 fisher_loss 0.260941625 triplet loss 1.05988216 l2_loss 13.3688726 fraction B 0.262248546 lossA 2.34879518 fraction A 0.0536783263\n",
      "step 2293 loss 1.21055937 fisher_loss 0.260150909 triplet loss 0.950408399 l2_loss 13.3746595 fraction B 0.261372358 lossA 2.31742215 fraction A 0.0508739315\n",
      "step 2294 loss 1.19547331 fisher_loss 0.259151578 triplet loss 0.936321735 l2_loss 13.3794699 fraction B 0.264302164 lossA 2.28040886 fraction A 0.0475991108\n",
      "step 2295 loss 0.990435123 fisher_loss 0.257971019 triplet loss 0.732464075 l2_loss 13.3838625 fraction B 0.224548489 lossA 2.28084731 fraction A 0.0466672815\n",
      "step 2296 loss 1.21053588 fisher_loss 0.257078916 triplet loss 0.953457 l2_loss 13.3883953 fraction B 0.22124806 lossA 2.28442264 fraction A 0.0463309884\n",
      "step 2297 loss 1.17218208 fisher_loss 0.256178707 triplet loss 0.916003406 l2_loss 13.3925352 fraction B 0.232316643 lossA 2.28924036 fraction A 0.0462302901\n",
      "step 2298 loss 1.07504213 fisher_loss 0.255464762 triplet loss 0.819577336 l2_loss 13.397006 fraction B 0.247616053 lossA 2.31218243 fraction A 0.047724802\n",
      "step 2299 loss 1.11361599 fisher_loss 0.254975557 triplet loss 0.858640373 l2_loss 13.4028063 fraction B 0.20644179 lossA 2.3207202 fraction A 0.0485597\n",
      "step 2300 loss 1.00723171 fisher_loss 0.254585296 triplet loss 0.752646446 l2_loss 13.4085112 fraction B 0.241678178 lossA 2.30521631 fraction A 0.0474184677\n",
      "step 2301 loss 1.14156497 fisher_loss 0.253881484 triplet loss 0.887683451 l2_loss 13.4133339 fraction B 0.244210348 lossA 2.28982806 fraction A 0.0458379611\n",
      "step 2302 loss 1.02686167 fisher_loss 0.253065556 triplet loss 0.773796141 l2_loss 13.4169006 fraction B 0.230235443 lossA 2.27649379 fraction A 0.0442733951\n",
      "step 2303 loss 0.946046293 fisher_loss 0.252117932 triplet loss 0.693928361 l2_loss 13.4196491 fraction B 0.202726126 lossA 2.3405726 fraction A 0.0484774746\n",
      "step 2304 loss 1.0339421 fisher_loss 0.251875192 triplet loss 0.782066882 l2_loss 13.4231024 fraction B 0.25150913 lossA 2.40971017 fraction A 0.052767653\n",
      "step 2305 loss 0.925040126 fisher_loss 0.251736075 triplet loss 0.673304 l2_loss 13.4264364 fraction B 0.119394965 lossA 2.44279075 fraction A 0.0544042289\n",
      "step 2306 loss 0.991508543 fisher_loss 0.251456201 triplet loss 0.740052342 l2_loss 13.4301071 fraction B 0.215492666 lossA 2.39493299 fraction A 0.0503665581\n",
      "step 2307 loss 1.15448177 fisher_loss 0.250594944 triplet loss 0.903886855 l2_loss 13.4323568 fraction B 0.299411476 lossA 2.33813739 fraction A 0.0452606827\n",
      "step 2308 loss 1.05065918 fisher_loss 0.249789879 triplet loss 0.800869286 l2_loss 13.4338751 fraction B 0.176936895 lossA 2.3400476 fraction A 0.0454101376\n",
      "step 2309 loss 1.02715242 fisher_loss 0.249731943 triplet loss 0.777420461 l2_loss 13.4376993 fraction B 0.233917624 lossA 2.4282093 fraction A 0.0521012619\n",
      "step 2310 loss 0.954252362 fisher_loss 0.250650734 triplet loss 0.703601599 l2_loss 13.4434958 fraction B 0.194465324 lossA 2.49350977 fraction A 0.0574161112\n",
      "step 2311 loss 1.00418186 fisher_loss 0.251577616 triplet loss 0.752604187 l2_loss 13.4503412 fraction B 0.183663473 lossA 2.53020835 fraction A 0.0614468269\n",
      "step 2312 loss 0.991782308 fisher_loss 0.252280354 triplet loss 0.739501953 l2_loss 13.4574881 fraction B 0.212750301 lossA 2.5110476 fraction A 0.06157846\n",
      "step 2313 loss 0.991634548 fisher_loss 0.252189934 triplet loss 0.739444613 l2_loss 13.4647322 fraction B 0.224268392 lossA 2.45110178 fraction A 0.0584818162\n",
      "step 2314 loss 1.09461486 fisher_loss 0.251714945 triplet loss 0.842899919 l2_loss 13.4712381 fraction B 0.210996896 lossA 2.38792896 fraction A 0.0548811741\n",
      "step 2315 loss 1.14511323 fisher_loss 0.251264811 triplet loss 0.89384836 l2_loss 13.4778595 fraction B 0.284819484 lossA 2.31119847 fraction A 0.0500843115\n",
      "step 2316 loss 0.791338205 fisher_loss 0.250775069 triplet loss 0.540563107 l2_loss 13.4835157 fraction B 0.130506724 lossA 2.26643705 fraction A 0.0477394722\n",
      "step 2317 loss 1.08157206 fisher_loss 0.25112462 triplet loss 0.830447435 l2_loss 13.4913826 fraction B 0.228517294 lossA 2.27458549 fraction A 0.04880815\n",
      "step 2318 loss 1.1073637 fisher_loss 0.251730561 triplet loss 0.85563314 l2_loss 13.4995632 fraction B 0.213965312 lossA 2.27863789 fraction A 0.0495362282\n",
      "step 2319 loss 0.982017756 fisher_loss 0.252423912 triplet loss 0.729593813 l2_loss 13.5075922 fraction B 0.171061426 lossA 2.30397081 fraction A 0.0516602173\n",
      "step 2320 loss 1.14427686 fisher_loss 0.253200054 triplet loss 0.891076803 l2_loss 13.515626 fraction B 0.28729412 lossA 2.32501721 fraction A 0.0534574762\n",
      "step 2321 loss 1.11041641 fisher_loss 0.253841758 triplet loss 0.856574595 l2_loss 13.5231838 fraction B 0.242319092 lossA 2.33457923 fraction A 0.053907074\n",
      "step 2322 loss 1.04164279 fisher_loss 0.253876477 triplet loss 0.787766337 l2_loss 13.5299196 fraction B 0.228681043 lossA 2.34746528 fraction A 0.0543776788\n",
      "step 2323 loss 1.04461884 fisher_loss 0.253694504 triplet loss 0.79092437 l2_loss 13.5358505 fraction B 0.246023625 lossA 2.31564 fraction A 0.0516148284\n",
      "step 2324 loss 0.939988256 fisher_loss 0.253065139 triplet loss 0.686923087 l2_loss 13.5406351 fraction B 0.149625272 lossA 2.31838059 fraction A 0.0517334938\n",
      "step 2325 loss 1.14422297 fisher_loss 0.252886176 triplet loss 0.891336858 l2_loss 13.5479078 fraction B 0.275931656 lossA 2.27482891 fraction A 0.0482107662\n",
      "step 2326 loss 0.940331578 fisher_loss 0.252240717 triplet loss 0.688090861 l2_loss 13.5531139 fraction B 0.207778096 lossA 2.3601768 fraction A 0.0540534854\n",
      "step 2327 loss 1.35039091 fisher_loss 0.252842695 triplet loss 1.09754825 l2_loss 13.560421 fraction B 0.149484754 lossA 2.39478278 fraction A 0.0555755273\n",
      "step 2328 loss 1.20802116 fisher_loss 0.253068268 triplet loss 0.954952896 l2_loss 13.5654335 fraction B 0.271503985 lossA 2.39355183 fraction A 0.0548946112\n",
      "step 2329 loss 1.04901552 fisher_loss 0.253056228 triplet loss 0.795959234 l2_loss 13.5696182 fraction B 0.181233883 lossA 2.44980931 fraction A 0.0589045361\n",
      "step 2330 loss 1.07655525 fisher_loss 0.253473163 triplet loss 0.82308203 l2_loss 13.5743275 fraction B 0.266744226 lossA 2.44988465 fraction A 0.0586305112\n",
      "step 2331 loss 0.98467052 fisher_loss 0.253563583 triplet loss 0.731106937 l2_loss 13.577342 fraction B 0.212150976 lossA 2.44516563 fraction A 0.0572174639\n",
      "step 2332 loss 0.914808154 fisher_loss 0.253365755 triplet loss 0.661442399 l2_loss 13.5804625 fraction B 0.12535356 lossA 2.40162182 fraction A 0.0515869372\n",
      "step 2333 loss 1.02649248 fisher_loss 0.252965182 triplet loss 0.773527265 l2_loss 13.581831 fraction B 0.248507962 lossA 2.33685088 fraction A 0.0457621105\n",
      "step 2334 loss 0.962186 fisher_loss 0.252527237 triplet loss 0.709658742 l2_loss 13.5837317 fraction B 0.245200992 lossA 2.29798508 fraction A 0.0424618274\n",
      "step 2335 loss 0.95087 fisher_loss 0.25235498 triplet loss 0.698515 l2_loss 13.5865784 fraction B 0.226893961 lossA 2.32272553 fraction A 0.0442061685\n",
      "step 2336 loss 1.22357416 fisher_loss 0.25265798 triplet loss 0.970916212 l2_loss 13.5917473 fraction B 0.200374767 lossA 2.35570121 fraction A 0.0471722968\n",
      "step 2337 loss 1.02698851 fisher_loss 0.25315094 triplet loss 0.773837507 l2_loss 13.598526 fraction B 0.170095071 lossA 2.40149188 fraction A 0.0508494787\n",
      "step 2338 loss 1.04100668 fisher_loss 0.253947258 triplet loss 0.787059426 l2_loss 13.6056232 fraction B 0.299514025 lossA 2.43537188 fraction A 0.0546438433\n",
      "step 2339 loss 1.00704646 fisher_loss 0.254313558 triplet loss 0.752732873 l2_loss 13.6126814 fraction B 0.229752794 lossA 2.45431232 fraction A 0.0564590693\n",
      "step 2340 loss 1.00253773 fisher_loss 0.254346073 triplet loss 0.748191595 l2_loss 13.6191187 fraction B 0.270007789 lossA 2.41287112 fraction A 0.0527766347\n",
      "step 2341 loss 1.05374265 fisher_loss 0.253628582 triplet loss 0.800114095 l2_loss 13.624155 fraction B 0.297971785 lossA 2.35758877 fraction A 0.0481962785\n",
      "step 2342 loss 0.893417835 fisher_loss 0.252781302 triplet loss 0.640636563 l2_loss 13.6287241 fraction B 0.169749066 lossA 2.28199577 fraction A 0.0411191955\n",
      "step 2343 loss 1.21400535 fisher_loss 0.251994699 triplet loss 0.962010682 l2_loss 13.6331015 fraction B 0.334482342 lossA 2.26822495 fraction A 0.039336279\n",
      "step 2344 loss 0.972962737 fisher_loss 0.251832455 triplet loss 0.721130311 l2_loss 13.6377373 fraction B 0.216756105 lossA 2.29247665 fraction A 0.0411352068\n",
      "step 2345 loss 1.14662719 fisher_loss 0.252298206 triplet loss 0.894329 l2_loss 13.6439915 fraction B 0.299396813 lossA 2.32253194 fraction A 0.0435522\n",
      "step 2346 loss 1.06876969 fisher_loss 0.252734303 triplet loss 0.81603539 l2_loss 13.6495056 fraction B 0.212686867 lossA 2.34004331 fraction A 0.0446940847\n",
      "step 2347 loss 1.14976275 fisher_loss 0.252988905 triplet loss 0.896773875 l2_loss 13.6542215 fraction B 0.297287047 lossA 2.3479054 fraction A 0.0451465435\n",
      "step 2348 loss 1.20297599 fisher_loss 0.253203452 triplet loss 0.949772537 l2_loss 13.6584339 fraction B 0.255495429 lossA 2.36742091 fraction A 0.0461799763\n",
      "step 2349 loss 1.12585938 fisher_loss 0.253524095 triplet loss 0.872335315 l2_loss 13.6629553 fraction B 0.204549298 lossA 2.37638617 fraction A 0.046858646\n",
      "step 2350 loss 1.10942054 fisher_loss 0.253694803 triplet loss 0.855725765 l2_loss 13.6680689 fraction B 0.186695904 lossA 2.41693664 fraction A 0.0509841554\n",
      "step 2351 loss 0.902498662 fisher_loss 0.254405856 triplet loss 0.648092806 l2_loss 13.6748943 fraction B 0.17412065 lossA 2.52009916 fraction A 0.0609919392\n",
      "step 2352 loss 0.985046506 fisher_loss 0.255961806 triplet loss 0.729084671 l2_loss 13.6849689 fraction B 0.19149296 lossA 2.58705068 fraction A 0.0664782673\n",
      "step 2353 loss 1.15181875 fisher_loss 0.2567572 triplet loss 0.895061493 l2_loss 13.69415 fraction B 0.206040248 lossA 2.53071284 fraction A 0.0634678\n",
      "step 2354 loss 1.04535496 fisher_loss 0.256438315 triplet loss 0.788916647 l2_loss 13.7009 fraction B 0.196599662 lossA 2.4216888 fraction A 0.0554780513\n",
      "step 2355 loss 1.00861335 fisher_loss 0.255396903 triplet loss 0.753216445 l2_loss 13.7066956 fraction B 0.276864529 lossA 2.28690886 fraction A 0.0445489064\n",
      "step 2356 loss 1.01218891 fisher_loss 0.254154384 triplet loss 0.758034527 l2_loss 13.7108221 fraction B 0.13282834 lossA 2.20378375 fraction A 0.038487982\n",
      "step 2357 loss 1.00448954 fisher_loss 0.25350973 triplet loss 0.750979781 l2_loss 13.7157764 fraction B 0.284214824 lossA 2.15870857 fraction A 0.0362951681\n",
      "step 2358 loss 1.34181619 fisher_loss 0.253546178 triplet loss 1.08827007 l2_loss 13.7222862 fraction B 0.315284163 lossA 2.16010094 fraction A 0.0371662155\n",
      "step 2359 loss 1.07240713 fisher_loss 0.253698289 triplet loss 0.818708837 l2_loss 13.7282372 fraction B 0.271813333 lossA 2.2524364 fraction A 0.0437450521\n",
      "step 2360 loss 1.12775898 fisher_loss 0.254011929 triplet loss 0.873747 l2_loss 13.7345686 fraction B 0.190585271 lossA 2.34178877 fraction A 0.0514705889\n",
      "step 2361 loss 1.03039956 fisher_loss 0.254284203 triplet loss 0.776115358 l2_loss 13.74053 fraction B 0.246822551 lossA 2.42256618 fraction A 0.0584913082\n",
      "step 2362 loss 0.986466706 fisher_loss 0.254631639 triplet loss 0.731835067 l2_loss 13.7464733 fraction B 0.194442138 lossA 2.50158691 fraction A 0.0654497221\n",
      "step 2363 loss 1.1612134 fisher_loss 0.255242646 triplet loss 0.905970693 l2_loss 13.7531805 fraction B 0.296200454 lossA 2.59174633 fraction A 0.0717529431\n",
      "step 2364 loss 1.23381341 fisher_loss 0.255565464 triplet loss 0.978247941 l2_loss 13.7588148 fraction B 0.214417472 lossA 2.63445354 fraction A 0.0741985515\n",
      "step 2365 loss 0.987681925 fisher_loss 0.255250812 triplet loss 0.732431114 l2_loss 13.7622118 fraction B 0.154039711 lossA 2.57311463 fraction A 0.0681802258\n",
      "step 2366 loss 0.954241753 fisher_loss 0.254110247 triplet loss 0.700131476 l2_loss 13.7636814 fraction B 0.290923119 lossA 2.46844697 fraction A 0.0579711869\n",
      "step 2367 loss 1.23751807 fisher_loss 0.252534151 triplet loss 0.984983861 l2_loss 13.764823 fraction B 0.20876047 lossA 2.34885478 fraction A 0.0462286621\n",
      "step 2368 loss 0.993149877 fisher_loss 0.251103133 triplet loss 0.742046714 l2_loss 13.7655659 fraction B 0.192745551 lossA 2.21934962 fraction A 0.0376537\n",
      "step 2369 loss 1.00103021 fisher_loss 0.250081837 triplet loss 0.75094831 l2_loss 13.7672234 fraction B 0.254838705 lossA 2.20675611 fraction A 0.0365856737\n",
      "step 2370 loss 1.16501498 fisher_loss 0.249635458 triplet loss 0.915379524 l2_loss 13.7718782 fraction B 0.269095033 lossA 2.30584192 fraction A 0.041797936\n",
      "step 2371 loss 1.13770986 fisher_loss 0.249656498 triplet loss 0.888053298 l2_loss 13.7773733 fraction B 0.230541766 lossA 2.44473 fraction A 0.0516772754\n",
      "step 2372 loss 1.01623034 fisher_loss 0.250385344 triplet loss 0.76584506 l2_loss 13.784585 fraction B 0.156516626 lossA 2.57298112 fraction A 0.0622528903\n",
      "step 2373 loss 1.08094907 fisher_loss 0.25146383 triplet loss 0.829485178 l2_loss 13.7926531 fraction B 0.265376389 lossA 2.66764 fraction A 0.0687195808\n",
      "step 2374 loss 1.11226916 fisher_loss 0.252236605 triplet loss 0.860032618 l2_loss 13.7995481 fraction B 0.204549134 lossA 2.67561841 fraction A 0.0688561723\n",
      "step 2375 loss 0.967323303 fisher_loss 0.25221771 triplet loss 0.715105593 l2_loss 13.8046093 fraction B 0.201457664 lossA 2.59212065 fraction A 0.0629329681\n",
      "step 2376 loss 1.04314733 fisher_loss 0.251328796 triplet loss 0.7918185 l2_loss 13.8086987 fraction B 0.288300604 lossA 2.49188852 fraction A 0.0550269336\n",
      "step 2377 loss 1.01300883 fisher_loss 0.250478387 triplet loss 0.762530506 l2_loss 13.8120403 fraction B 0.159874305 lossA 2.38917136 fraction A 0.0468340144\n",
      "step 2378 loss 1.02917683 fisher_loss 0.249908358 triplet loss 0.779268444 l2_loss 13.815218 fraction B 0.289429516 lossA 2.31894708 fraction A 0.041539453\n",
      "step 2379 loss 1.06067109 fisher_loss 0.249737963 triplet loss 0.810933113 l2_loss 13.8188677 fraction B 0.295020133 lossA 2.24707866 fraction A 0.0370787755\n",
      "step 2380 loss 1.33616686 fisher_loss 0.249474227 triplet loss 1.08669269 l2_loss 13.8220596 fraction B 0.30225578 lossA 2.21756029 fraction A 0.0354320891\n",
      "step 2381 loss 0.974404335 fisher_loss 0.249382541 triplet loss 0.72502178 l2_loss 13.8246861 fraction B 0.269294441 lossA 2.29430127 fraction A 0.0406312048\n",
      "step 2382 loss 1.121562 fisher_loss 0.249996901 triplet loss 0.871565104 l2_loss 13.82938 fraction B 0.312586963 lossA 2.48855591 fraction A 0.0560807958\n",
      "step 2383 loss 1.07981968 fisher_loss 0.251939 triplet loss 0.827880621 l2_loss 13.8361702 fraction B 0.230817869 lossA 2.69886279 fraction A 0.0723306909\n",
      "step 2384 loss 1.25625038 fisher_loss 0.254591405 triplet loss 1.00165904 l2_loss 13.8437433 fraction B 0.249522388 lossA 2.8313992 fraction A 0.0820510834\n",
      "step 2385 loss 1.20734501 fisher_loss 0.256537467 triplet loss 0.950807512 l2_loss 13.8498878 fraction B 0.308585197 lossA 2.82822466 fraction A 0.081801258\n",
      "step 2386 loss 1.31500232 fisher_loss 0.256445318 triplet loss 1.05855703 l2_loss 13.8529577 fraction B 0.235962734 lossA 2.64204884 fraction A 0.068956\n",
      "step 2387 loss 1.16251206 fisher_loss 0.253989577 triplet loss 0.908522487 l2_loss 13.85256 fraction B 0.191263303 lossA 2.45120311 fraction A 0.0547284223\n",
      "step 2388 loss 1.02354145 fisher_loss 0.251718938 triplet loss 0.771822512 l2_loss 13.8525028 fraction B 0.217297867 lossA 2.31984687 fraction A 0.0456834\n",
      "step 2389 loss 1.10731316 fisher_loss 0.250622392 triplet loss 0.856690824 l2_loss 13.8548088 fraction B 0.21506916 lossA 2.20402813 fraction A 0.0384791791\n",
      "step 2390 loss 1.23479927 fisher_loss 0.249670446 triplet loss 0.98512882 l2_loss 13.8573771 fraction B 0.214067921 lossA 2.15268874 fraction A 0.036221128\n",
      "step 2391 loss 1.16338122 fisher_loss 0.249289796 triplet loss 0.914091468 l2_loss 13.8618231 fraction B 0.231874734 lossA 2.13148403 fraction A 0.0356900319\n",
      "step 2392 loss 1.08800578 fisher_loss 0.249148026 triplet loss 0.83885771 l2_loss 13.86619 fraction B 0.22284691 lossA 2.17947125 fraction A 0.0388312638\n",
      "step 2393 loss 1.11459816 fisher_loss 0.249228939 triplet loss 0.865369201 l2_loss 13.8707886 fraction B 0.299580723 lossA 2.26077461 fraction A 0.0444214754\n",
      "step 2394 loss 0.907672226 fisher_loss 0.249442115 triplet loss 0.658230126 l2_loss 13.8753061 fraction B 0.197246149 lossA 2.35445261 fraction A 0.0519225039\n",
      "step 2395 loss 1.1426729 fisher_loss 0.2501221 triplet loss 0.892550766 l2_loss 13.8811226 fraction B 0.29313758 lossA 2.4501009 fraction A 0.0590524711\n",
      "step 2396 loss 1.06679809 fisher_loss 0.250644416 triplet loss 0.816153646 l2_loss 13.8861904 fraction B 0.203546807 lossA 2.47801471 fraction A 0.0604516082\n",
      "step 2397 loss 0.851100087 fisher_loss 0.250386477 triplet loss 0.600713611 l2_loss 13.8898487 fraction B 0.13560091 lossA 2.44885349 fraction A 0.0572192036\n",
      "step 2398 loss 1.12355912 fisher_loss 0.249737531 triplet loss 0.873821616 l2_loss 13.8936596 fraction B 0.207669824 lossA 2.42228055 fraction A 0.0543630794\n",
      "step 2399 loss 1.38061941 fisher_loss 0.249447703 triplet loss 1.1311717 l2_loss 13.8980179 fraction B 0.288265 lossA 2.40736771 fraction A 0.0523012131\n",
      "step 2400 loss 1.0597043 fisher_loss 0.249182656 triplet loss 0.810521603 l2_loss 13.9018936 fraction B 0.225678682 lossA 2.39064622 fraction A 0.0503518879\n",
      "step 2401 loss 0.965367138 fisher_loss 0.248856544 triplet loss 0.716510594 l2_loss 13.9061852 fraction B 0.191893026 lossA 2.35455275 fraction A 0.048078265\n",
      "step 2402 loss 1.07676888 fisher_loss 0.248622358 triplet loss 0.828146517 l2_loss 13.9121714 fraction B 0.163749486 lossA 2.36118484 fraction A 0.0487354882\n",
      "step 2403 loss 1.10170519 fisher_loss 0.24860172 triplet loss 0.853103459 l2_loss 13.9177723 fraction B 0.174408212 lossA 2.34322023 fraction A 0.0480358452\n",
      "step 2404 loss 1.18846524 fisher_loss 0.248417661 triplet loss 0.940047622 l2_loss 13.9235229 fraction B 0.19309102 lossA 2.36640692 fraction A 0.0499595627\n",
      "step 2405 loss 0.874616265 fisher_loss 0.248708814 triplet loss 0.625907481 l2_loss 13.9301128 fraction B 0.142233506 lossA 2.34590673 fraction A 0.0493643135\n",
      "step 2406 loss 1.07587266 fisher_loss 0.248717546 triplet loss 0.827155113 l2_loss 13.9362812 fraction B 0.2730169 lossA 2.33797741 fraction A 0.0499569178\n",
      "step 2407 loss 1.16552222 fisher_loss 0.248780236 triplet loss 0.916741967 l2_loss 13.9429903 fraction B 0.364480525 lossA 2.32802868 fraction A 0.049968183\n",
      "step 2408 loss 1.12211847 fisher_loss 0.248600259 triplet loss 0.873518169 l2_loss 13.9485388 fraction B 0.322670341 lossA 2.33506703 fraction A 0.0512894765\n",
      "step 2409 loss 0.986291826 fisher_loss 0.2486 triplet loss 0.73769182 l2_loss 13.9537334 fraction B 0.143801779 lossA 2.43660736 fraction A 0.0592017435\n",
      "step 2410 loss 1.19124651 fisher_loss 0.249564096 triplet loss 0.941682458 l2_loss 13.9601507 fraction B 0.311206907 lossA 2.46897793 fraction A 0.0613983609\n",
      "step 2411 loss 0.99485296 fisher_loss 0.24958612 triplet loss 0.745266855 l2_loss 13.9647884 fraction B 0.170942008 lossA 2.44575906 fraction A 0.0590956844\n",
      "step 2412 loss 0.877807915 fisher_loss 0.249062225 triplet loss 0.628745675 l2_loss 13.9691343 fraction B 0.173506916 lossA 2.36412024 fraction A 0.0525495932\n",
      "step 2413 loss 0.97205919 fisher_loss 0.247906163 triplet loss 0.724153042 l2_loss 13.9736662 fraction B 0.242948055 lossA 2.2835319 fraction A 0.0455450676\n",
      "step 2414 loss 1.45844042 fisher_loss 0.246877983 triplet loss 1.2115624 l2_loss 13.9776211 fraction B 0.287371129 lossA 2.20585346 fraction A 0.0393395387\n",
      "step 2415 loss 1.04488158 fisher_loss 0.245986238 triplet loss 0.798895359 l2_loss 13.980545 fraction B 0.267414302 lossA 2.21461463 fraction A 0.0391220227\n",
      "step 2416 loss 1.01001263 fisher_loss 0.245473698 triplet loss 0.764538944 l2_loss 13.9843416 fraction B 0.247993305 lossA 2.24933505 fraction A 0.0408788584\n",
      "step 2417 loss 0.927701592 fisher_loss 0.245083 triplet loss 0.682618558 l2_loss 13.9876871 fraction B 0.212138966 lossA 2.33999658 fraction A 0.0472488739\n",
      "step 2418 loss 0.991618156 fisher_loss 0.245301 triplet loss 0.746317148 l2_loss 13.9929848 fraction B 0.227457926 lossA 2.45572662 fraction A 0.0556208789\n",
      "step 2419 loss 1.09687018 fisher_loss 0.246022046 triplet loss 0.850848138 l2_loss 13.999938 fraction B 0.260112792 lossA 2.55913448 fraction A 0.0632950887\n",
      "step 2420 loss 1.22187543 fisher_loss 0.246638671 triplet loss 0.975236773 l2_loss 14.0070753 fraction B 0.238105968 lossA 2.59916592 fraction A 0.0659505352\n",
      "step 2421 loss 1.12072587 fisher_loss 0.24667877 triplet loss 0.87404716 l2_loss 14.0123053 fraction B 0.210048988 lossA 2.58310366 fraction A 0.0649849847\n",
      "step 2422 loss 1.0678966 fisher_loss 0.246292129 triplet loss 0.82160449 l2_loss 14.0171146 fraction B 0.199073881 lossA 2.50879669 fraction A 0.0600004159\n",
      "step 2423 loss 0.954652131 fisher_loss 0.245469674 triplet loss 0.709182441 l2_loss 14.020916 fraction B 0.223972723 lossA 2.43516397 fraction A 0.0537402332\n",
      "step 2424 loss 0.981118143 fisher_loss 0.244716212 triplet loss 0.736401916 l2_loss 14.0249462 fraction B 0.20308429 lossA 2.30891752 fraction A 0.0433426164\n",
      "step 2425 loss 0.817633748 fisher_loss 0.243533656 triplet loss 0.574100077 l2_loss 14.0274687 fraction B 0.129341692 lossA 2.22494197 fraction A 0.037210118\n",
      "step 2426 loss 1.06014037 fisher_loss 0.24312681 triplet loss 0.817013562 l2_loss 14.0322666 fraction B 0.252295524 lossA 2.20380974 fraction A 0.0358404294\n",
      "step 2427 loss 0.903381705 fisher_loss 0.243333787 triplet loss 0.660047889 l2_loss 14.0383787 fraction B 0.230947852 lossA 2.34691453 fraction A 0.0455569848\n",
      "step 2428 loss 1.14775634 fisher_loss 0.244529128 triplet loss 0.90322715 l2_loss 14.0464172 fraction B 0.291575044 lossA 2.50424886 fraction A 0.0579595938\n",
      "step 2429 loss 1.30489862 fisher_loss 0.246419311 triplet loss 1.05847931 l2_loss 14.0551605 fraction B 0.30287233 lossA 2.63060546 fraction A 0.0677121207\n",
      "step 2430 loss 0.907175064 fisher_loss 0.24821274 triplet loss 0.658962309 l2_loss 14.0628223 fraction B 0.148598462 lossA 2.7327137 fraction A 0.0744244754\n",
      "step 2431 loss 1.27377331 fisher_loss 0.249810249 triplet loss 1.02396309 l2_loss 14.0693922 fraction B 0.236733943 lossA 2.75210905 fraction A 0.0757508054\n",
      "step 2432 loss 1.06790817 fisher_loss 0.250502825 triplet loss 0.817405343 l2_loss 14.0738077 fraction B 0.248859659 lossA 2.71965885 fraction A 0.0730339587\n",
      "step 2433 loss 0.994641423 fisher_loss 0.25042513 triplet loss 0.744216323 l2_loss 14.0763092 fraction B 0.169484794 lossA 2.62855935 fraction A 0.065710634\n",
      "step 2434 loss 1.16573191 fisher_loss 0.249722183 triplet loss 0.916009665 l2_loss 14.0786781 fraction B 0.182405323 lossA 2.51844597 fraction A 0.05660972\n",
      "step 2435 loss 1.4127537 fisher_loss 0.248750791 triplet loss 1.1640029 l2_loss 14.0807734 fraction B 0.318746865 lossA 2.41986084 fraction A 0.047200948\n",
      "step 2436 loss 1.06301415 fisher_loss 0.247762486 triplet loss 0.815251648 l2_loss 14.0817261 fraction B 0.301124215 lossA 2.34718752 fraction A 0.0413401201\n",
      "step 2437 loss 1.10073268 fisher_loss 0.247201964 triplet loss 0.853530765 l2_loss 14.0838709 fraction B 0.236733034 lossA 2.3362689 fraction A 0.0403242894\n",
      "step 2438 loss 1.12755525 fisher_loss 0.246995598 triplet loss 0.880559683 l2_loss 14.0876932 fraction B 0.302560329 lossA 2.36507583 fraction A 0.0420589931\n",
      "step 2439 loss 1.01957309 fisher_loss 0.247033015 triplet loss 0.772540033 l2_loss 14.0918512 fraction B 0.309414655 lossA 2.48067379 fraction A 0.0520214662\n",
      "step 2440 loss 0.941459894 fisher_loss 0.248120517 triplet loss 0.693339407 l2_loss 14.0991421 fraction B 0.146351159 lossA 2.6023035 fraction A 0.0628581345\n",
      "step 2441 loss 1.15263689 fisher_loss 0.249657691 triplet loss 0.902979195 l2_loss 14.1073551 fraction B 0.193755046 lossA 2.72088575 fraction A 0.0715993941\n",
      "step 2442 loss 0.944145083 fisher_loss 0.251194268 triplet loss 0.692950785 l2_loss 14.1152811 fraction B 0.265272409 lossA 2.75129747 fraction A 0.0743932128\n",
      "step 2443 loss 1.14234459 fisher_loss 0.251913548 triplet loss 0.890431046 l2_loss 14.1218481 fraction B 0.236813053 lossA 2.70517945 fraction A 0.0717849582\n",
      "step 2444 loss 1.13753366 fisher_loss 0.251588404 triplet loss 0.885945201 l2_loss 14.1269331 fraction B 0.235660583 lossA 2.59174323 fraction A 0.0641917437\n",
      "step 2445 loss 1.1798327 fisher_loss 0.250377476 triplet loss 0.929455221 l2_loss 14.1305599 fraction B 0.331069171 lossA 2.46558166 fraction A 0.0545488298\n",
      "step 2446 loss 1.13209391 fisher_loss 0.248931408 triplet loss 0.883162498 l2_loss 14.1326685 fraction B 0.240877897 lossA 2.37020016 fraction A 0.0468108691\n",
      "step 2447 loss 1.0656265 fisher_loss 0.247888848 triplet loss 0.817737699 l2_loss 14.1354971 fraction B 0.229630396 lossA 2.27831197 fraction A 0.040218845\n",
      "step 2448 loss 1.25240123 fisher_loss 0.247000188 triplet loss 1.00540102 l2_loss 14.1391659 fraction B 0.277345747 lossA 2.21118903 fraction A 0.0362980291\n",
      "step 2449 loss 0.872893929 fisher_loss 0.246304318 triplet loss 0.626589596 l2_loss 14.1429224 fraction B 0.213701263 lossA 2.21729469 fraction A 0.0370963775\n",
      "step 2450 loss 0.995224416 fisher_loss 0.246240839 triplet loss 0.748983562 l2_loss 14.1492062 fraction B 0.21003148 lossA 2.31372905 fraction A 0.0442335494\n",
      "step 2451 loss 0.943247616 fisher_loss 0.247031972 triplet loss 0.69621563 l2_loss 14.1578875 fraction B 0.222365573 lossA 2.41488338 fraction A 0.0532636121\n",
      "step 2452 loss 1.23221803 fisher_loss 0.247956812 triplet loss 0.984261215 l2_loss 14.1659508 fraction B 0.264042169 lossA 2.47609115 fraction A 0.0590894185\n",
      "step 2453 loss 1.0282284 fisher_loss 0.248373464 triplet loss 0.779854894 l2_loss 14.1725655 fraction B 0.281349748 lossA 2.50255 fraction A 0.0614595413\n",
      "step 2454 loss 1.33967459 fisher_loss 0.248351693 triplet loss 1.0913229 l2_loss 14.1790333 fraction B 0.291274607 lossA 2.48165536 fraction A 0.0598691106\n",
      "step 2455 loss 1.07021224 fisher_loss 0.247555286 triplet loss 0.822657 l2_loss 14.1838455 fraction B 0.209725797 lossA 2.48940945 fraction A 0.060491126\n",
      "step 2456 loss 0.957372606 fisher_loss 0.246879086 triplet loss 0.710493505 l2_loss 14.18853 fraction B 0.153302073 lossA 2.55169106 fraction A 0.064573057\n",
      "step 2457 loss 1.13944 fisher_loss 0.246898323 triplet loss 0.892541587 l2_loss 14.1945953 fraction B 0.287019402 lossA 2.54491568 fraction A 0.0637777895\n",
      "step 2458 loss 1.1213789 fisher_loss 0.246323243 triplet loss 0.875055611 l2_loss 14.1998701 fraction B 0.314249843 lossA 2.45528102 fraction A 0.0566677116\n",
      "step 2459 loss 1.0184077 fisher_loss 0.244926423 triplet loss 0.773481309 l2_loss 14.2028894 fraction B 0.235543832 lossA 2.3798337 fraction A 0.0505589396\n",
      "step 2460 loss 0.878083766 fisher_loss 0.243925214 triplet loss 0.634158552 l2_loss 14.2066126 fraction B 0.190459892 lossA 2.30608726 fraction A 0.0447915606\n",
      "step 2461 loss 0.804007888 fisher_loss 0.243371606 triplet loss 0.560636282 l2_loss 14.2106819 fraction B 0.139311284 lossA 2.24134374 fraction A 0.0399105549\n",
      "step 2462 loss 1.19682956 fisher_loss 0.243330508 triplet loss 0.953499 l2_loss 14.2159786 fraction B 0.289604485 lossA 2.2548008 fraction A 0.0411182567\n",
      "step 2463 loss 1.24776 fisher_loss 0.243874773 triplet loss 1.00388527 l2_loss 14.22155 fraction B 0.25572598 lossA 2.2593298 fraction A 0.0414458178\n",
      "step 2464 loss 1.07933009 fisher_loss 0.244089574 triplet loss 0.835240483 l2_loss 14.2255096 fraction B 0.255988657 lossA 2.31553888 fraction A 0.0454687439\n",
      "step 2465 loss 1.11700308 fisher_loss 0.244400382 triplet loss 0.872602701 l2_loss 14.2309942 fraction B 0.22714439 lossA 2.36432624 fraction A 0.0498245955\n",
      "step 2466 loss 1.14120293 fisher_loss 0.244626716 triplet loss 0.896576166 l2_loss 14.2377453 fraction B 0.271892965 lossA 2.38395166 fraction A 0.0518003628\n",
      "step 2467 loss 1.09362864 fisher_loss 0.244568601 triplet loss 0.849060059 l2_loss 14.2433195 fraction B 0.282304168 lossA 2.43129897 fraction A 0.0552748069\n",
      "step 2468 loss 0.977893829 fisher_loss 0.244562745 triplet loss 0.733331084 l2_loss 14.2488356 fraction B 0.138538986 lossA 2.46343303 fraction A 0.0575305745\n",
      "step 2469 loss 1.21391773 fisher_loss 0.244611964 triplet loss 0.969305813 l2_loss 14.2548647 fraction B 0.26853177 lossA 2.47495484 fraction A 0.05852025\n",
      "step 2470 loss 0.913989127 fisher_loss 0.244604394 triplet loss 0.669384718 l2_loss 14.26015 fraction B 0.192234397 lossA 2.47009087 fraction A 0.0580496453\n",
      "step 2471 loss 1.07826388 fisher_loss 0.244625643 triplet loss 0.833638251 l2_loss 14.2657137 fraction B 0.229411766 lossA 2.42208 fraction A 0.0536341704\n",
      "step 2472 loss 0.970272303 fisher_loss 0.244035661 triplet loss 0.726236641 l2_loss 14.2695427 fraction B 0.229715243 lossA 2.37946653 fraction A 0.0495936386\n",
      "step 2473 loss 0.965506673 fisher_loss 0.243551522 triplet loss 0.72195518 l2_loss 14.2735758 fraction B 0.167864904 lossA 2.35205889 fraction A 0.04638239\n",
      "step 2474 loss 0.857949138 fisher_loss 0.243455917 triplet loss 0.614493191 l2_loss 14.2779016 fraction B 0.126568109 lossA 2.4780035 fraction A 0.0554982647\n",
      "step 2475 loss 1.08623528 fisher_loss 0.244864061 triplet loss 0.841371238 l2_loss 14.2872324 fraction B 0.197328776 lossA 2.55527163 fraction A 0.0613701046\n",
      "step 2476 loss 1.05205512 fisher_loss 0.245803505 triplet loss 0.806251585 l2_loss 14.2959766 fraction B 0.168171152 lossA 2.57029796 fraction A 0.0625524148\n",
      "step 2477 loss 1.00257814 fisher_loss 0.246267468 triplet loss 0.756310701 l2_loss 14.3043308 fraction B 0.233257398 lossA 2.61203027 fraction A 0.0658208206\n",
      "step 2478 loss 1.30801702 fisher_loss 0.247018814 triplet loss 1.0609982 l2_loss 14.3132277 fraction B 0.266455591 lossA 2.59588623 fraction A 0.0648070946\n",
      "step 2479 loss 1.31356287 fisher_loss 0.246991143 triplet loss 1.06657171 l2_loss 14.3193455 fraction B 0.267517507 lossA 2.52373672 fraction A 0.060089089\n",
      "step 2480 loss 0.958109856 fisher_loss 0.246437937 triplet loss 0.711671948 l2_loss 14.3241186 fraction B 0.187148944 lossA 2.44332933 fraction A 0.0540701114\n",
      "step 2481 loss 1.00548625 fisher_loss 0.245775327 triplet loss 0.759710908 l2_loss 14.3286591 fraction B 0.161424756 lossA 2.41868114 fraction A 0.0522496328\n",
      "step 2482 loss 1.3311547 fisher_loss 0.245523587 triplet loss 1.08563113 l2_loss 14.3340931 fraction B 0.283949077 lossA 2.39191437 fraction A 0.0498779863\n",
      "step 2483 loss 0.919059634 fisher_loss 0.245260373 triplet loss 0.673799276 l2_loss 14.3388872 fraction B 0.155456752 lossA 2.42109442 fraction A 0.0517307408\n",
      "step 2484 loss 0.85308516 fisher_loss 0.245521113 triplet loss 0.607564032 l2_loss 14.3447571 fraction B 0.138419569 lossA 2.50817037 fraction A 0.0587256327\n",
      "step 2485 loss 0.931724429 fisher_loss 0.246531829 triplet loss 0.685192585 l2_loss 14.3530874 fraction B 0.168940648 lossA 2.5922091 fraction A 0.0649912879\n",
      "step 2486 loss 0.895335555 fisher_loss 0.247511312 triplet loss 0.647824228 l2_loss 14.3612576 fraction B 0.130490631 lossA 2.57244253 fraction A 0.0627441779\n",
      "step 2487 loss 1.05855608 fisher_loss 0.247442111 triplet loss 0.811113954 l2_loss 14.3665247 fraction B 0.268184096 lossA 2.52187634 fraction A 0.0581091233\n",
      "step 2488 loss 0.988544464 fisher_loss 0.246879503 triplet loss 0.741664946 l2_loss 14.3714085 fraction B 0.230119288 lossA 2.46366858 fraction A 0.0524850786\n",
      "step 2489 loss 1.24438894 fisher_loss 0.246467099 triplet loss 0.997921824 l2_loss 14.3763542 fraction B 0.264237851 lossA 2.38363481 fraction A 0.0454684198\n",
      "step 2490 loss 1.18988049 fisher_loss 0.245943397 triplet loss 0.943937123 l2_loss 14.380928 fraction B 0.32069245 lossA 2.32226038 fraction A 0.0411121\n",
      "step 2491 loss 1.07925534 fisher_loss 0.245730281 triplet loss 0.833525121 l2_loss 14.385725 fraction B 0.230466664 lossA 2.29505324 fraction A 0.0399969108\n",
      "step 2492 loss 0.94772476 fisher_loss 0.245758966 triplet loss 0.701965809 l2_loss 14.3914347 fraction B 0.218971416 lossA 2.36706138 fraction A 0.0469209105\n",
      "step 2493 loss 0.830543697 fisher_loss 0.246536836 triplet loss 0.584006846 l2_loss 14.4005928 fraction B 0.167243302 lossA 2.50196409 fraction A 0.0601112954\n",
      "step 2494 loss 1.41323745 fisher_loss 0.248260856 triplet loss 1.1649766 l2_loss 14.412859 fraction B 0.197087616 lossA 2.586092 fraction A 0.0679565892\n",
      "step 2495 loss 1.05951941 fisher_loss 0.249800906 triplet loss 0.80971849 l2_loss 14.4232445 fraction B 0.182854712 lossA 2.54745412 fraction A 0.0659832433\n",
      "step 2496 loss 1.21274698 fisher_loss 0.24994652 triplet loss 0.962800443 l2_loss 14.4317894 fraction B 0.200700879 lossA 2.48722339 fraction A 0.06151025\n",
      "step 2497 loss 0.865862429 fisher_loss 0.249412522 triplet loss 0.616449893 l2_loss 14.4386911 fraction B 0.169517502 lossA 2.45351505 fraction A 0.059319973\n",
      "step 2498 loss 1.00144017 fisher_loss 0.249512345 triplet loss 0.751927853 l2_loss 14.4473648 fraction B 0.197957516 lossA 2.40457273 fraction A 0.0556460172\n",
      "step 2499 loss 1.04570436 fisher_loss 0.249486685 triplet loss 0.79621762 l2_loss 14.4564095 fraction B 0.216283992 lossA 2.32368565 fraction A 0.048756931\n",
      "step 2500 loss 0.899867415 fisher_loss 0.2487984 triplet loss 0.651069045 l2_loss 14.4628382 fraction B 0.190988764 lossA 2.3344636 fraction A 0.0493151583\n",
      "step 2501 loss 0.940268 fisher_loss 0.248807922 triplet loss 0.691460073 l2_loss 14.4709702 fraction B 0.195909038 lossA 2.35331511 fraction A 0.0508950464\n",
      "step 2502 loss 1.11764598 fisher_loss 0.249006674 triplet loss 0.86863935 l2_loss 14.4795504 fraction B 0.287136734 lossA 2.35492158 fraction A 0.0510122664\n",
      "step 2503 loss 1.08320701 fisher_loss 0.249025241 triplet loss 0.834181726 l2_loss 14.4866858 fraction B 0.253332764 lossA 2.36186171 fraction A 0.0514218695\n",
      "step 2504 loss 0.967606544 fisher_loss 0.249083593 triplet loss 0.718522966 l2_loss 14.4938021 fraction B 0.210981563 lossA 2.39303112 fraction A 0.0538805574\n",
      "step 2505 loss 0.983496547 fisher_loss 0.24947378 triplet loss 0.734022737 l2_loss 14.5015774 fraction B 0.165045127 lossA 2.43046808 fraction A 0.056630224\n",
      "step 2506 loss 0.962078571 fisher_loss 0.250023872 triplet loss 0.712054729 l2_loss 14.5091505 fraction B 0.201987982 lossA 2.43846869 fraction A 0.0580389947\n",
      "step 2507 loss 1.13662326 fisher_loss 0.250897676 triplet loss 0.885725617 l2_loss 14.5180111 fraction B 0.20681338 lossA 2.40252042 fraction A 0.0554650091\n",
      "step 2508 loss 1.21295619 fisher_loss 0.251520693 triplet loss 0.961435437 l2_loss 14.5258532 fraction B 0.275966734 lossA 2.36430168 fraction A 0.0523931831\n",
      "step 2509 loss 0.943926334 fisher_loss 0.25210765 triplet loss 0.691818714 l2_loss 14.5331545 fraction B 0.169377208 lossA 2.34199262 fraction A 0.0503999926\n",
      "step 2510 loss 1.12941194 fisher_loss 0.252928615 triplet loss 0.876483381 l2_loss 14.5406971 fraction B 0.297608674 lossA 2.32359338 fraction A 0.0489451103\n",
      "step 2511 loss 0.933689058 fisher_loss 0.253457844 triplet loss 0.680231214 l2_loss 14.5482216 fraction B 0.186606392 lossA 2.29665327 fraction A 0.0465810336\n",
      "step 2512 loss 0.952144861 fisher_loss 0.25327158 triplet loss 0.698873281 l2_loss 14.5548773 fraction B 0.248495638 lossA 2.3002162 fraction A 0.0466270372\n",
      "step 2513 loss 1.03032041 fisher_loss 0.253206283 triplet loss 0.777114093 l2_loss 14.562912 fraction B 0.247058585 lossA 2.35391283 fraction A 0.0505644418\n",
      "step 2514 loss 1.34350693 fisher_loss 0.253550649 triplet loss 1.08995628 l2_loss 14.571456 fraction B 0.175834104 lossA 2.39661264 fraction A 0.0540981106\n",
      "step 2515 loss 1.33680463 fisher_loss 0.25410527 triplet loss 1.08269942 l2_loss 14.5792561 fraction B 0.298282087 lossA 2.50423622 fraction A 0.0630453303\n",
      "step 2516 loss 1.32413 fisher_loss 0.255533904 triplet loss 1.06859612 l2_loss 14.5874681 fraction B 0.223045528 lossA 2.52527785 fraction A 0.0650849938\n",
      "step 2517 loss 0.821966887 fisher_loss 0.256390244 triplet loss 0.565576673 l2_loss 14.5941954 fraction B 0.161886707 lossA 2.50771809 fraction A 0.0644334257\n",
      "step 2518 loss 0.906468034 fisher_loss 0.257157862 triplet loss 0.649310172 l2_loss 14.6019096 fraction B 0.173302338 lossA 2.41884255 fraction A 0.0576470308\n",
      "step 2519 loss 1.0897 fisher_loss 0.25722909 triplet loss 0.832470953 l2_loss 14.6089687 fraction B 0.263456672 lossA 2.3515439 fraction A 0.0519560091\n",
      "step 2520 loss 1.01674008 fisher_loss 0.257334709 triplet loss 0.759405434 l2_loss 14.6155291 fraction B 0.17093873 lossA 2.2929759 fraction A 0.0465904884\n",
      "step 2521 loss 1.24479699 fisher_loss 0.257611603 triplet loss 0.987185359 l2_loss 14.6220636 fraction B 0.338503927 lossA 2.20901561 fraction A 0.0401346646\n",
      "step 2522 loss 1.14297175 fisher_loss 0.257373869 triplet loss 0.885597885 l2_loss 14.6280775 fraction B 0.230423808 lossA 2.20134163 fraction A 0.0403794944\n",
      "step 2523 loss 0.783939242 fisher_loss 0.257764369 triplet loss 0.526174843 l2_loss 14.6363792 fraction B 0.141454026 lossA 2.24459481 fraction A 0.0449313074\n",
      "step 2524 loss 1.07267714 fisher_loss 0.258838177 triplet loss 0.813839 l2_loss 14.6472187 fraction B 0.26454398 lossA 2.3089695 fraction A 0.0511121303\n",
      "step 2525 loss 1.08187175 fisher_loss 0.26024282 triplet loss 0.821629 l2_loss 14.658123 fraction B 0.221048102 lossA 2.36517978 fraction A 0.0563596748\n",
      "step 2526 loss 1.21322095 fisher_loss 0.261323184 triplet loss 0.95189774 l2_loss 14.6680079 fraction B 0.302568763 lossA 2.3930614 fraction A 0.0589686148\n",
      "step 2527 loss 1.00028932 fisher_loss 0.261915833 triplet loss 0.738373458 l2_loss 14.6763706 fraction B 0.291844666 lossA 2.36360312 fraction A 0.0563548952\n",
      "step 2528 loss 1.08232749 fisher_loss 0.261552 triplet loss 0.820775509 l2_loss 14.6823416 fraction B 0.237797081 lossA 2.30162525 fraction A 0.0510815233\n",
      "step 2529 loss 1.02206695 fisher_loss 0.2607117 triplet loss 0.761355221 l2_loss 14.6876383 fraction B 0.212883309 lossA 2.24350572 fraction A 0.0455058\n",
      "step 2530 loss 1.19296753 fisher_loss 0.259948909 triplet loss 0.933018625 l2_loss 14.6926193 fraction B 0.264367104 lossA 2.22206163 fraction A 0.0428990349\n",
      "step 2531 loss 1.00394249 fisher_loss 0.259859115 triplet loss 0.744083405 l2_loss 14.6974831 fraction B 0.224121481 lossA 2.23371935 fraction A 0.0429790877\n",
      "step 2532 loss 1.11632252 fisher_loss 0.260025978 triplet loss 0.85629648 l2_loss 14.7029123 fraction B 0.211655363 lossA 2.31213093 fraction A 0.0479611568\n",
      "step 2533 loss 0.956944823 fisher_loss 0.260477781 triplet loss 0.696467042 l2_loss 14.7092495 fraction B 0.172025383 lossA 2.38380289 fraction A 0.0538130403\n",
      "step 2534 loss 0.93354249 fisher_loss 0.260680348 triplet loss 0.672862113 l2_loss 14.7161751 fraction B 0.169108078 lossA 2.49239969 fraction A 0.0622091666\n",
      "step 2535 loss 1.17301321 fisher_loss 0.260655463 triplet loss 0.912357748 l2_loss 14.7232962 fraction B 0.275666118 lossA 2.56695056 fraction A 0.0672648773\n",
      "step 2536 loss 1.12546778 fisher_loss 0.260204554 triplet loss 0.865263283 l2_loss 14.7290077 fraction B 0.232299581 lossA 2.56679487 fraction A 0.0668778345\n",
      "step 2537 loss 1.19457841 fisher_loss 0.259101242 triplet loss 0.935477197 l2_loss 14.7328863 fraction B 0.211500779 lossA 2.51416945 fraction A 0.0617648959\n",
      "step 2538 loss 1.09089398 fisher_loss 0.257361829 triplet loss 0.833532095 l2_loss 14.7349176 fraction B 0.181564361 lossA 2.43567371 fraction A 0.054303024\n",
      "step 2539 loss 1.10902393 fisher_loss 0.255571812 triplet loss 0.853452086 l2_loss 14.7363605 fraction B 0.297529846 lossA 2.32574201 fraction A 0.0448061973\n",
      "step 2540 loss 1.08294261 fisher_loss 0.253749907 triplet loss 0.829192698 l2_loss 14.7363243 fraction B 0.236811519 lossA 2.24789166 fraction A 0.0377152786\n",
      "step 2541 loss 1.0505209 fisher_loss 0.252396911 triplet loss 0.798124 l2_loss 14.7361202 fraction B 0.28483519 lossA 2.12961817 fraction A 0.0315120332\n",
      "step 2542 loss 1.28665125 fisher_loss 0.251332253 triplet loss 1.03531897 l2_loss 14.736124 fraction B 0.280679226 lossA 2.17380881 fraction A 0.0330341831\n",
      "step 2543 loss 1.1475153 fisher_loss 0.250817597 triplet loss 0.89669776 l2_loss 14.738452 fraction B 0.264967948 lossA 2.25970459 fraction A 0.0372099727\n",
      "step 2544 loss 1.04175568 fisher_loss 0.250529498 triplet loss 0.791226149 l2_loss 14.7413158 fraction B 0.235940248 lossA 2.36157632 fraction A 0.0443032421\n",
      "step 2545 loss 1.06978428 fisher_loss 0.250703305 triplet loss 0.819081 l2_loss 14.7462397 fraction B 0.249937296 lossA 2.50913811 fraction A 0.0551225618\n",
      "step 2546 loss 0.9387151 fisher_loss 0.251549423 triplet loss 0.687165678 l2_loss 14.752039 fraction B 0.181951374 lossA 2.56855726 fraction A 0.0606904961\n",
      "step 2547 loss 1.03431261 fisher_loss 0.252038777 triplet loss 0.782273829 l2_loss 14.758563 fraction B 0.21336022 lossA 2.56856346 fraction A 0.062414404\n",
      "step 2548 loss 1.26063848 fisher_loss 0.251911759 triplet loss 1.00872672 l2_loss 14.764575 fraction B 0.298923314 lossA 2.52011871 fraction A 0.0607955046\n",
      "step 2549 loss 1.10610878 fisher_loss 0.251257 triplet loss 0.854851782 l2_loss 14.7686882 fraction B 0.223424867 lossA 2.37382412 fraction A 0.0519616269\n",
      "step 2550 loss 0.923967361 fisher_loss 0.249986708 triplet loss 0.673980653 l2_loss 14.7709713 fraction B 0.189458385 lossA 2.22797394 fraction A 0.0422276482\n",
      "step 2551 loss 1.06468725 fisher_loss 0.248958915 triplet loss 0.815728307 l2_loss 14.7731295 fraction B 0.253343165 lossA 2.14509773 fraction A 0.0380145125\n",
      "step 2552 loss 0.932826519 fisher_loss 0.248773262 triplet loss 0.684053242 l2_loss 14.7766438 fraction B 0.194343731 lossA 2.12356353 fraction A 0.0375827774\n",
      "step 2553 loss 1.06864476 fisher_loss 0.249088854 triplet loss 0.819555938 l2_loss 14.780756 fraction B 0.250812948 lossA 2.13057637 fraction A 0.0387127064\n",
      "step 2554 loss 0.906856894 fisher_loss 0.249704614 triplet loss 0.657152295 l2_loss 14.7848988 fraction B 0.172528118 lossA 2.1984024 fraction A 0.044286\n",
      "step 2555 loss 1.0060451 fisher_loss 0.250899374 triplet loss 0.755145729 l2_loss 14.7912598 fraction B 0.16856651 lossA 2.30511928 fraction A 0.0534971058\n",
      "step 2556 loss 0.819746733 fisher_loss 0.252494246 triplet loss 0.567252517 l2_loss 14.7989702 fraction B 0.125137836 lossA 2.37618375 fraction A 0.0590986162\n",
      "step 2557 loss 1.09149933 fisher_loss 0.253621399 triplet loss 0.837878 l2_loss 14.8064423 fraction B 0.165118784 lossA 2.44730639 fraction A 0.063870959\n",
      "step 2558 loss 1.47311544 fisher_loss 0.254459381 triplet loss 1.21865606 l2_loss 14.8137398 fraction B 0.276183963 lossA 2.46153021 fraction A 0.0640981421\n",
      "step 2559 loss 1.09362972 fisher_loss 0.254426956 triplet loss 0.839202762 l2_loss 14.8187046 fraction B 0.187054947 lossA 2.42294836 fraction A 0.0605329275\n",
      "step 2560 loss 1.05425143 fisher_loss 0.25380829 triplet loss 0.800443172 l2_loss 14.8233309 fraction B 0.29202953 lossA 2.3705802 fraction A 0.0552167781\n",
      "step 2561 loss 1.15430832 fisher_loss 0.252714306 triplet loss 0.901594043 l2_loss 14.8270216 fraction B 0.286953807 lossA 2.31108356 fraction A 0.0493448265\n",
      "step 2562 loss 1.17685902 fisher_loss 0.251333535 triplet loss 0.925525486 l2_loss 14.8301 fraction B 0.215496123 lossA 2.25774169 fraction A 0.0439378284\n",
      "step 2563 loss 0.948339343 fisher_loss 0.250132203 triplet loss 0.69820714 l2_loss 14.8330202 fraction B 0.14685 lossA 2.26676941 fraction A 0.0439647771\n",
      "step 2564 loss 1.14867151 fisher_loss 0.249858424 triplet loss 0.898813128 l2_loss 14.8384933 fraction B 0.283821672 lossA 2.32549334 fraction A 0.0479337722\n",
      "step 2565 loss 1.04069626 fisher_loss 0.250124186 triplet loss 0.790572107 l2_loss 14.8449059 fraction B 0.181474477 lossA 2.410918 fraction A 0.0536967628\n",
      "step 2566 loss 1.08609676 fisher_loss 0.250891626 triplet loss 0.835205078 l2_loss 14.8519983 fraction B 0.191171721 lossA 2.45948434 fraction A 0.0568047054\n",
      "step 2567 loss 1.06226158 fisher_loss 0.251058161 triplet loss 0.811203361 l2_loss 14.8573694 fraction B 0.190590248 lossA 2.46111155 fraction A 0.0565533563\n",
      "step 2568 loss 1.07392859 fisher_loss 0.250646919 triplet loss 0.823281705 l2_loss 14.8622465 fraction B 0.157745078 lossA 2.43825054 fraction A 0.0545322783\n",
      "step 2569 loss 0.980548 fisher_loss 0.24993369 triplet loss 0.730614305 l2_loss 14.8668423 fraction B 0.202490583 lossA 2.41390467 fraction A 0.0526951\n",
      "step 2570 loss 1.1543752 fisher_loss 0.249382347 triplet loss 0.904992819 l2_loss 14.8723 fraction B 0.309118 lossA 2.34504485 fraction A 0.0477700084\n",
      "step 2571 loss 1.06349123 fisher_loss 0.248292 triplet loss 0.815199196 l2_loss 14.8767157 fraction B 0.245892525 lossA 2.29877496 fraction A 0.0440016165\n",
      "step 2572 loss 0.96369648 fisher_loss 0.247451395 triplet loss 0.716245055 l2_loss 14.8810205 fraction B 0.190283984 lossA 2.33226371 fraction A 0.0467752591\n",
      "step 2573 loss 1.36246657 fisher_loss 0.247516781 triplet loss 1.11494982 l2_loss 14.8885269 fraction B 0.224131435 lossA 2.34877372 fraction A 0.0478982367\n",
      "step 2574 loss 0.872219741 fisher_loss 0.247187302 triplet loss 0.625032425 l2_loss 14.8942671 fraction B 0.181446686 lossA 2.37481189 fraction A 0.0501018427\n",
      "step 2575 loss 1.2200911 fisher_loss 0.2471091 triplet loss 0.972982049 l2_loss 14.9015522 fraction B 0.232643 lossA 2.42232776 fraction A 0.0545704551\n",
      "step 2576 loss 0.930757761 fisher_loss 0.247349843 triplet loss 0.683407903 l2_loss 14.9093256 fraction B 0.204387605 lossA 2.45329475 fraction A 0.0578560717\n",
      "step 2577 loss 1.21734333 fisher_loss 0.247825369 triplet loss 0.969518 l2_loss 14.9167595 fraction B 0.281771868 lossA 2.48084617 fraction A 0.0605013035\n",
      "step 2578 loss 1.02141881 fisher_loss 0.248063818 triplet loss 0.773354948 l2_loss 14.9232035 fraction B 0.206910312 lossA 2.46946383 fraction A 0.0602915026\n",
      "step 2579 loss 0.919250727 fisher_loss 0.247944325 triplet loss 0.671306431 l2_loss 14.9287815 fraction B 0.175269186 lossA 2.37121224 fraction A 0.0525121\n",
      "step 2580 loss 1.04251528 fisher_loss 0.246717215 triplet loss 0.795798063 l2_loss 14.931776 fraction B 0.165551692 lossA 2.27196312 fraction A 0.0443588085\n",
      "step 2581 loss 1.08367014 fisher_loss 0.245733589 triplet loss 0.837936521 l2_loss 14.9350452 fraction B 0.236889094 lossA 2.26343799 fraction A 0.0433200151\n",
      "step 2582 loss 1.04939842 fisher_loss 0.245614156 triplet loss 0.803784311 l2_loss 14.9406233 fraction B 0.181312293 lossA 2.31050754 fraction A 0.0464367606\n",
      "step 2583 loss 1.3253336 fisher_loss 0.246040672 triplet loss 1.07929289 l2_loss 14.946661 fraction B 0.251701117 lossA 2.32321334 fraction A 0.0469684713\n",
      "step 2584 loss 1.24716914 fisher_loss 0.245887265 triplet loss 1.00128186 l2_loss 14.9507828 fraction B 0.327334851 lossA 2.33767033 fraction A 0.0474011898\n",
      "step 2585 loss 1.0061909 fisher_loss 0.245536685 triplet loss 0.760654211 l2_loss 14.953908 fraction B 0.118292302 lossA 2.37803411 fraction A 0.0498071723\n",
      "step 2586 loss 1.01807714 fisher_loss 0.24527204 triplet loss 0.772805154 l2_loss 14.9590302 fraction B 0.209705517 lossA 2.42976379 fraction A 0.0534175225\n",
      "step 2587 loss 0.993138909 fisher_loss 0.245022207 triplet loss 0.748116672 l2_loss 14.964426 fraction B 0.195101917 lossA 2.46908283 fraction A 0.0560946353\n",
      "step 2588 loss 1.03403735 fisher_loss 0.24454613 triplet loss 0.789491236 l2_loss 14.9691277 fraction B 0.163923755 lossA 2.49993396 fraction A 0.0577300526\n",
      "step 2589 loss 1.12887287 fisher_loss 0.243992388 triplet loss 0.884880543 l2_loss 14.9732695 fraction B 0.285615802 lossA 2.47437263 fraction A 0.0550091863\n",
      "step 2590 loss 0.909845471 fisher_loss 0.243028551 triplet loss 0.66681695 l2_loss 14.9759169 fraction B 0.179234415 lossA 2.44187903 fraction A 0.0515788607\n",
      "step 2591 loss 1.19843781 fisher_loss 0.242201045 triplet loss 0.95623672 l2_loss 14.9797583 fraction B 0.283509523 lossA 2.36214876 fraction A 0.0449458\n",
      "step 2592 loss 0.911075175 fisher_loss 0.241279244 triplet loss 0.66979593 l2_loss 14.9825773 fraction B 0.164574295 lossA 2.27830672 fraction A 0.0388075747\n",
      "step 2593 loss 1.12755644 fisher_loss 0.240621269 triplet loss 0.886935174 l2_loss 14.9854202 fraction B 0.296928465 lossA 2.20176315 fraction A 0.0339884348\n",
      "step 2594 loss 1.08822262 fisher_loss 0.2401236 triplet loss 0.848099053 l2_loss 14.9877014 fraction B 0.319471538 lossA 2.14867902 fraction A 0.0315087028\n",
      "step 2595 loss 1.00869703 fisher_loss 0.239919543 triplet loss 0.76877743 l2_loss 14.9902782 fraction B 0.217526183 lossA 2.20655656 fraction A 0.0339703597\n",
      "step 2596 loss 0.939956486 fisher_loss 0.240230277 triplet loss 0.699726224 l2_loss 14.9944267 fraction B 0.280359417 lossA 2.36777115 fraction A 0.0441174209\n",
      "step 2597 loss 1.09553635 fisher_loss 0.241356418 triplet loss 0.85418 l2_loss 15.0017376 fraction B 0.235326827 lossA 2.54862618 fraction A 0.0577721074\n",
      "step 2598 loss 1.21906006 fisher_loss 0.243360415 triplet loss 0.975699604 l2_loss 15.0094967 fraction B 0.320184022 lossA 2.67251968 fraction A 0.0671981573\n",
      "step 2599 loss 1.08537436 fisher_loss 0.245065928 triplet loss 0.840308368 l2_loss 15.0164852 fraction B 0.217173547 lossA 2.66590285 fraction A 0.067022875\n",
      "step 2600 loss 0.988220334 fisher_loss 0.245351225 triplet loss 0.742869079 l2_loss 15.0218277 fraction B 0.218239427 lossA 2.49445415 fraction A 0.0552847683\n",
      "step 2601 loss 1.06000042 fisher_loss 0.243995413 triplet loss 0.816005051 l2_loss 15.0259323 fraction B 0.190767065 lossA 2.29081607 fraction A 0.0414447673\n",
      "step 2602 loss 1.12890613 fisher_loss 0.242739886 triplet loss 0.886166275 l2_loss 15.0302181 fraction B 0.18233858 lossA 2.08320832 fraction A 0.0306952503\n",
      "step 2603 loss 1.06350577 fisher_loss 0.242124587 triplet loss 0.821381211 l2_loss 15.0345097 fraction B 0.272847086 lossA 2.06049895 fraction A 0.0304069538\n",
      "step 2604 loss 0.93080771 fisher_loss 0.242770836 triplet loss 0.688036859 l2_loss 15.0420609 fraction B 0.268797219 lossA 2.22570705 fraction A 0.0393641368\n",
      "step 2605 loss 1.12469292 fisher_loss 0.244245723 triplet loss 0.880447209 l2_loss 15.0511379 fraction B 0.200738922 lossA 2.41879034 fraction A 0.054487288\n",
      "step 2606 loss 1.13671 fisher_loss 0.246561572 triplet loss 0.890148461 l2_loss 15.0612383 fraction B 0.17676872 lossA 2.5526545 fraction A 0.0642135516\n",
      "step 2607 loss 1.10256886 fisher_loss 0.248591825 triplet loss 0.853977084 l2_loss 15.0693922 fraction B 0.251501739 lossA 2.58918691 fraction A 0.0672995821\n",
      "step 2608 loss 1.05603242 fisher_loss 0.249638453 triplet loss 0.806394 l2_loss 15.0751448 fraction B 0.245724261 lossA 2.5745275 fraction A 0.0664221942\n",
      "step 2609 loss 1.01051211 fisher_loss 0.250037134 triplet loss 0.76047492 l2_loss 15.0796022 fraction B 0.146008313 lossA 2.51290679 fraction A 0.0621431\n",
      "step 2610 loss 1.09968162 fisher_loss 0.249620616 triplet loss 0.850061059 l2_loss 15.0830421 fraction B 0.243599132 lossA 2.41003323 fraction A 0.0535151437\n",
      "step 2611 loss 1.1449194 fisher_loss 0.248714879 triplet loss 0.896204472 l2_loss 15.0856171 fraction B 0.282331 lossA 2.28062749 fraction A 0.0435174294\n",
      "step 2612 loss 1.02362847 fisher_loss 0.247635528 triplet loss 0.77599293 l2_loss 15.0873604 fraction B 0.283416152 lossA 2.1929338 fraction A 0.037199866\n",
      "step 2613 loss 0.942336082 fisher_loss 0.246733636 triplet loss 0.695602477 l2_loss 15.0900888 fraction B 0.179875419 lossA 2.19718766 fraction A 0.0374658145\n",
      "step 2614 loss 1.04195821 fisher_loss 0.24617 triplet loss 0.795788229 l2_loss 15.0955887 fraction B 0.255375892 lossA 2.26059914 fraction A 0.0420359224\n",
      "step 2615 loss 1.31339 fisher_loss 0.246012643 triplet loss 1.06737733 l2_loss 15.101367 fraction B 0.309165567 lossA 2.32816625 fraction A 0.0471346639\n",
      "step 2616 loss 0.957177758 fisher_loss 0.246006906 triplet loss 0.711170852 l2_loss 15.1067524 fraction B 0.178939745 lossA 2.40913224 fraction A 0.0532320254\n",
      "step 2617 loss 1.1789335 fisher_loss 0.246133745 triplet loss 0.932799757 l2_loss 15.1123285 fraction B 0.283329725 lossA 2.4566431 fraction A 0.0567574352\n",
      "step 2618 loss 1.04032111 fisher_loss 0.246000096 triplet loss 0.79432106 l2_loss 15.116621 fraction B 0.148585 lossA 2.50366354 fraction A 0.059864074\n",
      "step 2619 loss 1.14374959 fisher_loss 0.245879427 triplet loss 0.897870183 l2_loss 15.1214943 fraction B 0.213449284 lossA 2.5301168 fraction A 0.0611772202\n",
      "step 2620 loss 1.23718548 fisher_loss 0.245436177 triplet loss 0.991749346 l2_loss 15.1253548 fraction B 0.303571433 lossA 2.55515313 fraction A 0.0625570118\n",
      "step 2621 loss 0.903877079 fisher_loss 0.244816676 triplet loss 0.659060419 l2_loss 15.128912 fraction B 0.203310162 lossA 2.49372935 fraction A 0.0577701852\n",
      "step 2622 loss 1.1106894 fisher_loss 0.243801102 triplet loss 0.866888285 l2_loss 15.1322184 fraction B 0.235715702 lossA 2.3911109 fraction A 0.0497708768\n",
      "step 2623 loss 1.04142249 fisher_loss 0.24252902 triplet loss 0.798893511 l2_loss 15.1352482 fraction B 0.302264035 lossA 2.30975103 fraction A 0.0433758721\n",
      "step 2624 loss 1.02793193 fisher_loss 0.241513133 triplet loss 0.786418796 l2_loss 15.1391087 fraction B 0.229911193 lossA 2.24256206 fraction A 0.0384674445\n",
      "step 2625 loss 1.15796733 fisher_loss 0.240835831 triplet loss 0.917131543 l2_loss 15.1436052 fraction B 0.255287707 lossA 2.28769231 fraction A 0.0411783122\n",
      "step 2626 loss 1.31730664 fisher_loss 0.240873545 triplet loss 1.07643306 l2_loss 15.1489382 fraction B 0.225676537 lossA 2.34284163 fraction A 0.0453505516\n",
      "step 2627 loss 0.846083879 fisher_loss 0.241108328 triplet loss 0.604975581 l2_loss 15.154829 fraction B 0.149969131 lossA 2.49531627 fraction A 0.0578104667\n",
      "step 2628 loss 1.10994184 fisher_loss 0.242589489 triplet loss 0.867352307 l2_loss 15.164999 fraction B 0.297322929 lossA 2.60516596 fraction A 0.0665964633\n",
      "step 2629 loss 1.11759031 fisher_loss 0.243993953 triplet loss 0.87359637 l2_loss 15.1741056 fraction B 0.245156676 lossA 2.62711668 fraction A 0.0687562\n",
      "step 2630 loss 1.1234014 fisher_loss 0.244506255 triplet loss 0.878895104 l2_loss 15.1807976 fraction B 0.13550882 lossA 2.57314873 fraction A 0.065658547\n",
      "step 2631 loss 1.03906369 fisher_loss 0.244269967 triplet loss 0.794793785 l2_loss 15.186758 fraction B 0.148246244 lossA 2.47197032 fraction A 0.0581720769\n",
      "step 2632 loss 0.916716099 fisher_loss 0.243511662 triplet loss 0.673204422 l2_loss 15.1911201 fraction B 0.156262517 lossA 2.31083441 fraction A 0.0455088429\n",
      "step 2633 loss 0.958301544 fisher_loss 0.242476121 triplet loss 0.715825438 l2_loss 15.1953831 fraction B 0.167196497 lossA 2.17894053 fraction A 0.035402786\n",
      "step 2634 loss 1.11489427 fisher_loss 0.241755337 triplet loss 0.873138964 l2_loss 15.2001619 fraction B 0.221573293 lossA 2.05529475 fraction A 0.0297143385\n",
      "step 2635 loss 0.905245364 fisher_loss 0.241345227 triplet loss 0.663900137 l2_loss 15.2052755 fraction B 0.206539318 lossA 2.00102496 fraction A 0.027998725\n",
      "step 2636 loss 1.00983906 fisher_loss 0.24152939 triplet loss 0.768309712 l2_loss 15.2117367 fraction B 0.247526795 lossA 2.18324113 fraction A 0.0365806744\n",
      "step 2637 loss 1.11511278 fisher_loss 0.242620796 triplet loss 0.872491956 l2_loss 15.2212839 fraction B 0.219718024 lossA 2.31859207 fraction A 0.0480430536\n",
      "step 2638 loss 1.4411937 fisher_loss 0.244022965 triplet loss 1.19717073 l2_loss 15.2304134 fraction B 0.240846977 lossA 2.47593808 fraction A 0.0612950511\n",
      "step 2639 loss 1.14519191 fisher_loss 0.245731935 triplet loss 0.899459958 l2_loss 15.2384892 fraction B 0.247956634 lossA 2.61386871 fraction A 0.0710409135\n",
      "step 2640 loss 1.04302514 fisher_loss 0.246941224 triplet loss 0.796083927 l2_loss 15.2451172 fraction B 0.290845513 lossA 2.63167477 fraction A 0.0719519109\n",
      "step 2641 loss 1.05389774 fisher_loss 0.246889487 triplet loss 0.807008266 l2_loss 15.2492571 fraction B 0.213913932 lossA 2.57661963 fraction A 0.0675999\n",
      "step 2642 loss 1.08334792 fisher_loss 0.245950192 triplet loss 0.837397754 l2_loss 15.2522097 fraction B 0.244653836 lossA 2.45032716 fraction A 0.056872081\n",
      "step 2643 loss 0.933718324 fisher_loss 0.24409005 triplet loss 0.689628303 l2_loss 15.2535267 fraction B 0.224709183 lossA 2.32282066 fraction A 0.0454497635\n",
      "step 2644 loss 1.35408592 fisher_loss 0.242207825 triplet loss 1.11187816 l2_loss 15.2542887 fraction B 0.365965515 lossA 2.21182323 fraction A 0.0360588506\n",
      "step 2645 loss 0.858567417 fisher_loss 0.240700126 triplet loss 0.617867291 l2_loss 15.2539034 fraction B 0.210481822 lossA 2.22372317 fraction A 0.0365799516\n",
      "step 2646 loss 0.933173895 fisher_loss 0.240412533 triplet loss 0.692761362 l2_loss 15.2580328 fraction B 0.26426369 lossA 2.32173872 fraction A 0.04422272\n",
      "step 2647 loss 1.12346268 fisher_loss 0.241036251 triplet loss 0.882426441 l2_loss 15.264081 fraction B 0.330045551 lossA 2.45843124 fraction A 0.0548741817\n",
      "step 2648 loss 0.969468653 fisher_loss 0.242211506 triplet loss 0.727257133 l2_loss 15.2701197 fraction B 0.225945294 lossA 2.55317855 fraction A 0.0627754405\n",
      "step 2649 loss 1.15726352 fisher_loss 0.243492559 triplet loss 0.913770914 l2_loss 15.2763405 fraction B 0.247026682 lossA 2.59456468 fraction A 0.0660075843\n",
      "step 2650 loss 1.04475582 fisher_loss 0.244112909 triplet loss 0.800642908 l2_loss 15.2805243 fraction B 0.308133721 lossA 2.57173896 fraction A 0.064404048\n",
      "step 2651 loss 1.21467102 fisher_loss 0.243996829 triplet loss 0.970674217 l2_loss 15.2839422 fraction B 0.239098027 lossA 2.51558042 fraction A 0.0603212416\n",
      "step 2652 loss 1.03646493 fisher_loss 0.24339658 triplet loss 0.79306829 l2_loss 15.2872391 fraction B 0.220123395 lossA 2.42308545 fraction A 0.0529213436\n",
      "step 2653 loss 1.21357226 fisher_loss 0.242533147 triplet loss 0.971039057 l2_loss 15.2905598 fraction B 0.210739911 lossA 2.34215212 fraction A 0.0466781855\n",
      "step 2654 loss 1.0937475 fisher_loss 0.241842896 triplet loss 0.851904571 l2_loss 15.2939 fraction B 0.223304391 lossA 2.31036472 fraction A 0.0446459092\n",
      "step 2655 loss 1.01304066 fisher_loss 0.2416628 triplet loss 0.771377861 l2_loss 15.2986879 fraction B 0.166860819 lossA 2.33961034 fraction A 0.0477709509\n",
      "step 2656 loss 1.0971452 fisher_loss 0.241854087 triplet loss 0.855291069 l2_loss 15.3059692 fraction B 0.222517401 lossA 2.44201541 fraction A 0.0568467975\n",
      "step 2657 loss 0.91600287 fisher_loss 0.242792979 triplet loss 0.673209906 l2_loss 15.3147421 fraction B 0.161165178 lossA 2.54681826 fraction A 0.0655074269\n",
      "step 2658 loss 0.930628181 fisher_loss 0.243991286 triplet loss 0.686636865 l2_loss 15.3232393 fraction B 0.22494638 lossA 2.61187 fraction A 0.0704780817\n",
      "step 2659 loss 1.03198326 fisher_loss 0.244700879 triplet loss 0.787282407 l2_loss 15.330822 fraction B 0.209199741 lossA 2.57873774 fraction A 0.0682381466\n",
      "step 2660 loss 1.14217424 fisher_loss 0.244363412 triplet loss 0.897810817 l2_loss 15.3365593 fraction B 0.311558753 lossA 2.51803875 fraction A 0.0635757446\n",
      "step 2661 loss 1.19201934 fisher_loss 0.243638813 triplet loss 0.94838053 l2_loss 15.340683 fraction B 0.179204449 lossA 2.39236283 fraction A 0.0538763218\n",
      "step 2662 loss 1.07080328 fisher_loss 0.24243556 triplet loss 0.82836771 l2_loss 15.3427801 fraction B 0.263897181 lossA 2.31369686 fraction A 0.0469716229\n",
      "step 2663 loss 1.0152036 fisher_loss 0.241655096 triplet loss 0.773548484 l2_loss 15.3445635 fraction B 0.245673418 lossA 2.23758698 fraction A 0.040427018\n",
      "step 2664 loss 1.10067058 fisher_loss 0.241116956 triplet loss 0.859553576 l2_loss 15.3473539 fraction B 0.336593747 lossA 2.24348259 fraction A 0.0403242894\n",
      "step 2665 loss 0.990171552 fisher_loss 0.241295904 triplet loss 0.748875618 l2_loss 15.351655 fraction B 0.23639442 lossA 2.26636696 fraction A 0.0412566625\n",
      "step 2666 loss 0.915251732 fisher_loss 0.241657645 triplet loss 0.673594058 l2_loss 15.3562565 fraction B 0.15497756 lossA 2.36901879 fraction A 0.0491886\n",
      "step 2667 loss 1.08276141 fisher_loss 0.242864504 triplet loss 0.839896917 l2_loss 15.3645372 fraction B 0.298871934 lossA 2.4615705 fraction A 0.0564291514\n",
      "step 2668 loss 0.859189808 fisher_loss 0.244069159 triplet loss 0.615120649 l2_loss 15.3723726 fraction B 0.139682278 lossA 2.50281644 fraction A 0.0602210127\n",
      "step 2669 loss 0.906431198 fisher_loss 0.244781524 triplet loss 0.661649644 l2_loss 15.3815241 fraction B 0.123797834 lossA 2.52619195 fraction A 0.0628796145\n",
      "step 2670 loss 0.982998133 fisher_loss 0.245413974 triplet loss 0.737584174 l2_loss 15.3915854 fraction B 0.224568367 lossA 2.53414 fraction A 0.0632002205\n",
      "step 2671 loss 1.6140697 fisher_loss 0.245690867 triplet loss 1.36837888 l2_loss 15.4004011 fraction B 0.151861444 lossA 2.54469872 fraction A 0.063644819\n",
      "step 2672 loss 0.993214071 fisher_loss 0.245682895 triplet loss 0.747531176 l2_loss 15.407567 fraction B 0.189643487 lossA 2.51605701 fraction A 0.0610283799\n",
      "step 2673 loss 0.867187798 fisher_loss 0.245405182 triplet loss 0.621782601 l2_loss 15.4139385 fraction B 0.157698691 lossA 2.46142912 fraction A 0.0566842295\n",
      "step 2674 loss 1.27401793 fisher_loss 0.245042726 triplet loss 1.02897525 l2_loss 15.4205208 fraction B 0.228053153 lossA 2.39704728 fraction A 0.0515062697\n",
      "step 2675 loss 0.655505836 fisher_loss 0.244816333 triplet loss 0.410689503 l2_loss 15.4258051 fraction B 0.0911980867 lossA 2.3939662 fraction A 0.0514682\n",
      "step 2676 loss 1.08543921 fisher_loss 0.245791137 triplet loss 0.839648 l2_loss 15.4349804 fraction B 0.277962685 lossA 2.39276743 fraction A 0.0514236838\n",
      "step 2677 loss 1.1050334 fisher_loss 0.246794105 triplet loss 0.858239353 l2_loss 15.4436493 fraction B 0.166486934 lossA 2.4310348 fraction A 0.0541298799\n",
      "step 2678 loss 1.11519146 fisher_loss 0.248195291 triplet loss 0.866996229 l2_loss 15.4520607 fraction B 0.201634824 lossA 2.47563124 fraction A 0.0575239435\n",
      "step 2679 loss 0.906322 fisher_loss 0.24950406 triplet loss 0.656817913 l2_loss 15.4599676 fraction B 0.145640925 lossA 2.49362922 fraction A 0.0596118197\n",
      "step 2680 loss 0.970846653 fisher_loss 0.250448793 triplet loss 0.72039789 l2_loss 15.4684553 fraction B 0.201817796 lossA 2.49531198 fraction A 0.0598002523\n",
      "step 2681 loss 1.03169 fisher_loss 0.250890285 triplet loss 0.780799687 l2_loss 15.4760675 fraction B 0.186780497 lossA 2.46262383 fraction A 0.0573403314\n",
      "step 2682 loss 1.42840922 fisher_loss 0.250636101 triplet loss 1.17777312 l2_loss 15.483511 fraction B 0.216033757 lossA 2.42861056 fraction A 0.0544206761\n",
      "step 2683 loss 1.04968274 fisher_loss 0.249943405 triplet loss 0.799739361 l2_loss 15.4899025 fraction B 0.23825936 lossA 2.3574729 fraction A 0.0486623161\n",
      "step 2684 loss 1.10616827 fisher_loss 0.248493314 triplet loss 0.857674956 l2_loss 15.4955807 fraction B 0.195244297 lossA 2.26246619 fraction A 0.041302558\n",
      "step 2685 loss 1.0834341 fisher_loss 0.246983752 triplet loss 0.836450398 l2_loss 15.5006742 fraction B 0.172118604 lossA 2.22862577 fraction A 0.0387749374\n",
      "step 2686 loss 0.992323637 fisher_loss 0.246115059 triplet loss 0.746208608 l2_loss 15.5067701 fraction B 0.209558517 lossA 2.29091072 fraction A 0.043592155\n",
      "step 2687 loss 0.91897887 fisher_loss 0.246286705 triplet loss 0.67269218 l2_loss 15.5144691 fraction B 0.172455668 lossA 2.40635395 fraction A 0.0530312099\n",
      "step 2688 loss 1.28223574 fisher_loss 0.247152835 triplet loss 1.03508294 l2_loss 15.5235147 fraction B 0.389267355 lossA 2.48463511 fraction A 0.0591179952\n",
      "step 2689 loss 1.1409936 fisher_loss 0.247514173 triplet loss 0.893479407 l2_loss 15.5302353 fraction B 0.13938953 lossA 2.49132609 fraction A 0.0597370416\n",
      "step 2690 loss 1.1054523 fisher_loss 0.247455671 triplet loss 0.857996643 l2_loss 15.5351295 fraction B 0.213244513 lossA 2.49783659 fraction A 0.0602477081\n",
      "step 2691 loss 1.0035162 fisher_loss 0.24728471 triplet loss 0.756231427 l2_loss 15.5395603 fraction B 0.191872299 lossA 2.48046637 fraction A 0.0585491918\n",
      "step 2692 loss 1.21538043 fisher_loss 0.246851593 triplet loss 0.968528867 l2_loss 15.5438786 fraction B 0.254092366 lossA 2.43772435 fraction A 0.0547669269\n",
      "step 2693 loss 1.2096374 fisher_loss 0.246173903 triplet loss 0.963463545 l2_loss 15.5482035 fraction B 0.244103566 lossA 2.38141537 fraction A 0.0501101762\n",
      "step 2694 loss 1.15271926 fisher_loss 0.245343044 triplet loss 0.90737623 l2_loss 15.5513639 fraction B 0.286174238 lossA 2.34670568 fraction A 0.0474059\n",
      "step 2695 loss 1.16704369 fisher_loss 0.244709432 triplet loss 0.922334194 l2_loss 15.5534964 fraction B 0.222805187 lossA 2.30093932 fraction A 0.0436058119\n",
      "step 2696 loss 0.90422219 fisher_loss 0.243974105 triplet loss 0.660248101 l2_loss 15.5546446 fraction B 0.175127834 lossA 2.36146235 fraction A 0.0484822914\n",
      "step 2697 loss 0.942250431 fisher_loss 0.244337857 triplet loss 0.697912574 l2_loss 15.5603695 fraction B 0.190888315 lossA 2.44842124 fraction A 0.0560121536\n",
      "step 2698 loss 1.03718519 fisher_loss 0.245136738 triplet loss 0.792048395 l2_loss 15.5689392 fraction B 0.145272538 lossA 2.52327108 fraction A 0.0621803366\n",
      "step 2699 loss 1.01008177 fisher_loss 0.245930225 triplet loss 0.764151514 l2_loss 15.5777683 fraction B 0.189187 lossA 2.5648334 fraction A 0.0655142\n",
      "step 2700 loss 0.888662696 fisher_loss 0.246392444 triplet loss 0.642270267 l2_loss 15.586298 fraction B 0.144603923 lossA 2.51384401 fraction A 0.0624579452\n",
      "step 2701 loss 1.25471342 fisher_loss 0.246368691 triplet loss 1.00834477 l2_loss 15.5942802 fraction B 0.198295221 lossA 2.43811965 fraction A 0.0573151559\n",
      "step 2702 loss 1.04005456 fisher_loss 0.246130168 triplet loss 0.793924451 l2_loss 15.6011362 fraction B 0.222523749 lossA 2.35389113 fraction A 0.0515475273\n",
      "step 2703 loss 0.881476521 fisher_loss 0.246088713 triplet loss 0.635387778 l2_loss 15.6080017 fraction B 0.152677119 lossA 2.31604 fraction A 0.0491352789\n",
      "step 2704 loss 0.699574709 fisher_loss 0.246657148 triplet loss 0.452917576 l2_loss 15.6154661 fraction B 0.123086222 lossA 2.22839856 fraction A 0.0422768369\n",
      "step 2705 loss 0.808891594 fisher_loss 0.24675715 triplet loss 0.562134445 l2_loss 15.6240578 fraction B 0.147007287 lossA 2.28295946 fraction A 0.0470678322\n",
      "step 2706 loss 1.01482046 fisher_loss 0.248344094 triplet loss 0.766476333 l2_loss 15.6375103 fraction B 0.173481107 lossA 2.40695095 fraction A 0.0572502837\n",
      "step 2707 loss 0.973037958 fisher_loss 0.250652224 triplet loss 0.722385705 l2_loss 15.6525421 fraction B 0.201624945 lossA 2.48470926 fraction A 0.0634042621\n",
      "step 2708 loss 1.02974677 fisher_loss 0.252225727 triplet loss 0.777521074 l2_loss 15.6671495 fraction B 0.104147047 lossA 2.52415085 fraction A 0.0665165856\n",
      "step 2709 loss 1.22260094 fisher_loss 0.253379971 triplet loss 0.969220936 l2_loss 15.6805677 fraction B 0.221927255 lossA 2.52331805 fraction A 0.0666067451\n",
      "step 2710 loss 1.15823948 fisher_loss 0.253836483 triplet loss 0.904403 l2_loss 15.6911659 fraction B 0.186384067 lossA 2.50306249 fraction A 0.0651244074\n",
      "step 2711 loss 1.07485008 fisher_loss 0.254002929 triplet loss 0.820847094 l2_loss 15.7008476 fraction B 0.327546388 lossA 2.41757393 fraction A 0.0587018356\n",
      "step 2712 loss 1.23570096 fisher_loss 0.253457844 triplet loss 0.982243121 l2_loss 15.7090931 fraction B 0.268666774 lossA 2.34896016 fraction A 0.0530263186\n",
      "step 2713 loss 1.05435443 fisher_loss 0.252859771 triplet loss 0.801494658 l2_loss 15.7158718 fraction B 0.254181504 lossA 2.29268789 fraction A 0.0480635203\n",
      "step 2714 loss 1.05443048 fisher_loss 0.252514809 triplet loss 0.801915705 l2_loss 15.7219582 fraction B 0.249021873 lossA 2.28136826 fraction A 0.046209462\n",
      "step 2715 loss 1.20190787 fisher_loss 0.252334297 triplet loss 0.949573576 l2_loss 15.7274666 fraction B 0.281258076 lossA 2.30181122 fraction A 0.0468639694\n",
      "step 2716 loss 1.15374851 fisher_loss 0.252333522 triplet loss 0.901414931 l2_loss 15.7327538 fraction B 0.251391947 lossA 2.37769485 fraction A 0.0515948348\n",
      "step 2717 loss 1.03381991 fisher_loss 0.252401978 triplet loss 0.781417906 l2_loss 15.7368822 fraction B 0.215084493 lossA 2.48638463 fraction A 0.0581550896\n",
      "step 2718 loss 1.25139475 fisher_loss 0.25256142 triplet loss 0.998833299 l2_loss 15.7406301 fraction B 0.267357022 lossA 2.59471917 fraction A 0.0641689226\n",
      "step 2719 loss 1.52094078 fisher_loss 0.252467245 triplet loss 1.26847351 l2_loss 15.7435188 fraction B 0.165256634 lossA 2.64186764 fraction A 0.0660763\n",
      "step 2720 loss 1.45653188 fisher_loss 0.251954108 triplet loss 1.2045778 l2_loss 15.7447643 fraction B 0.257558107 lossA 2.61062837 fraction A 0.0632723048\n",
      "step 2721 loss 1.4324733 fisher_loss 0.250932187 triplet loss 1.18154109 l2_loss 15.7442503 fraction B 0.242090404 lossA 2.53517056 fraction A 0.0576873831\n",
      "step 2722 loss 1.11757827 fisher_loss 0.249518633 triplet loss 0.868059635 l2_loss 15.7431498 fraction B 0.274002075 lossA 2.4417336 fraction A 0.0506472848\n",
      "step 2723 loss 0.839630485 fisher_loss 0.248318136 triplet loss 0.591312349 l2_loss 15.7419519 fraction B 0.136957452 lossA 2.35726833 fraction A 0.0442456864\n",
      "step 2724 loss 1.00363815 fisher_loss 0.247662067 triplet loss 0.755976081 l2_loss 15.7426424 fraction B 0.174156621 lossA 2.33546662 fraction A 0.0423653647\n",
      "step 2725 loss 0.978041887 fisher_loss 0.24751693 triplet loss 0.730524957 l2_loss 15.7455206 fraction B 0.246147037 lossA 2.40341091 fraction A 0.0480252\n",
      "step 2726 loss 0.927439094 fisher_loss 0.247798741 triplet loss 0.679640353 l2_loss 15.751544 fraction B 0.168989763 lossA 2.43956852 fraction A 0.0514709167\n",
      "step 2727 loss 1.18730807 fisher_loss 0.247830436 triplet loss 0.939477682 l2_loss 15.7578697 fraction B 0.280445814 lossA 2.45014691 fraction A 0.0531249531\n",
      "step 2728 loss 1.27160537 fisher_loss 0.247663185 triplet loss 1.02394223 l2_loss 15.7626553 fraction B 0.190482214 lossA 2.39994693 fraction A 0.0498714671\n",
      "step 2729 loss 0.98235178 fisher_loss 0.247091085 triplet loss 0.735260725 l2_loss 15.7660961 fraction B 0.24249284 lossA 2.36764145 fraction A 0.0482287332\n",
      "step 2730 loss 0.9541291 fisher_loss 0.246805757 triplet loss 0.707323313 l2_loss 15.7706852 fraction B 0.164229855 lossA 2.34625149 fraction A 0.0476936512\n",
      "step 2731 loss 0.892022192 fisher_loss 0.246551648 triplet loss 0.64547056 l2_loss 15.7753181 fraction B 0.172470137 lossA 2.30808067 fraction A 0.0451190881\n",
      "step 2732 loss 1.1379267 fisher_loss 0.246160045 triplet loss 0.891766667 l2_loss 15.7794313 fraction B 0.289514452 lossA 2.23877525 fraction A 0.0396252647\n",
      "step 2733 loss 1.05389583 fisher_loss 0.245372146 triplet loss 0.808523655 l2_loss 15.7817812 fraction B 0.14826031 lossA 2.19555211 fraction A 0.0366531946\n",
      "step 2734 loss 1.0074439 fisher_loss 0.244768977 triplet loss 0.762674868 l2_loss 15.7848721 fraction B 0.283377588 lossA 2.1958797 fraction A 0.0367266908\n",
      "step 2735 loss 1.23069024 fisher_loss 0.244546011 triplet loss 0.986144185 l2_loss 15.7893038 fraction B 0.22626023 lossA 2.21857715 fraction A 0.0382072926\n",
      "step 2736 loss 1.07557428 fisher_loss 0.244461149 triplet loss 0.83111316 l2_loss 15.7932272 fraction B 0.251573265 lossA 2.27987695 fraction A 0.0426021554\n",
      "step 2737 loss 1.12370813 fisher_loss 0.244956806 triplet loss 0.878751278 l2_loss 15.7980585 fraction B 0.33794266 lossA 2.31813478 fraction A 0.0458968617\n",
      "step 2738 loss 1.22331178 fisher_loss 0.245088443 triplet loss 0.978223324 l2_loss 15.8022213 fraction B 0.253207535 lossA 2.35322 fraction A 0.0496262051\n",
      "step 2739 loss 1.03299737 fisher_loss 0.245121658 triplet loss 0.787875652 l2_loss 15.806407 fraction B 0.254732847 lossA 2.36259532 fraction A 0.0513393208\n",
      "step 2740 loss 0.990170956 fisher_loss 0.244860038 triplet loss 0.745310903 l2_loss 15.8107176 fraction B 0.238940716 lossA 2.35227537 fraction A 0.0511215106\n",
      "step 2741 loss 1.17959774 fisher_loss 0.244243622 triplet loss 0.935354114 l2_loss 15.8148804 fraction B 0.199937552 lossA 2.37564826 fraction A 0.0531875081\n",
      "step 2742 loss 0.97565645 fisher_loss 0.243897259 triplet loss 0.731759191 l2_loss 15.8198318 fraction B 0.227665707 lossA 2.36909032 fraction A 0.0525823385\n",
      "step 2743 loss 1.03845072 fisher_loss 0.243216336 triplet loss 0.795234442 l2_loss 15.823575 fraction B 0.252306044 lossA 2.34140897 fraction A 0.0501830205\n",
      "step 2744 loss 1.05684125 fisher_loss 0.242378399 triplet loss 0.8144629 l2_loss 15.8270082 fraction B 0.265555114 lossA 2.31706834 fraction A 0.0484156795\n",
      "step 2745 loss 1.12821555 fisher_loss 0.241726279 triplet loss 0.886489213 l2_loss 15.8315516 fraction B 0.267750323 lossA 2.29234838 fraction A 0.0470867753\n",
      "step 2746 loss 0.937642157 fisher_loss 0.241266191 triplet loss 0.696375966 l2_loss 15.8364792 fraction B 0.218982533 lossA 2.29048634 fraction A 0.0472526029\n",
      "step 2747 loss 1.14630616 fisher_loss 0.241240248 triplet loss 0.905065894 l2_loss 15.8423738 fraction B 0.302048147 lossA 2.26321697 fraction A 0.045388367\n",
      "step 2748 loss 1.26455128 fisher_loss 0.241012305 triplet loss 1.02353895 l2_loss 15.8471718 fraction B 0.193431109 lossA 2.24559808 fraction A 0.0451951176\n",
      "step 2749 loss 0.915767908 fisher_loss 0.241160542 triplet loss 0.674607396 l2_loss 15.852973 fraction B 0.248912305 lossA 2.29607964 fraction A 0.0502763279\n",
      "step 2750 loss 1.45308518 fisher_loss 0.24212414 triplet loss 1.21096098 l2_loss 15.8610201 fraction B 0.158148527 lossA 2.24277854 fraction A 0.0464353114\n",
      "step 2751 loss 1.03088331 fisher_loss 0.242216513 triplet loss 0.788666844 l2_loss 15.8663797 fraction B 0.191240534 lossA 2.17850876 fraction A 0.0414386094\n",
      "step 2752 loss 0.996263862 fisher_loss 0.242303506 triplet loss 0.753960371 l2_loss 15.8715811 fraction B 0.220914915 lossA 2.13527703 fraction A 0.0387804806\n",
      "step 2753 loss 1.27895415 fisher_loss 0.242644474 triplet loss 1.03630972 l2_loss 15.8769293 fraction B 0.250874162 lossA 2.15220594 fraction A 0.0404169485\n",
      "step 2754 loss 0.986080647 fisher_loss 0.243290469 triplet loss 0.742790163 l2_loss 15.882432 fraction B 0.192762345 lossA 2.16150761 fraction A 0.0409908965\n",
      "step 2755 loss 0.880714536 fisher_loss 0.243776783 triplet loss 0.636937737 l2_loss 15.8880959 fraction B 0.165980935 lossA 2.18322325 fraction A 0.0422034152\n",
      "step 2756 loss 0.998886347 fisher_loss 0.244343892 triplet loss 0.75454247 l2_loss 15.8947306 fraction B 0.260549784 lossA 2.24114442 fraction A 0.0464957319\n",
      "step 2757 loss 1.05368543 fisher_loss 0.245173022 triplet loss 0.80851239 l2_loss 15.9019 fraction B 0.207020715 lossA 2.29316664 fraction A 0.0496195033\n",
      "step 2758 loss 0.976722658 fisher_loss 0.245710313 triplet loss 0.731012344 l2_loss 15.9080153 fraction B 0.162768036 lossA 2.32064199 fraction A 0.0504168\n",
      "step 2759 loss 0.978630602 fisher_loss 0.245805487 triplet loss 0.7328251 l2_loss 15.9130392 fraction B 0.160636 lossA 2.37813115 fraction A 0.0547252707\n",
      "step 2760 loss 1.04224038 fisher_loss 0.246234238 triplet loss 0.796006083 l2_loss 15.9194317 fraction B 0.191929534 lossA 2.42584848 fraction A 0.0578688569\n",
      "step 2761 loss 0.935654759 fisher_loss 0.246367738 triplet loss 0.689287 l2_loss 15.9253826 fraction B 0.14746204 lossA 2.4359796 fraction A 0.0577871762\n",
      "step 2762 loss 1.24822497 fisher_loss 0.245991185 triplet loss 1.00223374 l2_loss 15.9308519 fraction B 0.236521959 lossA 2.39436817 fraction A 0.0533605814\n",
      "step 2763 loss 1.02501714 fisher_loss 0.24521327 triplet loss 0.779803872 l2_loss 15.9348841 fraction B 0.233089432 lossA 2.35380507 fraction A 0.0483669937\n",
      "step 2764 loss 1.18098307 fisher_loss 0.244494841 triplet loss 0.936488211 l2_loss 15.9383574 fraction B 0.169676632 lossA 2.28324986 fraction A 0.0422274657\n",
      "step 2765 loss 1.0849365 fisher_loss 0.243870988 triplet loss 0.841065466 l2_loss 15.942009 fraction B 0.231929243 lossA 2.25034332 fraction A 0.0391206481\n",
      "step 2766 loss 0.935870767 fisher_loss 0.243673369 triplet loss 0.692197382 l2_loss 15.9450779 fraction B 0.190248489 lossA 2.31475973 fraction A 0.0440763794\n",
      "step 2767 loss 1.10518873 fisher_loss 0.244504452 triplet loss 0.860684276 l2_loss 15.9510794 fraction B 0.178023845 lossA 2.40791702 fraction A 0.0510308817\n",
      "step 2768 loss 0.982236683 fisher_loss 0.245545968 triplet loss 0.7366907 l2_loss 15.9585428 fraction B 0.20792079 lossA 2.49947572 fraction A 0.0584692135\n",
      "step 2769 loss 1.18063521 fisher_loss 0.246736392 triplet loss 0.933898807 l2_loss 15.967145 fraction B 0.179827601 lossA 2.52653193 fraction A 0.0612005852\n",
      "step 2770 loss 0.785797536 fisher_loss 0.247381806 triplet loss 0.53841573 l2_loss 15.9740438 fraction B 0.140672356 lossA 2.54123211 fraction A 0.0626801699\n",
      "step 2771 loss 1.1783998 fisher_loss 0.247996509 triplet loss 0.930403233 l2_loss 15.9820204 fraction B 0.190363407 lossA 2.51170492 fraction A 0.0608317293\n",
      "step 2772 loss 1.48669255 fisher_loss 0.247907415 triplet loss 1.23878515 l2_loss 15.9883518 fraction B 0.302086025 lossA 2.47323704 fraction A 0.057982631\n",
      "step 2773 loss 1.09282219 fisher_loss 0.247334599 triplet loss 0.845487595 l2_loss 15.9927673 fraction B 0.213705063 lossA 2.44216943 fraction A 0.0557997078\n",
      "step 2774 loss 1.20280099 fisher_loss 0.246876359 triplet loss 0.95592463 l2_loss 15.9969101 fraction B 0.279807985 lossA 2.35611939 fraction A 0.0487361029\n",
      "step 2775 loss 1.18105137 fisher_loss 0.245732516 triplet loss 0.935318828 l2_loss 15.9998198 fraction B 0.296746939 lossA 2.24103761 fraction A 0.0400441065\n",
      "step 2776 loss 1.09240866 fisher_loss 0.244316176 triplet loss 0.848092496 l2_loss 16.0018044 fraction B 0.168229774 lossA 2.15366936 fraction A 0.0341437235\n",
      "step 2777 loss 1.01384377 fisher_loss 0.243116915 triplet loss 0.7707268 l2_loss 16.0040169 fraction B 0.205120057 lossA 2.15253353 fraction A 0.0335736871\n",
      "step 2778 loss 1.14689469 fisher_loss 0.242536 triplet loss 0.904358745 l2_loss 16.0081825 fraction B 0.285905242 lossA 2.20712209 fraction A 0.0363630503\n",
      "step 2779 loss 1.1741401 fisher_loss 0.242361322 triplet loss 0.931778729 l2_loss 16.0136509 fraction B 0.213944703 lossA 2.32093501 fraction A 0.0437478423\n",
      "step 2780 loss 0.95652 fisher_loss 0.242821798 triplet loss 0.713698208 l2_loss 16.020483 fraction B 0.181325197 lossA 2.46397138 fraction A 0.0554761328\n",
      "step 2781 loss 1.43399286 fisher_loss 0.243826389 triplet loss 1.19016647 l2_loss 16.029459 fraction B 0.237919703 lossA 2.60348248 fraction A 0.066243723\n",
      "step 2782 loss 1.07803726 fisher_loss 0.244842499 triplet loss 0.833194733 l2_loss 16.0369034 fraction B 0.204975024 lossA 2.64497948 fraction A 0.0695065558\n",
      "step 2783 loss 1.05933988 fisher_loss 0.245011121 triplet loss 0.81432879 l2_loss 16.0434456 fraction B 0.199234441 lossA 2.63105726 fraction A 0.0682112277\n",
      "step 2784 loss 1.16823316 fisher_loss 0.244653374 triplet loss 0.923579752 l2_loss 16.048811 fraction B 0.193382442 lossA 2.51597023 fraction A 0.0601634569\n",
      "step 2785 loss 1.21221066 fisher_loss 0.243687481 triplet loss 0.968523204 l2_loss 16.0534096 fraction B 0.388119519 lossA 2.39145279 fraction A 0.0504032895\n",
      "step 2786 loss 1.06768727 fisher_loss 0.242815971 triplet loss 0.824871302 l2_loss 16.057478 fraction B 0.318844914 lossA 2.25990868 fraction A 0.0391761772\n",
      "step 2787 loss 1.02097082 fisher_loss 0.242088422 triplet loss 0.778882384 l2_loss 16.0601788 fraction B 0.191226617 lossA 2.19153929 fraction A 0.0348271\n",
      "step 2788 loss 0.927924633 fisher_loss 0.241987333 triplet loss 0.685937285 l2_loss 16.0638027 fraction B 0.237633392 lossA 2.27745199 fraction A 0.0405897312\n",
      "step 2789 loss 1.01224422 fisher_loss 0.243041009 triplet loss 0.769203186 l2_loss 16.0710583 fraction B 0.235116154 lossA 2.42298317 fraction A 0.0539511181\n",
      "step 2790 loss 0.843998194 fisher_loss 0.244721308 triplet loss 0.5992769 l2_loss 16.0803661 fraction B 0.125957832 lossA 2.50433373 fraction A 0.0603841618\n",
      "step 2791 loss 0.96240437 fisher_loss 0.245650157 triplet loss 0.716754198 l2_loss 16.0881786 fraction B 0.241157293 lossA 2.48960733 fraction A 0.0584812723\n",
      "step 2792 loss 1.31699693 fisher_loss 0.245411798 triplet loss 1.07158518 l2_loss 16.0947266 fraction B 0.280252069 lossA 2.4004426 fraction A 0.0499664433\n",
      "step 2793 loss 1.25243664 fisher_loss 0.244593486 triplet loss 1.00784314 l2_loss 16.0986691 fraction B 0.270688474 lossA 2.2642808 fraction A 0.0378364809\n",
      "step 2794 loss 0.754695892 fisher_loss 0.243450046 triplet loss 0.511245847 l2_loss 16.1001854 fraction B 0.150439098 lossA 2.30440593 fraction A 0.0405880623\n",
      "step 2795 loss 0.979498208 fisher_loss 0.24411267 triplet loss 0.735385537 l2_loss 16.106926 fraction B 0.242282405 lossA 2.40544 fraction A 0.048385758\n",
      "step 2796 loss 1.09437084 fisher_loss 0.245424047 triplet loss 0.84894675 l2_loss 16.115139 fraction B 0.215858847 lossA 2.5126667 fraction A 0.056797497\n",
      "step 2797 loss 0.943343103 fisher_loss 0.246790826 triplet loss 0.696552277 l2_loss 16.1229343 fraction B 0.211132824 lossA 2.57555532 fraction A 0.0618184693\n",
      "step 2798 loss 1.16171861 fisher_loss 0.247736141 triplet loss 0.913982511 l2_loss 16.1308479 fraction B 0.226305887 lossA 2.57840443 fraction A 0.0623614118\n",
      "step 2799 loss 0.825637817 fisher_loss 0.247869462 triplet loss 0.577768385 l2_loss 16.1378307 fraction B 0.130531624 lossA 2.48844457 fraction A 0.0558029339\n",
      "step 2800 loss 1.08954704 fisher_loss 0.247310534 triplet loss 0.842236459 l2_loss 16.1439533 fraction B 0.246433035 lossA 2.36822248 fraction A 0.0464438237\n",
      "step 2801 loss 1.05137932 fisher_loss 0.246415645 triplet loss 0.804963708 l2_loss 16.1490307 fraction B 0.199604586 lossA 2.24684048 fraction A 0.0381447338\n",
      "step 2802 loss 1.00895071 fisher_loss 0.245708764 triplet loss 0.763241887 l2_loss 16.1536789 fraction B 0.144354314 lossA 2.16491818 fraction A 0.0341692232\n",
      "step 2803 loss 0.928243399 fisher_loss 0.245317787 triplet loss 0.682925642 l2_loss 16.1596107 fraction B 0.196300745 lossA 2.17555022 fraction A 0.0352141745\n",
      "step 2804 loss 1.23484612 fisher_loss 0.245577604 triplet loss 0.989268541 l2_loss 16.166851 fraction B 0.216756642 lossA 2.22039557 fraction A 0.0384133272\n",
      "step 2805 loss 1.0329268 fisher_loss 0.245738223 triplet loss 0.78718853 l2_loss 16.1740456 fraction B 0.26433593 lossA 2.26074553 fraction A 0.0424059704\n",
      "step 2806 loss 0.91178894 fisher_loss 0.246046424 triplet loss 0.665742517 l2_loss 16.1809654 fraction B 0.150257662 lossA 2.31617355 fraction A 0.0483525768\n",
      "step 2807 loss 1.26696408 fisher_loss 0.246774837 triplet loss 1.02018929 l2_loss 16.1902504 fraction B 0.343795061 lossA 2.37834907 fraction A 0.0548232533\n",
      "step 2808 loss 1.35913193 fisher_loss 0.247611925 triplet loss 1.11152 l2_loss 16.1984138 fraction B 0.168401331 lossA 2.38231921 fraction A 0.0563485548\n",
      "step 2809 loss 1.31123865 fisher_loss 0.24769412 triplet loss 1.06354451 l2_loss 16.2045383 fraction B 0.202248126 lossA 2.37968397 fraction A 0.0568817891\n",
      "step 2810 loss 0.961867392 fisher_loss 0.247440711 triplet loss 0.714426696 l2_loss 16.2094803 fraction B 0.204960227 lossA 2.31289291 fraction A 0.0522971563\n",
      "step 2811 loss 1.01387715 fisher_loss 0.246706441 triplet loss 0.767170668 l2_loss 16.2136326 fraction B 0.195339367 lossA 2.22590899 fraction A 0.0451105386\n",
      "step 2812 loss 0.978517652 fisher_loss 0.24588266 triplet loss 0.732634962 l2_loss 16.2175674 fraction B 0.19922322 lossA 2.1629653 fraction A 0.0394525193\n",
      "step 2813 loss 1.01258695 fisher_loss 0.24556604 triplet loss 0.767020881 l2_loss 16.2216358 fraction B 0.253304154 lossA 2.13210893 fraction A 0.0371339396\n",
      "step 2814 loss 0.896117389 fisher_loss 0.245534584 triplet loss 0.65058279 l2_loss 16.2272472 fraction B 0.171664074 lossA 2.11302757 fraction A 0.0360516086\n",
      "step 2815 loss 0.917259037 fisher_loss 0.245656148 triplet loss 0.671602905 l2_loss 16.2344418 fraction B 0.179004803 lossA 2.19298244 fraction A 0.0414180346\n",
      "step 2816 loss 1.08962727 fisher_loss 0.24634029 triplet loss 0.843287 l2_loss 16.2442551 fraction B 0.298017055 lossA 2.28098655 fraction A 0.0486070439\n",
      "step 2817 loss 0.992704511 fisher_loss 0.247170776 triplet loss 0.745533705 l2_loss 16.2543678 fraction B 0.249430478 lossA 2.31934094 fraction A 0.0516595654\n",
      "step 2818 loss 0.987799227 fisher_loss 0.247242942 triplet loss 0.7405563 l2_loss 16.2625618 fraction B 0.200424448 lossA 2.3787396 fraction A 0.0566487312\n",
      "step 2819 loss 1.12502265 fisher_loss 0.247577697 triplet loss 0.877444923 l2_loss 16.2715 fraction B 0.249563739 lossA 2.39941549 fraction A 0.0582879521\n",
      "step 2820 loss 1.15844262 fisher_loss 0.247409046 triplet loss 0.911033571 l2_loss 16.2788181 fraction B 0.19913438 lossA 2.31550813 fraction A 0.0513433032\n",
      "step 2821 loss 1.1149267 fisher_loss 0.246396586 triplet loss 0.868530154 l2_loss 16.2836342 fraction B 0.243312895 lossA 2.24488497 fraction A 0.0445891134\n",
      "step 2822 loss 1.17485368 fisher_loss 0.245515227 triplet loss 0.929338455 l2_loss 16.2879543 fraction B 0.143537506 lossA 2.19884682 fraction A 0.0392834321\n",
      "step 2823 loss 1.0319618 fisher_loss 0.244839936 triplet loss 0.787121832 l2_loss 16.2908497 fraction B 0.268243402 lossA 2.17876983 fraction A 0.0367029645\n",
      "step 2824 loss 0.922958672 fisher_loss 0.244464055 triplet loss 0.678494632 l2_loss 16.2940063 fraction B 0.174481869 lossA 2.25875521 fraction A 0.0415137\n",
      "step 2825 loss 1.05749106 fisher_loss 0.244942352 triplet loss 0.812548697 l2_loss 16.299902 fraction B 0.277921438 lossA 2.39008951 fraction A 0.0512513332\n",
      "step 2826 loss 1.44375455 fisher_loss 0.246022567 triplet loss 1.19773197 l2_loss 16.3066654 fraction B 0.200273827 lossA 2.51563 fraction A 0.0596113503\n",
      "step 2827 loss 1.14921272 fisher_loss 0.247048244 triplet loss 0.902164519 l2_loss 16.3129215 fraction B 0.298887849 lossA 2.60563588 fraction A 0.0653623566\n",
      "step 2828 loss 1.01189613 fisher_loss 0.247853756 triplet loss 0.764042318 l2_loss 16.3182144 fraction B 0.249476328 lossA 2.63012576 fraction A 0.0661426261\n",
      "step 2829 loss 1.06181085 fisher_loss 0.247994289 triplet loss 0.813816607 l2_loss 16.3223114 fraction B 0.195847392 lossA 2.58976412 fraction A 0.0622391962\n",
      "step 2830 loss 1.10121846 fisher_loss 0.247482121 triplet loss 0.853736341 l2_loss 16.3250885 fraction B 0.267713517 lossA 2.49929118 fraction A 0.0549175777\n",
      "step 2831 loss 1.0301491 fisher_loss 0.246563777 triplet loss 0.78358537 l2_loss 16.3273163 fraction B 0.152450383 lossA 2.41427898 fraction A 0.0479546338\n",
      "step 2832 loss 0.854876518 fisher_loss 0.245748401 triplet loss 0.609128118 l2_loss 16.3303185 fraction B 0.132316 lossA 2.30915618 fraction A 0.0405940749\n",
      "step 2833 loss 0.882918954 fisher_loss 0.244725287 triplet loss 0.638193667 l2_loss 16.3342819 fraction B 0.197575763 lossA 2.24279737 fraction A 0.0371695459\n",
      "step 2834 loss 0.875750542 fisher_loss 0.244303957 triplet loss 0.6314466 l2_loss 16.340744 fraction B 0.203081414 lossA 2.21299243 fraction A 0.036168281\n",
      "step 2835 loss 1.02118838 fisher_loss 0.244431406 triplet loss 0.776756942 l2_loss 16.348959 fraction B 0.2226208 lossA 2.26600313 fraction A 0.040898528\n",
      "step 2836 loss 1.09743941 fisher_loss 0.245173782 triplet loss 0.852265656 l2_loss 16.3590069 fraction B 0.230028465 lossA 2.30850506 fraction A 0.0453173704\n",
      "step 2837 loss 0.75144434 fisher_loss 0.245925337 triplet loss 0.505519032 l2_loss 16.3684349 fraction B 0.130303845 lossA 2.38673592 fraction A 0.051963836\n",
      "step 2838 loss 1.3356564 fisher_loss 0.247087941 triplet loss 1.08856845 l2_loss 16.3793449 fraction B 0.236657396 lossA 2.46360683 fraction A 0.0583482645\n",
      "step 2839 loss 1.10338748 fisher_loss 0.248096734 triplet loss 0.855290711 l2_loss 16.388876 fraction B 0.144228444 lossA 2.49104738 fraction A 0.0601571165\n",
      "step 2840 loss 1.0977006 fisher_loss 0.248544127 triplet loss 0.849156499 l2_loss 16.3964672 fraction B 0.254007667 lossA 2.48840809 fraction A 0.0593712665\n",
      "step 2841 loss 1.56401515 fisher_loss 0.248283386 triplet loss 1.31573176 l2_loss 16.4026394 fraction B 0.149092555 lossA 2.40477943 fraction A 0.0516443141\n",
      "step 2842 loss 1.14347517 fisher_loss 0.247062534 triplet loss 0.896412611 l2_loss 16.4058819 fraction B 0.233180106 lossA 2.30440664 fraction A 0.0431398451\n",
      "step 2843 loss 0.96035111 fisher_loss 0.245911121 triplet loss 0.71444 l2_loss 16.408844 fraction B 0.189300582 lossA 2.26056051 fraction A 0.0400968827\n",
      "step 2844 loss 1.20222807 fisher_loss 0.245535135 triplet loss 0.956692874 l2_loss 16.4140396 fraction B 0.292263299 lossA 2.25725842 fraction A 0.0398899801\n",
      "step 2845 loss 0.734220743 fisher_loss 0.24522835 triplet loss 0.488992393 l2_loss 16.4195766 fraction B 0.093365185 lossA 2.29781842 fraction A 0.042909719\n",
      "step 2846 loss 1.06012 fisher_loss 0.245753974 triplet loss 0.814366043 l2_loss 16.42803 fraction B 0.291125447 lossA 2.3489387 fraction A 0.0471255\n",
      "step 2847 loss 0.936148763 fisher_loss 0.246357471 triplet loss 0.689791322 l2_loss 16.4362392 fraction B 0.195760384 lossA 2.35490298 fraction A 0.0471379571\n",
      "step 2848 loss 0.967023075 fisher_loss 0.246613398 triplet loss 0.720409691 l2_loss 16.4434147 fraction B 0.16433683 lossA 2.42802405 fraction A 0.0534890257\n",
      "step 2849 loss 1.07618976 fisher_loss 0.247335464 triplet loss 0.828854322 l2_loss 16.4513798 fraction B 0.248083308 lossA 2.49525881 fraction A 0.0588817522\n",
      "step 2850 loss 1.22795379 fisher_loss 0.247822 triplet loss 0.980131805 l2_loss 16.4577847 fraction B 0.181324393 lossA 2.50489664 fraction A 0.0587549359\n",
      "step 2851 loss 0.872896433 fisher_loss 0.247634768 triplet loss 0.625261664 l2_loss 16.4627228 fraction B 0.163839549 lossA 2.44296 fraction A 0.051480405\n",
      "step 2852 loss 1.02426732 fisher_loss 0.246702194 triplet loss 0.777565122 l2_loss 16.4662514 fraction B 0.120506532 lossA 2.32656217 fraction A 0.04179167\n",
      "step 2853 loss 1.40514898 fisher_loss 0.245690346 triplet loss 1.15945864 l2_loss 16.4698639 fraction B 0.167745814 lossA 2.17710114 fraction A 0.0337148458\n",
      "step 2854 loss 1.1890974 fisher_loss 0.244927675 triplet loss 0.94416976 l2_loss 16.4741554 fraction B 0.226558819 lossA 2.14739585 fraction A 0.0326746032\n",
      "step 2855 loss 0.993431032 fisher_loss 0.24481903 triplet loss 0.748612 l2_loss 16.4808693 fraction B 0.239303395 lossA 2.18753314 fraction A 0.0344849378\n",
      "step 2856 loss 1.07220399 fisher_loss 0.24487628 triplet loss 0.827327669 l2_loss 16.4874268 fraction B 0.220803291 lossA 2.24651313 fraction A 0.0386767387\n",
      "step 2857 loss 1.03936434 fisher_loss 0.245035619 triplet loss 0.794328749 l2_loss 16.4949989 fraction B 0.194319487 lossA 2.31867313 fraction A 0.0453625396\n",
      "step 2858 loss 1.13356543 fisher_loss 0.24549523 triplet loss 0.888070226 l2_loss 16.5024338 fraction B 0.267086327 lossA 2.39874697 fraction A 0.0525075719\n",
      "step 2859 loss 1.04069805 fisher_loss 0.246032283 triplet loss 0.794665813 l2_loss 16.508812 fraction B 0.179182291 lossA 2.46219802 fraction A 0.0580062866\n",
      "step 2860 loss 1.01510274 fisher_loss 0.246604696 triplet loss 0.768498063 l2_loss 16.5152264 fraction B 0.166349262 lossA 2.48155546 fraction A 0.0595458597\n",
      "step 2861 loss 0.968821466 fisher_loss 0.246809676 triplet loss 0.722011805 l2_loss 16.5200386 fraction B 0.209011629 lossA 2.44513535 fraction A 0.0568978\n",
      "step 2862 loss 0.815878749 fisher_loss 0.246865034 triplet loss 0.569013715 l2_loss 16.5244541 fraction B 0.119902715 lossA 2.42663717 fraction A 0.0558997914\n",
      "step 2863 loss 1.33039963 fisher_loss 0.247513786 triplet loss 1.08288586 l2_loss 16.5316486 fraction B 0.183202699 lossA 2.37467408 fraction A 0.0524023473\n",
      "step 2864 loss 1.1603899 fisher_loss 0.247934595 triplet loss 0.91245532 l2_loss 16.5374928 fraction B 0.214801595 lossA 2.33983922 fraction A 0.0503397919\n",
      "step 2865 loss 1.20602846 fisher_loss 0.248317048 triplet loss 0.957711399 l2_loss 16.5436306 fraction B 0.246182978 lossA 2.30576324 fraction A 0.0485021397\n",
      "step 2866 loss 1.02042806 fisher_loss 0.248493388 triplet loss 0.771934688 l2_loss 16.5491886 fraction B 0.242109701 lossA 2.29413414 fraction A 0.049045302\n",
      "step 2867 loss 1.283077 fisher_loss 0.248614356 triplet loss 1.03446269 l2_loss 16.556282 fraction B 0.143566176 lossA 2.28655148 fraction A 0.0501345545\n",
      "step 2868 loss 1.11231804 fisher_loss 0.248603493 triplet loss 0.863714516 l2_loss 16.5634613 fraction B 0.295648545 lossA 2.26145172 fraction A 0.0497201644\n",
      "step 2869 loss 1.10476303 fisher_loss 0.248391449 triplet loss 0.856371641 l2_loss 16.5693302 fraction B 0.293536752 lossA 2.20585179 fraction A 0.046322912\n",
      "step 2870 loss 0.992095113 fisher_loss 0.247892812 triplet loss 0.744202316 l2_loss 16.5738049 fraction B 0.177001238 lossA 2.15531087 fraction A 0.0434140116\n",
      "step 2871 loss 1.26446044 fisher_loss 0.247543648 triplet loss 1.01691675 l2_loss 16.5784035 fraction B 0.209287748 lossA 2.13942146 fraction A 0.0427846089\n",
      "step 2872 loss 1.13830781 fisher_loss 0.247241557 triplet loss 0.891066253 l2_loss 16.5826054 fraction B 0.30786255 lossA 2.15112853 fraction A 0.0437277\n",
      "step 2873 loss 0.900235891 fisher_loss 0.246950164 triplet loss 0.653285742 l2_loss 16.5865326 fraction B 0.185475037 lossA 2.28735709 fraction A 0.0541525893\n",
      "step 2874 loss 1.44785941 fisher_loss 0.247301102 triplet loss 1.2005583 l2_loss 16.5930977 fraction B 0.307945758 lossA 2.4182303 fraction A 0.0635292307\n",
      "step 2875 loss 0.822312 fisher_loss 0.247394294 triplet loss 0.574917734 l2_loss 16.597826 fraction B 0.0893682241 lossA 2.45085216 fraction A 0.0659667626\n",
      "step 2876 loss 1.06245494 fisher_loss 0.247261509 triplet loss 0.815193474 l2_loss 16.6024494 fraction B 0.141091138 lossA 2.40913773 fraction A 0.062666662\n",
      "step 2877 loss 1.28978753 fisher_loss 0.246632755 triplet loss 1.04315484 l2_loss 16.6052818 fraction B 0.225713447 lossA 2.30764771 fraction A 0.0552179739\n",
      "step 2878 loss 1.1795404 fisher_loss 0.245717093 triplet loss 0.933823347 l2_loss 16.6072369 fraction B 0.178900436 lossA 2.2265954 fraction A 0.048181463\n",
      "step 2879 loss 1.10836804 fisher_loss 0.244875431 triplet loss 0.863492608 l2_loss 16.6080894 fraction B 0.181404695 lossA 2.18269348 fraction A 0.043918632\n",
      "step 2880 loss 1.08457136 fisher_loss 0.244015813 triplet loss 0.840555608 l2_loss 16.6089 fraction B 0.243887022 lossA 2.15787673 fraction A 0.0406029522\n",
      "step 2881 loss 1.24385381 fisher_loss 0.243291855 triplet loss 1.00056195 l2_loss 16.6105366 fraction B 0.257454544 lossA 2.22191858 fraction A 0.0439195\n",
      "step 2882 loss 1.37539589 fisher_loss 0.243001223 triplet loss 1.13239467 l2_loss 16.613409 fraction B 0.252937376 lossA 2.26924205 fraction A 0.0461015552\n",
      "step 2883 loss 0.968148768 fisher_loss 0.242571726 triplet loss 0.725577056 l2_loss 16.6149 fraction B 0.207458973 lossA 2.41026306 fraction A 0.0557968095\n",
      "step 2884 loss 1.16300237 fisher_loss 0.242721111 triplet loss 0.920281291 l2_loss 16.6192551 fraction B 0.237681106 lossA 2.56225944 fraction A 0.0667288899\n",
      "step 2885 loss 1.16644073 fisher_loss 0.243194342 triplet loss 0.923246443 l2_loss 16.6241703 fraction B 0.178870901 lossA 2.71946287 fraction A 0.0773045048\n",
      "step 2886 loss 0.971483648 fisher_loss 0.243978977 triplet loss 0.727504671 l2_loss 16.6298904 fraction B 0.234273732 lossA 2.75711966 fraction A 0.0796142742\n",
      "step 2887 loss 0.9836905 fisher_loss 0.244051367 triplet loss 0.739639103 l2_loss 16.6334419 fraction B 0.37995407 lossA 2.65645933 fraction A 0.072659485\n",
      "step 2888 loss 1.19566202 fisher_loss 0.243184045 triplet loss 0.952477932 l2_loss 16.6359348 fraction B 0.155972734 lossA 2.50931931 fraction A 0.0611089766\n",
      "step 2889 loss 0.895381868 fisher_loss 0.24183546 triplet loss 0.653546393 l2_loss 16.6367836 fraction B 0.223339617 lossA 2.35592413 fraction A 0.0485777035\n",
      "step 2890 loss 1.0711807 fisher_loss 0.240544543 triplet loss 0.830636144 l2_loss 16.6384182 fraction B 0.206945822 lossA 2.27642751 fraction A 0.0427136123\n",
      "step 2891 loss 1.18799365 fisher_loss 0.240002275 triplet loss 0.947991371 l2_loss 16.6423244 fraction B 0.209618762 lossA 2.19821692 fraction A 0.0371936373\n",
      "step 2892 loss 1.20139217 fisher_loss 0.239348903 triplet loss 0.962043226 l2_loss 16.6450043 fraction B 0.205382138 lossA 2.17059398 fraction A 0.0357560664\n",
      "step 2893 loss 1.04183328 fisher_loss 0.239187419 triplet loss 0.802645862 l2_loss 16.6488113 fraction B 0.287925631 lossA 2.17359138 fraction A 0.0361781344\n",
      "step 2894 loss 1.01787877 fisher_loss 0.239069864 triplet loss 0.778808951 l2_loss 16.6527214 fraction B 0.272754252 lossA 2.26418042 fraction A 0.0430338904\n",
      "step 2895 loss 1.22017419 fisher_loss 0.239590272 triplet loss 0.980583906 l2_loss 16.658 fraction B 0.200586021 lossA 2.4207263 fraction A 0.0554971769\n",
      "step 2896 loss 1.05654764 fisher_loss 0.240759358 triplet loss 0.815788329 l2_loss 16.6642303 fraction B 0.208011225 lossA 2.60855889 fraction A 0.068834804\n",
      "step 2897 loss 1.20175636 fisher_loss 0.242244348 triplet loss 0.959512055 l2_loss 16.6700745 fraction B 0.210838065 lossA 2.6655581 fraction A 0.0725272\n",
      "step 2898 loss 1.1181469 fisher_loss 0.242857769 triplet loss 0.875289083 l2_loss 16.6741638 fraction B 0.182136342 lossA 2.60632 fraction A 0.0688482821\n",
      "step 2899 loss 1.17903173 fisher_loss 0.242704302 triplet loss 0.936327457 l2_loss 16.6774235 fraction B 0.326713294 lossA 2.50592828 fraction A 0.0619603917\n",
      "step 2900 loss 1.20134127 fisher_loss 0.242050797 triplet loss 0.959290504 l2_loss 16.679039 fraction B 0.32402572 lossA 2.38131642 fraction A 0.0518006533\n",
      "step 2901 loss 1.01886427 fisher_loss 0.240887329 triplet loss 0.777977 l2_loss 16.6788197 fraction B 0.206666946 lossA 2.31261683 fraction A 0.0459431186\n",
      "step 2902 loss 1.20450211 fisher_loss 0.240154 triplet loss 0.964348078 l2_loss 16.6801758 fraction B 0.290652663 lossA 2.23763418 fraction A 0.0398960672\n",
      "step 2903 loss 1.04786837 fisher_loss 0.239469871 triplet loss 0.808398485 l2_loss 16.6816044 fraction B 0.262086034 lossA 2.19393826 fraction A 0.0368062\n",
      "step 2904 loss 0.824582 fisher_loss 0.238891959 triplet loss 0.58569 l2_loss 16.6838322 fraction B 0.203422531 lossA 2.26759672 fraction A 0.0432199314\n",
      "step 2905 loss 1.19487834 fisher_loss 0.239204198 triplet loss 0.955674112 l2_loss 16.6903896 fraction B 0.286949545 lossA 2.33503699 fraction A 0.0495348498\n",
      "step 2906 loss 1.16733897 fisher_loss 0.23940286 triplet loss 0.927936077 l2_loss 16.6957512 fraction B 0.269301385 lossA 2.40971947 fraction A 0.0565341227\n",
      "step 2907 loss 0.86449486 fisher_loss 0.239644662 triplet loss 0.624850214 l2_loss 16.7007103 fraction B 0.19899106 lossA 2.44337535 fraction A 0.0598222\n",
      "step 2908 loss 0.950199962 fisher_loss 0.239463359 triplet loss 0.710736573 l2_loss 16.7060795 fraction B 0.186394215 lossA 2.42880702 fraction A 0.0586972684\n",
      "step 2909 loss 1.1440779 fisher_loss 0.239053532 triplet loss 0.90502435 l2_loss 16.7111549 fraction B 0.272326916 lossA 2.37760139 fraction A 0.0546178333\n",
      "step 2910 loss 0.936993718 fisher_loss 0.238380581 triplet loss 0.698613167 l2_loss 16.7147617 fraction B 0.186514437 lossA 2.32583117 fraction A 0.0499318503\n",
      "step 2911 loss 0.951442 fisher_loss 0.237534612 triplet loss 0.713907361 l2_loss 16.7175941 fraction B 0.18803294 lossA 2.29013944 fraction A 0.0455513671\n",
      "step 2912 loss 0.99279803 fisher_loss 0.236975655 triplet loss 0.755822361 l2_loss 16.72 fraction B 0.277041167 lossA 2.28511596 fraction A 0.0437930822\n",
      "step 2913 loss 0.994075775 fisher_loss 0.236989632 triplet loss 0.757086158 l2_loss 16.7237511 fraction B 0.274120659 lossA 2.26452041 fraction A 0.0410652235\n",
      "step 2914 loss 1.29765308 fisher_loss 0.237142637 triplet loss 1.0605104 l2_loss 16.7265491 fraction B 0.222800374 lossA 2.23538375 fraction A 0.038753096\n",
      "step 2915 loss 0.95630157 fisher_loss 0.237184182 triplet loss 0.719117403 l2_loss 16.7281342 fraction B 0.188421533 lossA 2.27857542 fraction A 0.0410315394\n",
      "step 2916 loss 0.966568947 fisher_loss 0.237786204 triplet loss 0.728782713 l2_loss 16.7320709 fraction B 0.227675378 lossA 2.35977173 fraction A 0.0470349416\n",
      "step 2917 loss 1.13024616 fisher_loss 0.238427222 triplet loss 0.891819 l2_loss 16.7387295 fraction B 0.255124867 lossA 2.44450521 fraction A 0.0563139245\n",
      "step 2918 loss 1.31856239 fisher_loss 0.239121586 triplet loss 1.07944083 l2_loss 16.7473373 fraction B 0.249720797 lossA 2.5185473 fraction A 0.0630817\n",
      "step 2919 loss 1.05071521 fisher_loss 0.239432529 triplet loss 0.811282635 l2_loss 16.7547092 fraction B 0.230971694 lossA 2.58789539 fraction A 0.0688938126\n",
      "step 2920 loss 1.08731461 fisher_loss 0.23965466 triplet loss 0.847659945 l2_loss 16.7613964 fraction B 0.156596556 lossA 2.57978916 fraction A 0.0682670847\n",
      "step 2921 loss 1.43997359 fisher_loss 0.239022791 triplet loss 1.20095074 l2_loss 16.7668152 fraction B 0.185041547 lossA 2.48079562 fraction A 0.0606599599\n",
      "step 2922 loss 0.957536757 fisher_loss 0.237827793 triplet loss 0.719709 l2_loss 16.7695446 fraction B 0.26055342 lossA 2.38559318 fraction A 0.0523190349\n",
      "step 2923 loss 1.24815691 fisher_loss 0.236791566 triplet loss 1.01136529 l2_loss 16.7720833 fraction B 0.189258203 lossA 2.31860733 fraction A 0.0457694307\n",
      "step 2924 loss 0.954449773 fisher_loss 0.236095876 triplet loss 0.718353868 l2_loss 16.7742691 fraction B 0.213752434 lossA 2.24353743 fraction A 0.0378996506\n",
      "step 2925 loss 1.14651608 fisher_loss 0.235408917 triplet loss 0.911107123 l2_loss 16.775528 fraction B 0.248597339 lossA 2.19072938 fraction A 0.0337203145\n",
      "step 2926 loss 1.03038156 fisher_loss 0.234934 triplet loss 0.795447528 l2_loss 16.7769318 fraction B 0.216976553 lossA 2.2193675 fraction A 0.0351690426\n",
      "step 2927 loss 0.916888714 fisher_loss 0.234889358 triplet loss 0.681999326 l2_loss 16.7809525 fraction B 0.198618844 lossA 2.27877212 fraction A 0.0391356051\n",
      "step 2928 loss 0.983713269 fisher_loss 0.235403806 triplet loss 0.748309433 l2_loss 16.7872791 fraction B 0.244650289 lossA 2.37363601 fraction A 0.0472131558\n",
      "step 2929 loss 1.19273889 fisher_loss 0.23656024 triplet loss 0.956178665 l2_loss 16.7955742 fraction B 0.308859974 lossA 2.44852567 fraction A 0.0546327941\n",
      "step 2930 loss 1.08144987 fisher_loss 0.237750128 triplet loss 0.843699753 l2_loss 16.8035984 fraction B 0.166107312 lossA 2.48449922 fraction A 0.058455158\n",
      "step 2931 loss 1.18520021 fisher_loss 0.23854506 triplet loss 0.946655214 l2_loss 16.8104954 fraction B 0.279827207 lossA 2.47889757 fraction A 0.0589032695\n",
      "step 2932 loss 1.20156217 fisher_loss 0.238627613 triplet loss 0.962934613 l2_loss 16.815876 fraction B 0.232293844 lossA 2.42974401 fraction A 0.0560577959\n",
      "step 2933 loss 0.893921077 fisher_loss 0.238126501 triplet loss 0.655794561 l2_loss 16.8201275 fraction B 0.141663909 lossA 2.30454803 fraction A 0.0460114703\n",
      "step 2934 loss 1.28885126 fisher_loss 0.236573637 triplet loss 1.05227757 l2_loss 16.8237152 fraction B 0.225225791 lossA 2.18100357 fraction A 0.0369721353\n",
      "step 2935 loss 1.12607765 fisher_loss 0.235380411 triplet loss 0.890697241 l2_loss 16.827055 fraction B 0.271092117 lossA 2.06451797 fraction A 0.0308072865\n",
      "step 2936 loss 0.91547513 fisher_loss 0.23471272 triplet loss 0.68076241 l2_loss 16.8299141 fraction B 0.21630685 lossA 2.10089397 fraction A 0.0322177634\n",
      "step 2937 loss 0.945215762 fisher_loss 0.234862685 triplet loss 0.710353076 l2_loss 16.8351688 fraction B 0.270706862 lossA 2.20766568 fraction A 0.0391601287\n",
      "step 2938 loss 1.1992408 fisher_loss 0.235654145 triplet loss 0.963586628 l2_loss 16.841629 fraction B 0.162052184 lossA 2.30427 fraction A 0.047823146\n",
      "step 2939 loss 1.25556731 fisher_loss 0.236757338 triplet loss 1.01881 l2_loss 16.8474827 fraction B 0.225105956 lossA 2.41687965 fraction A 0.0566617362\n",
      "step 2940 loss 0.827831388 fisher_loss 0.237880617 triplet loss 0.58995074 l2_loss 16.853302 fraction B 0.22064282 lossA 2.49701 fraction A 0.0626919791\n",
      "step 2941 loss 0.951793432 fisher_loss 0.239060625 triplet loss 0.712732792 l2_loss 16.8598957 fraction B 0.161179438 lossA 2.47622633 fraction A 0.0606539845\n",
      "step 2942 loss 1.14721012 fisher_loss 0.239288867 triplet loss 0.907921255 l2_loss 16.8648605 fraction B 0.190773219 lossA 2.42030501 fraction A 0.0557732657\n",
      "step 2943 loss 1.11184037 fisher_loss 0.238990113 triplet loss 0.872850299 l2_loss 16.8685112 fraction B 0.225156859 lossA 2.3555572 fraction A 0.0493105948\n",
      "step 2944 loss 1.15361929 fisher_loss 0.238324881 triplet loss 0.915294409 l2_loss 16.8716831 fraction B 0.254110217 lossA 2.30566335 fraction A 0.0445302166\n",
      "step 2945 loss 0.911241651 fisher_loss 0.237799361 triplet loss 0.673442304 l2_loss 16.8750629 fraction B 0.180367172 lossA 2.29549861 fraction A 0.0433151238\n",
      "step 2946 loss 1.27064157 fisher_loss 0.237737969 triplet loss 1.03290355 l2_loss 16.8794899 fraction B 0.168924764 lossA 2.31689572 fraction A 0.0452453606\n",
      "step 2947 loss 1.02192295 fisher_loss 0.237875372 triplet loss 0.784047544 l2_loss 16.8846722 fraction B 0.183004 lossA 2.41359305 fraction A 0.0539104417\n",
      "step 2948 loss 0.958645582 fisher_loss 0.238805383 triplet loss 0.719840169 l2_loss 16.8920078 fraction B 0.168677136 lossA 2.52247715 fraction A 0.0626444891\n",
      "step 2949 loss 1.12604928 fisher_loss 0.239867851 triplet loss 0.886181474 l2_loss 16.9002476 fraction B 0.187914431 lossA 2.56045938 fraction A 0.0654344708\n",
      "step 2950 loss 1.1360836 fisher_loss 0.240179792 triplet loss 0.895903826 l2_loss 16.9065456 fraction B 0.223443151 lossA 2.5696733 fraction A 0.0659808218\n",
      "step 2951 loss 1.00931084 fisher_loss 0.240136236 triplet loss 0.769174576 l2_loss 16.9116135 fraction B 0.162104562 lossA 2.53335404 fraction A 0.0633452907\n",
      "step 2952 loss 1.20233452 fisher_loss 0.239869937 triplet loss 0.962464571 l2_loss 16.9177055 fraction B 0.173206136 lossA 2.44140196 fraction A 0.0561633483\n",
      "step 2953 loss 1.00828171 fisher_loss 0.23914 triplet loss 0.769141674 l2_loss 16.9222012 fraction B 0.184215844 lossA 2.32611012 fraction A 0.0461109728\n",
      "step 2954 loss 1.05963051 fisher_loss 0.238398775 triplet loss 0.821231782 l2_loss 16.9265137 fraction B 0.257534415 lossA 2.26355696 fraction A 0.0405112\n",
      "step 2955 loss 0.978812873 fisher_loss 0.238538444 triplet loss 0.740274429 l2_loss 16.9312096 fraction B 0.170725942 lossA 2.24329615 fraction A 0.038667392\n",
      "step 2956 loss 1.09122622 fisher_loss 0.239235803 triplet loss 0.851990461 l2_loss 16.9362183 fraction B 0.24689725 lossA 2.27381635 fraction A 0.0399029851\n",
      "step 2957 loss 1.19994903 fisher_loss 0.240102 triplet loss 0.959847 l2_loss 16.9425793 fraction B 0.278259784 lossA 2.37160277 fraction A 0.0468747281\n",
      "step 2958 loss 1.00868535 fisher_loss 0.24112469 triplet loss 0.76756072 l2_loss 16.9504681 fraction B 0.233260676 lossA 2.47014737 fraction A 0.0565384\n",
      "step 2959 loss 1.29389262 fisher_loss 0.242059067 triplet loss 1.05183351 l2_loss 16.9600868 fraction B 0.221090034 lossA 2.6397562 fraction A 0.0707529783\n",
      "step 2960 loss 0.990405738 fisher_loss 0.243286327 triplet loss 0.747119427 l2_loss 16.9705276 fraction B 0.191842645 lossA 2.73725486 fraction A 0.0781459585\n",
      "step 2961 loss 1.20411038 fisher_loss 0.243963838 triplet loss 0.960146606 l2_loss 16.9792786 fraction B 0.286476493 lossA 2.7754457 fraction A 0.0806389451\n",
      "step 2962 loss 1.01213038 fisher_loss 0.243948251 triplet loss 0.768182158 l2_loss 16.9863472 fraction B 0.182723492 lossA 2.73003626 fraction A 0.0768987387\n",
      "step 2963 loss 1.13480842 fisher_loss 0.242857 triplet loss 0.891951442 l2_loss 16.9903965 fraction B 0.239258692 lossA 2.60733366 fraction A 0.0675014853\n",
      "step 2964 loss 1.23826766 fisher_loss 0.240975186 triplet loss 0.997292459 l2_loss 16.9922047 fraction B 0.215258688 lossA 2.45481253 fraction A 0.0555010177\n",
      "step 2965 loss 1.13902116 fisher_loss 0.239247113 triplet loss 0.899774 l2_loss 16.993042 fraction B 0.196297184 lossA 2.35543132 fraction A 0.0467132479\n",
      "step 2966 loss 0.988317788 fisher_loss 0.23797971 triplet loss 0.750338078 l2_loss 16.9936504 fraction B 0.162904188 lossA 2.38101387 fraction A 0.0477075949\n",
      "step 2967 loss 0.834573865 fisher_loss 0.237831324 triplet loss 0.59674257 l2_loss 16.9973869 fraction B 0.184730098 lossA 2.45663691 fraction A 0.0523003787\n",
      "step 2968 loss 1.01683438 fisher_loss 0.238470778 triplet loss 0.778363645 l2_loss 17.0033607 fraction B 0.231358796 lossA 2.51896453 fraction A 0.0558755584\n",
      "step 2969 loss 1.17609048 fisher_loss 0.239304528 triplet loss 0.936786 l2_loss 17.0088139 fraction B 0.246149018 lossA 2.58114052 fraction A 0.0590899959\n",
      "step 2970 loss 1.14332068 fisher_loss 0.240232795 triplet loss 0.903087914 l2_loss 17.0139065 fraction B 0.205972731 lossA 2.60624123 fraction A 0.0601486415\n",
      "step 2971 loss 0.949998736 fisher_loss 0.240668222 triplet loss 0.709330499 l2_loss 17.0190716 fraction B 0.2392499 lossA 2.55295396 fraction A 0.0557548665\n",
      "step 2972 loss 0.922773 fisher_loss 0.240649551 triplet loss 0.682123423 l2_loss 17.0238876 fraction B 0.181925595 lossA 2.46758246 fraction A 0.0491072051\n",
      "step 2973 loss 1.30312347 fisher_loss 0.240128711 triplet loss 1.06299472 l2_loss 17.0290928 fraction B 0.231116101 lossA 2.38826466 fraction A 0.0444261469\n",
      "step 2974 loss 1.04852498 fisher_loss 0.239462271 triplet loss 0.80906266 l2_loss 17.034771 fraction B 0.289301664 lossA 2.34140944 fraction A 0.0435213782\n",
      "step 2975 loss 0.934703 fisher_loss 0.239279181 triplet loss 0.695423782 l2_loss 17.0420666 fraction B 0.187014386 lossA 2.34449863 fraction A 0.0463803262\n",
      "step 2976 loss 0.893698692 fisher_loss 0.239477813 triplet loss 0.654220879 l2_loss 17.0505199 fraction B 0.167350069 lossA 2.35715604 fraction A 0.0496692359\n",
      "step 2977 loss 1.14670634 fisher_loss 0.239917487 triplet loss 0.906788886 l2_loss 17.0589485 fraction B 0.218790501 lossA 2.40778875 fraction A 0.0549867637\n",
      "step 2978 loss 0.894290328 fisher_loss 0.240651295 triplet loss 0.653639 l2_loss 17.067234 fraction B 0.146114081 lossA 2.45802546 fraction A 0.0599487647\n",
      "step 2979 loss 1.10497069 fisher_loss 0.241657093 triplet loss 0.863313556 l2_loss 17.075325 fraction B 0.242621437 lossA 2.44768357 fraction A 0.0601136498\n",
      "step 2980 loss 1.14900446 fisher_loss 0.241929442 triplet loss 0.907075047 l2_loss 17.0810452 fraction B 0.251358569 lossA 2.44578362 fraction A 0.060250029\n",
      "step 2981 loss 1.12963319 fisher_loss 0.241872698 triplet loss 0.88776052 l2_loss 17.0858116 fraction B 0.163463503 lossA 2.36552596 fraction A 0.0548251383\n",
      "step 2982 loss 1.1201421 fisher_loss 0.241454348 triplet loss 0.878687739 l2_loss 17.0897083 fraction B 0.191485301 lossA 2.31298208 fraction A 0.0502720885\n",
      "step 2983 loss 1.13467968 fisher_loss 0.241321087 triplet loss 0.893358588 l2_loss 17.0933437 fraction B 0.202977255 lossA 2.25056458 fraction A 0.0439140312\n",
      "step 2984 loss 1.13155425 fisher_loss 0.241520211 triplet loss 0.890034 l2_loss 17.0961056 fraction B 0.22450763 lossA 2.20547032 fraction A 0.0393083543\n",
      "step 2985 loss 0.996659696 fisher_loss 0.242084309 triplet loss 0.754575372 l2_loss 17.0989094 fraction B 0.18079029 lossA 2.26417756 fraction A 0.0429562293\n",
      "step 2986 loss 0.764093757 fisher_loss 0.243439704 triplet loss 0.520654082 l2_loss 17.1055031 fraction B 0.150767624 lossA 2.35405087 fraction A 0.0505979136\n",
      "step 2987 loss 1.09890246 fisher_loss 0.244623959 triplet loss 0.854278445 l2_loss 17.1166172 fraction B 0.171558931 lossA 2.44737864 fraction A 0.0618133619\n",
      "step 2988 loss 1.04341578 fisher_loss 0.245608509 triplet loss 0.797807336 l2_loss 17.1291027 fraction B 0.237333834 lossA 2.49447179 fraction A 0.0679077208\n",
      "step 2989 loss 1.12285399 fisher_loss 0.245782375 triplet loss 0.877071619 l2_loss 17.1409607 fraction B 0.182163745 lossA 2.50099468 fraction A 0.0699919\n",
      "step 2990 loss 1.2577157 fisher_loss 0.245498806 triplet loss 1.01221693 l2_loss 17.1518021 fraction B 0.22799097 lossA 2.45627332 fraction A 0.0680330843\n",
      "step 2991 loss 1.07947671 fisher_loss 0.244696543 triplet loss 0.834780216 l2_loss 17.1611614 fraction B 0.241033569 lossA 2.39916945 fraction A 0.0637415722\n",
      "step 2992 loss 1.14210737 fisher_loss 0.243622407 triplet loss 0.898485 l2_loss 17.1691856 fraction B 0.213173449 lossA 2.32247448 fraction A 0.0580108874\n",
      "step 2993 loss 1.02828455 fisher_loss 0.242398947 triplet loss 0.785885632 l2_loss 17.1755028 fraction B 0.280046642 lossA 2.26003313 fraction A 0.0531210788\n",
      "step 2994 loss 1.03765869 fisher_loss 0.241643608 triplet loss 0.796015 l2_loss 17.1812286 fraction B 0.326531917 lossA 2.24330568 fraction A 0.0512673445\n",
      "step 2995 loss 1.01267374 fisher_loss 0.241296738 triplet loss 0.771377 l2_loss 17.1866169 fraction B 0.222489759 lossA 2.22617078 fraction A 0.0484808795\n",
      "step 2996 loss 0.858111501 fisher_loss 0.240767792 triplet loss 0.617343724 l2_loss 17.1910458 fraction B 0.143136531 lossA 2.28327036 fraction A 0.0526895933\n",
      "step 2997 loss 0.998078 fisher_loss 0.241331249 triplet loss 0.756746769 l2_loss 17.1994286 fraction B 0.183979496 lossA 2.36925864 fraction A 0.0585649833\n",
      "step 2998 loss 1.35452831 fisher_loss 0.242089584 triplet loss 1.11243868 l2_loss 17.2079182 fraction B 0.198600069 lossA 2.45207381 fraction A 0.0638641119\n",
      "step 2999 loss 1.14565039 fisher_loss 0.242663234 triplet loss 0.902987182 l2_loss 17.2145519 fraction B 0.280111194 lossA 2.52574039 fraction A 0.0684016198\n",
      "step 3000 loss 1.0880878 fisher_loss 0.243200511 triplet loss 0.844887257 l2_loss 17.2206 fraction B 0.218923181 lossA 2.54556608 fraction A 0.0683088526\n",
      "step 3001 loss 1.02005446 fisher_loss 0.243311331 triplet loss 0.776743174 l2_loss 17.2245407 fraction B 0.166302323 lossA 2.55052876 fraction A 0.0663706511\n",
      "step 3002 loss 1.1412977 fisher_loss 0.24332054 triplet loss 0.897977114 l2_loss 17.2284088 fraction B 0.326880574 lossA 2.50691891 fraction A 0.0604685247\n",
      "step 3003 loss 1.0107733 fisher_loss 0.242828339 triplet loss 0.767944932 l2_loss 17.2299709 fraction B 0.211415157 lossA 2.46430922 fraction A 0.054323379\n",
      "step 3004 loss 1.17661035 fisher_loss 0.242311254 triplet loss 0.934299052 l2_loss 17.2325439 fraction B 0.228132889 lossA 2.43145323 fraction A 0.0498771556\n",
      "step 3005 loss 1.1391443 fisher_loss 0.241808623 triplet loss 0.897335708 l2_loss 17.2359371 fraction B 0.278623 lossA 2.39669037 fraction A 0.0460501909\n",
      "step 3006 loss 1.15770817 fisher_loss 0.241111353 triplet loss 0.91659683 l2_loss 17.239006 fraction B 0.261985123 lossA 2.42272282 fraction A 0.0483539179\n",
      "step 3007 loss 0.997517467 fisher_loss 0.240716904 triplet loss 0.756800592 l2_loss 17.2444134 fraction B 0.197425336 lossA 2.49328637 fraction A 0.0561116226\n",
      "step 3008 loss 1.22490799 fisher_loss 0.240908593 triplet loss 0.983999372 l2_loss 17.2520351 fraction B 0.338326693 lossA 2.54065347 fraction A 0.0607088953\n",
      "step 3009 loss 1.16733968 fisher_loss 0.240914658 triplet loss 0.926425 l2_loss 17.2583752 fraction B 0.195972696 lossA 2.59363 fraction A 0.0652516186\n",
      "step 3010 loss 1.06616843 fisher_loss 0.240862861 triplet loss 0.825305521 l2_loss 17.2648335 fraction B 0.209268585 lossA 2.58062172 fraction A 0.0650547147\n",
      "step 3011 loss 1.14240265 fisher_loss 0.240463331 triplet loss 0.901939332 l2_loss 17.2700367 fraction B 0.21107778 lossA 2.52994013 fraction A 0.0623702146\n",
      "step 3012 loss 1.02612758 fisher_loss 0.239843726 triplet loss 0.786283851 l2_loss 17.2751427 fraction B 0.12816231 lossA 2.46965384 fraction A 0.058495909\n",
      "step 3013 loss 1.01491201 fisher_loss 0.239124209 triplet loss 0.77578783 l2_loss 17.2800865 fraction B 0.198884457 lossA 2.37257552 fraction A 0.0520685166\n",
      "step 3014 loss 0.968853593 fisher_loss 0.238389581 triplet loss 0.730464041 l2_loss 17.2848167 fraction B 0.225292042 lossA 2.32145643 fraction A 0.0486817\n",
      "step 3015 loss 0.962091565 fisher_loss 0.238184318 triplet loss 0.723907232 l2_loss 17.2894936 fraction B 0.221635938 lossA 2.28138661 fraction A 0.0465357192\n",
      "step 3016 loss 1.20053077 fisher_loss 0.238399237 triplet loss 0.9621315 l2_loss 17.2957382 fraction B 0.21566999 lossA 2.25393963 fraction A 0.0456707962\n",
      "step 3017 loss 1.11851728 fisher_loss 0.238675833 triplet loss 0.879841447 l2_loss 17.3015518 fraction B 0.22397393 lossA 2.25782132 fraction A 0.0475937501\n",
      "step 3018 loss 1.22990394 fisher_loss 0.239410147 triplet loss 0.990493834 l2_loss 17.30797 fraction B 0.265945941 lossA 2.28565621 fraction A 0.0506461971\n",
      "step 3019 loss 1.00914681 fisher_loss 0.240297794 triplet loss 0.768849 l2_loss 17.3138542 fraction B 0.157494485 lossA 2.3267982 fraction A 0.0542504638\n",
      "step 3020 loss 1.06359792 fisher_loss 0.241126642 triplet loss 0.822471321 l2_loss 17.3197708 fraction B 0.178383872 lossA 2.33246827 fraction A 0.0547632314\n",
      "step 3021 loss 1.04340851 fisher_loss 0.24142684 triplet loss 0.801981628 l2_loss 17.325016 fraction B 0.31263265 lossA 2.3200314 fraction A 0.0534139387\n",
      "step 3022 loss 1.00418496 fisher_loss 0.241267 triplet loss 0.762917936 l2_loss 17.3293667 fraction B 0.230362296 lossA 2.27967358 fraction A 0.050132487\n",
      "step 3023 loss 0.84151262 fisher_loss 0.240411058 triplet loss 0.601101577 l2_loss 17.3328381 fraction B 0.146595255 lossA 2.22590804 fraction A 0.0453304835\n",
      "step 3024 loss 1.08227801 fisher_loss 0.239912823 triplet loss 0.842365205 l2_loss 17.3377514 fraction B 0.19304511 lossA 2.27104592 fraction A 0.0490992\n",
      "step 3025 loss 0.916131556 fisher_loss 0.240333632 triplet loss 0.675797939 l2_loss 17.3449898 fraction B 0.191642657 lossA 2.3102634 fraction A 0.0520106703\n",
      "step 3026 loss 1.15739512 fisher_loss 0.240847901 triplet loss 0.916547179 l2_loss 17.3516293 fraction B 0.247426212 lossA 2.3200624 fraction A 0.0522091724\n",
      "step 3027 loss 1.25714445 fisher_loss 0.240952909 triplet loss 1.0161916 l2_loss 17.3568897 fraction B 0.217088401 lossA 2.32520962 fraction A 0.0518868975\n",
      "step 3028 loss 1.02555633 fisher_loss 0.240989596 triplet loss 0.78456676 l2_loss 17.3618221 fraction B 0.144128188 lossA 2.32649493 fraction A 0.051455088\n",
      "step 3029 loss 1.34742403 fisher_loss 0.240978017 triplet loss 1.10644603 l2_loss 17.3668213 fraction B 0.276648462 lossA 2.36893201 fraction A 0.0541421957\n",
      "step 3030 loss 1.04371512 fisher_loss 0.241098955 triplet loss 0.802616119 l2_loss 17.3710022 fraction B 0.224576265 lossA 2.44856954 fraction A 0.0599718392\n",
      "step 3031 loss 1.33428121 fisher_loss 0.241415784 triplet loss 1.09286547 l2_loss 17.3765488 fraction B 0.272811651 lossA 2.50238514 fraction A 0.0627083555\n",
      "step 3032 loss 1.2557466 fisher_loss 0.241272748 triplet loss 1.01447392 l2_loss 17.380209 fraction B 0.22396794 lossA 2.4871614 fraction A 0.0606356189\n",
      "step 3033 loss 1.22283435 fisher_loss 0.240785569 triplet loss 0.98204881 l2_loss 17.3827419 fraction B 0.276152462 lossA 2.46150947 fraction A 0.0578926541\n",
      "step 3034 loss 1.05326498 fisher_loss 0.240152135 triplet loss 0.813112855 l2_loss 17.3847103 fraction B 0.152867883 lossA 2.37559199 fraction A 0.0481864959\n",
      "step 3035 loss 1.00874293 fisher_loss 0.23907195 triplet loss 0.769671 l2_loss 17.3852024 fraction B 0.124888927 lossA 2.32556772 fraction A 0.0421291925\n",
      "step 3036 loss 0.901430607 fisher_loss 0.238417715 triplet loss 0.663012862 l2_loss 17.3866692 fraction B 0.175261557 lossA 2.38102 fraction A 0.0455581434\n",
      "step 3037 loss 1.09133482 fisher_loss 0.238451838 triplet loss 0.852883 l2_loss 17.3913059 fraction B 0.173529416 lossA 2.43382549 fraction A 0.0507427305\n",
      "step 3038 loss 1.09525108 fisher_loss 0.23852706 triplet loss 0.856724 l2_loss 17.3973579 fraction B 0.245428234 lossA 2.49191403 fraction A 0.0554425158\n",
      "step 3039 loss 0.971847773 fisher_loss 0.238539279 triplet loss 0.733308494 l2_loss 17.4026413 fraction B 0.155818909 lossA 2.5293951 fraction A 0.0577112176\n",
      "step 3040 loss 1.13101912 fisher_loss 0.238362163 triplet loss 0.892657 l2_loss 17.4074478 fraction B 0.310492456 lossA 2.50226212 fraction A 0.0548919328\n",
      "step 3041 loss 1.0797174 fisher_loss 0.237595394 triplet loss 0.842121959 l2_loss 17.4103508 fraction B 0.318886489 lossA 2.45543647 fraction A 0.050530538\n",
      "step 3042 loss 1.1097362 fisher_loss 0.236488193 triplet loss 0.873248 l2_loss 17.4122601 fraction B 0.316804171 lossA 2.41493201 fraction A 0.046381738\n",
      "step 3043 loss 1.25264359 fisher_loss 0.235516086 triplet loss 1.01712751 l2_loss 17.4145393 fraction B 0.169323698 lossA 2.37973952 fraction A 0.044161249\n",
      "step 3044 loss 0.923880577 fisher_loss 0.234809592 triplet loss 0.689071 l2_loss 17.4167614 fraction B 0.299328983 lossA 2.34047 fraction A 0.0419551097\n",
      "step 3045 loss 0.9454633 fisher_loss 0.234255418 triplet loss 0.711207867 l2_loss 17.4192543 fraction B 0.194494829 lossA 2.32385039 fraction A 0.0418652743\n",
      "step 3046 loss 1.39375889 fisher_loss 0.234133556 triplet loss 1.15962529 l2_loss 17.4228363 fraction B 0.23449637 lossA 2.3365128 fraction A 0.0444499105\n",
      "step 3047 loss 0.94364816 fisher_loss 0.234235823 triplet loss 0.709412336 l2_loss 17.426096 fraction B 0.161014557 lossA 2.35221434 fraction A 0.047459472\n",
      "step 3048 loss 1.15229559 fisher_loss 0.234502897 triplet loss 0.917792678 l2_loss 17.4303093 fraction B 0.193428621 lossA 2.37667346 fraction A 0.0503840558\n",
      "step 3049 loss 0.912045836 fisher_loss 0.234570801 triplet loss 0.677475035 l2_loss 17.4330311 fraction B 0.156851128 lossA 2.43069911 fraction A 0.0555175692\n",
      "step 3050 loss 0.855280876 fisher_loss 0.235018596 triplet loss 0.620262265 l2_loss 17.4364777 fraction B 0.159156248 lossA 2.42586875 fraction A 0.0548874028\n",
      "step 3051 loss 1.02480531 fisher_loss 0.234976768 triplet loss 0.789828539 l2_loss 17.4410667 fraction B 0.252080411 lossA 2.36053371 fraction A 0.0482515544\n",
      "step 3052 loss 1.00807464 fisher_loss 0.234335944 triplet loss 0.773738742 l2_loss 17.4447861 fraction B 0.171024144 lossA 2.31855083 fraction A 0.0438675582\n",
      "step 3053 loss 1.09189355 fisher_loss 0.233852178 triplet loss 0.858041346 l2_loss 17.4489136 fraction B 0.23439239 lossA 2.32050395 fraction A 0.043780189\n",
      "step 3054 loss 1.12749279 fisher_loss 0.233632773 triplet loss 0.893860042 l2_loss 17.4539242 fraction B 0.284307122 lossA 2.32808685 fraction A 0.04463863\n",
      "step 3055 loss 0.918922246 fisher_loss 0.233498856 triplet loss 0.685423374 l2_loss 17.4595566 fraction B 0.12772651 lossA 2.39446425 fraction A 0.0502175\n",
      "step 3056 loss 1.19300163 fisher_loss 0.233784869 triplet loss 0.959216714 l2_loss 17.4668331 fraction B 0.24242489 lossA 2.46052766 fraction A 0.0553029515\n",
      "step 3057 loss 1.15358496 fisher_loss 0.233961076 triplet loss 0.919623852 l2_loss 17.4729195 fraction B 0.301059544 lossA 2.50767756 fraction A 0.0586394966\n",
      "step 3058 loss 0.91811341 fisher_loss 0.2339499 triplet loss 0.684163511 l2_loss 17.4782829 fraction B 0.188104287 lossA 2.53665018 fraction A 0.0602194183\n",
      "step 3059 loss 1.01875746 fisher_loss 0.234013408 triplet loss 0.784744 l2_loss 17.4841404 fraction B 0.207009673 lossA 2.5239675 fraction A 0.0586790852\n",
      "step 3060 loss 1.04678667 fisher_loss 0.233881533 triplet loss 0.812905133 l2_loss 17.4899044 fraction B 0.261327147 lossA 2.48073077 fraction A 0.0543927476\n",
      "step 3061 loss 1.14478612 fisher_loss 0.233561307 triplet loss 0.911224842 l2_loss 17.495163 fraction B 0.335629255 lossA 2.43831706 fraction A 0.0492297448\n",
      "step 3062 loss 1.14054513 fisher_loss 0.233052835 triplet loss 0.90749234 l2_loss 17.4987068 fraction B 0.212866277 lossA 2.40286064 fraction A 0.0447454154\n",
      "step 3063 loss 1.24864364 fisher_loss 0.232692897 triplet loss 1.01595068 l2_loss 17.5025616 fraction B 0.315977782 lossA 2.39604211 fraction A 0.0428118818\n",
      "step 3064 loss 0.94600445 fisher_loss 0.232699767 triplet loss 0.713304698 l2_loss 17.50634 fraction B 0.234895602 lossA 2.46888351 fraction A 0.0483852141\n",
      "step 3065 loss 0.849379 fisher_loss 0.233433425 triplet loss 0.615945578 l2_loss 17.5143299 fraction B 0.172013834 lossA 2.49975824 fraction A 0.0521255322\n",
      "step 3066 loss 1.10209119 fisher_loss 0.233963028 triplet loss 0.868128181 l2_loss 17.5229053 fraction B 0.214085668 lossA 2.55528617 fraction A 0.058228042\n",
      "step 3067 loss 1.05368209 fisher_loss 0.234641522 triplet loss 0.819040596 l2_loss 17.531971 fraction B 0.186312184 lossA 2.52057147 fraction A 0.0577213578\n",
      "step 3068 loss 1.28641558 fisher_loss 0.235000715 triplet loss 1.05141485 l2_loss 17.5404778 fraction B 0.241124451 lossA 2.46220064 fraction A 0.0547258146\n",
      "step 3069 loss 1.31317139 fisher_loss 0.235059664 triplet loss 1.07811177 l2_loss 17.5474072 fraction B 0.246789962 lossA 2.4028461 fraction A 0.0513457656\n",
      "step 3070 loss 0.912355244 fisher_loss 0.23509483 triplet loss 0.677260399 l2_loss 17.5541821 fraction B 0.170336232 lossA 2.35184908 fraction A 0.0486059561\n",
      "step 3071 loss 0.869327724 fisher_loss 0.235452965 triplet loss 0.633874774 l2_loss 17.5618782 fraction B 0.146539941 lossA 2.37335825 fraction A 0.0518758483\n",
      "step 3072 loss 0.939768 fisher_loss 0.236725077 triplet loss 0.703042924 l2_loss 17.571764 fraction B 0.130493224 lossA 2.39989734 fraction A 0.0553161725\n",
      "step 3073 loss 1.20450151 fisher_loss 0.238197759 triplet loss 0.966303706 l2_loss 17.5823078 fraction B 0.262899786 lossA 2.38519716 fraction A 0.0552258343\n",
      "step 3074 loss 1.31547046 fisher_loss 0.239085957 triplet loss 1.07638454 l2_loss 17.591114 fraction B 0.191452354 lossA 2.37465715 fraction A 0.0550724305\n",
      "step 3075 loss 1.30651283 fisher_loss 0.239661917 triplet loss 1.0668509 l2_loss 17.5986938 fraction B 0.197808072 lossA 2.37348819 fraction A 0.0554484576\n",
      "step 3076 loss 1.14634514 fisher_loss 0.240083098 triplet loss 0.9062621 l2_loss 17.6049461 fraction B 0.178064257 lossA 2.3762219 fraction A 0.0561141931\n",
      "step 3077 loss 1.05061769 fisher_loss 0.240311578 triplet loss 0.810306072 l2_loss 17.6106205 fraction B 0.228358269 lossA 2.35506797 fraction A 0.0536265634\n",
      "step 3078 loss 1.03253913 fisher_loss 0.239616573 triplet loss 0.792922556 l2_loss 17.6144657 fraction B 0.204487711 lossA 2.34197 fraction A 0.0522197112\n",
      "step 3079 loss 0.971393168 fisher_loss 0.239067316 triplet loss 0.732325852 l2_loss 17.6189289 fraction B 0.228271216 lossA 2.26978946 fraction A 0.045361273\n",
      "step 3080 loss 1.2614131 fisher_loss 0.238320038 triplet loss 1.0230931 l2_loss 17.6222839 fraction B 0.200318038 lossA 2.18316197 fraction A 0.0381475613\n",
      "step 3081 loss 0.931248665 fisher_loss 0.237463102 triplet loss 0.693785548 l2_loss 17.6252556 fraction B 0.260264397 lossA 2.16139126 fraction A 0.0363338552\n",
      "step 3082 loss 1.08774018 fisher_loss 0.237061501 triplet loss 0.850678682 l2_loss 17.6301403 fraction B 0.251437038 lossA 2.2047019 fraction A 0.0386908278\n",
      "step 3083 loss 0.959596694 fisher_loss 0.237036124 triplet loss 0.722560585 l2_loss 17.6351032 fraction B 0.202681318 lossA 2.2596817 fraction A 0.0419966206\n",
      "step 3084 loss 1.09408045 fisher_loss 0.237213925 triplet loss 0.856866479 l2_loss 17.6397114 fraction B 0.244754225 lossA 2.37673521 fraction A 0.0500923544\n",
      "step 3085 loss 0.886833429 fisher_loss 0.237687528 triplet loss 0.649145901 l2_loss 17.6444569 fraction B 0.172798768 lossA 2.43011904 fraction A 0.053584002\n",
      "step 3086 loss 1.15090752 fisher_loss 0.238031 triplet loss 0.912876546 l2_loss 17.6490917 fraction B 0.201036334 lossA 2.48664927 fraction A 0.0575167\n",
      "step 3087 loss 1.58397567 fisher_loss 0.238244981 triplet loss 1.34573066 l2_loss 17.6532269 fraction B 0.335308045 lossA 2.51545191 fraction A 0.0590856858\n",
      "step 3088 loss 0.949446321 fisher_loss 0.238156468 triplet loss 0.711289883 l2_loss 17.6554909 fraction B 0.147171482 lossA 2.47404504 fraction A 0.0556753911\n",
      "step 3089 loss 0.972835243 fisher_loss 0.237381443 triplet loss 0.735453784 l2_loss 17.6572361 fraction B 0.1909132 lossA 2.42959881 fraction A 0.0518040583\n",
      "step 3090 loss 0.966163456 fisher_loss 0.236674562 triplet loss 0.729488909 l2_loss 17.6605606 fraction B 0.221337453 lossA 2.3833797 fraction A 0.0484323762\n",
      "step 3091 loss 1.12174451 fisher_loss 0.236030132 triplet loss 0.885714352 l2_loss 17.665 fraction B 0.128327191 lossA 2.35462046 fraction A 0.0475824103\n",
      "step 3092 loss 0.961830616 fisher_loss 0.235547945 triplet loss 0.726282656 l2_loss 17.6704292 fraction B 0.165788069 lossA 2.36594 fraction A 0.0496266373\n",
      "step 3093 loss 0.909443736 fisher_loss 0.235368297 triplet loss 0.674075425 l2_loss 17.6764889 fraction B 0.153171808 lossA 2.40665221 fraction A 0.0531433187\n",
      "step 3094 loss 1.10125804 fisher_loss 0.235416234 triplet loss 0.865841866 l2_loss 17.6822758 fraction B 0.21136269 lossA 2.41815591 fraction A 0.0541445836\n",
      "step 3095 loss 0.991584837 fisher_loss 0.235363349 triplet loss 0.756221473 l2_loss 17.6873608 fraction B 0.13897334 lossA 2.431252 fraction A 0.0551464334\n",
      "step 3096 loss 0.961788237 fisher_loss 0.235522687 triplet loss 0.72626555 l2_loss 17.6930866 fraction B 0.248780489 lossA 2.37561631 fraction A 0.0504573323\n",
      "step 3097 loss 1.0841198 fisher_loss 0.235397637 triplet loss 0.848722219 l2_loss 17.6976433 fraction B 0.285147935 lossA 2.39077616 fraction A 0.051485911\n",
      "step 3098 loss 0.897902668 fisher_loss 0.235798582 triplet loss 0.66210407 l2_loss 17.7028732 fraction B 0.277023345 lossA 2.38290501 fraction A 0.0503309146\n",
      "step 3099 loss 1.02127445 fisher_loss 0.236044735 triplet loss 0.785229683 l2_loss 17.7078781 fraction B 0.305575281 lossA 2.39282513 fraction A 0.0504852608\n",
      "step 3100 loss 0.951462209 fisher_loss 0.236321017 triplet loss 0.715141177 l2_loss 17.7135143 fraction B 0.158818841 lossA 2.43034 fraction A 0.053011179\n",
      "step 3101 loss 1.08812976 fisher_loss 0.23685436 triplet loss 0.851275444 l2_loss 17.7207603 fraction B 0.169892862 lossA 2.49599195 fraction A 0.0578539334\n",
      "step 3102 loss 1.05621254 fisher_loss 0.237571567 triplet loss 0.818640947 l2_loss 17.7282524 fraction B 0.221014947 lossA 2.53332472 fraction A 0.0601034723\n",
      "step 3103 loss 1.11141968 fisher_loss 0.237949982 triplet loss 0.87346971 l2_loss 17.7352753 fraction B 0.211011901 lossA 2.48099446 fraction A 0.0550184585\n",
      "step 3104 loss 1.10219097 fisher_loss 0.237348601 triplet loss 0.864842415 l2_loss 17.7401 fraction B 0.252284706 lossA 2.43832707 fraction A 0.0497564599\n",
      "step 3105 loss 0.981573105 fisher_loss 0.236783385 triplet loss 0.74478972 l2_loss 17.7443886 fraction B 0.188693032 lossA 2.45253825 fraction A 0.0497451238\n",
      "step 3106 loss 0.97207129 fisher_loss 0.236950547 triplet loss 0.735120714 l2_loss 17.7501354 fraction B 0.210220337 lossA 2.51387835 fraction A 0.0538422726\n",
      "step 3107 loss 1.20993721 fisher_loss 0.237667888 triplet loss 0.972269356 l2_loss 17.7571602 fraction B 0.236151785 lossA 2.54902935 fraction A 0.0557504818\n",
      "step 3108 loss 1.0693773 fisher_loss 0.237988532 triplet loss 0.831388772 l2_loss 17.7632484 fraction B 0.225585267 lossA 2.56760144 fraction A 0.05658748\n",
      "step 3109 loss 0.994964242 fisher_loss 0.237923324 triplet loss 0.757040918 l2_loss 17.7689228 fraction B 0.181282088 lossA 2.544415 fraction A 0.055403579\n",
      "step 3110 loss 1.11477768 fisher_loss 0.237491101 triplet loss 0.877286613 l2_loss 17.7752628 fraction B 0.227350429 lossA 2.50357151 fraction A 0.0530370027\n",
      "step 3111 loss 0.942014 fisher_loss 0.236935154 triplet loss 0.70507884 l2_loss 17.7800121 fraction B 0.171172783 lossA 2.495929 fraction A 0.0539893694\n",
      "step 3112 loss 1.00876689 fisher_loss 0.236719474 triplet loss 0.77204746 l2_loss 17.7869186 fraction B 0.285483807 lossA 2.47256136 fraction A 0.053309761\n",
      "step 3113 loss 0.997198701 fisher_loss 0.236469358 triplet loss 0.760729373 l2_loss 17.793251 fraction B 0.209458888 lossA 2.45085287 fraction A 0.0527232438\n",
      "step 3114 loss 1.20012045 fisher_loss 0.236306161 triplet loss 0.963814259 l2_loss 17.8001499 fraction B 0.214822352 lossA 2.41961694 fraction A 0.0511291176\n",
      "step 3115 loss 0.958287477 fisher_loss 0.236232728 triplet loss 0.72205478 l2_loss 17.8068676 fraction B 0.160519317 lossA 2.38438582 fraction A 0.0505101457\n",
      "step 3116 loss 1.38743591 fisher_loss 0.236482933 triplet loss 1.15095294 l2_loss 17.8155689 fraction B 0.192114964 lossA 2.33967352 fraction A 0.0489360169\n",
      "step 3117 loss 0.921035171 fisher_loss 0.236842319 triplet loss 0.684192836 l2_loss 17.8235989 fraction B 0.0932093188 lossA 2.31755114 fraction A 0.048491817\n",
      "step 3118 loss 1.10083938 fisher_loss 0.237200961 triplet loss 0.863638401 l2_loss 17.8312054 fraction B 0.185603306 lossA 2.36586666 fraction A 0.0525541566\n",
      "step 3119 loss 1.12168634 fisher_loss 0.237687245 triplet loss 0.883999109 l2_loss 17.8382759 fraction B 0.241965979 lossA 2.3605938 fraction A 0.0517956875\n",
      "step 3120 loss 1.01952684 fisher_loss 0.237618461 triplet loss 0.781908393 l2_loss 17.8436604 fraction B 0.122425251 lossA 2.36772633 fraction A 0.0513099805\n",
      "step 3121 loss 0.931111813 fisher_loss 0.237453565 triplet loss 0.693658233 l2_loss 17.8484898 fraction B 0.191815645 lossA 2.40416288 fraction A 0.0532047525\n",
      "step 3122 loss 1.07191086 fisher_loss 0.237628505 triplet loss 0.834282398 l2_loss 17.8545113 fraction B 0.231744796 lossA 2.41345644 fraction A 0.0524294414\n",
      "step 3123 loss 0.997245789 fisher_loss 0.237530023 triplet loss 0.759715796 l2_loss 17.8591881 fraction B 0.208817497 lossA 2.37446809 fraction A 0.0475780629\n",
      "step 3124 loss 1.20099831 fisher_loss 0.2370639 triplet loss 0.963934422 l2_loss 17.8628597 fraction B 0.10696239 lossA 2.31636286 fraction A 0.0419247895\n",
      "step 3125 loss 1.03180766 fisher_loss 0.236383066 triplet loss 0.79542464 l2_loss 17.8666344 fraction B 0.187520489 lossA 2.35259223 fraction A 0.0447316505\n",
      "step 3126 loss 1.02608335 fisher_loss 0.236487702 triplet loss 0.789595604 l2_loss 17.8726959 fraction B 0.223307133 lossA 2.36406636 fraction A 0.0456341021\n",
      "step 3127 loss 1.19473028 fisher_loss 0.236419648 triplet loss 0.958310664 l2_loss 17.8783493 fraction B 0.168734238 lossA 2.38483429 fraction A 0.0482082665\n",
      "step 3128 loss 1.17129529 fisher_loss 0.236521944 triplet loss 0.934773326 l2_loss 17.885704 fraction B 0.290748179 lossA 2.37926531 fraction A 0.0484353453\n",
      "step 3129 loss 1.04982769 fisher_loss 0.236297816 triplet loss 0.813529909 l2_loss 17.891468 fraction B 0.217355371 lossA 2.34040594 fraction A 0.0455119237\n",
      "step 3130 loss 0.985847235 fisher_loss 0.235977992 triplet loss 0.749869227 l2_loss 17.896616 fraction B 0.140785739 lossA 2.29872131 fraction A 0.0419815518\n",
      "step 3131 loss 0.964278877 fisher_loss 0.235708937 triplet loss 0.728569925 l2_loss 17.9009914 fraction B 0.123165362 lossA 2.24487257 fraction A 0.0381085835\n",
      "step 3132 loss 1.13657403 fisher_loss 0.235467583 triplet loss 0.901106477 l2_loss 17.9059162 fraction B 0.24548611 lossA 2.17869329 fraction A 0.0340508111\n",
      "step 3133 loss 1.05630195 fisher_loss 0.235139385 triplet loss 0.821162522 l2_loss 17.9098835 fraction B 0.189134479 lossA 2.22204661 fraction A 0.0374618284\n",
      "step 3134 loss 1.07151341 fisher_loss 0.235443652 triplet loss 0.836069763 l2_loss 17.916666 fraction B 0.308762342 lossA 2.26944351 fraction A 0.0415726341\n",
      "step 3135 loss 0.995249391 fisher_loss 0.235769302 triplet loss 0.759480059 l2_loss 17.9226379 fraction B 0.330604106 lossA 2.31266975 fraction A 0.0456283763\n",
      "step 3136 loss 0.967940867 fisher_loss 0.236171648 triplet loss 0.731769204 l2_loss 17.928936 fraction B 0.155959085 lossA 2.3382051 fraction A 0.0485248901\n",
      "step 3137 loss 1.13887477 fisher_loss 0.236519337 triplet loss 0.902355433 l2_loss 17.9360237 fraction B 0.221708864 lossA 2.35676503 fraction A 0.0510241836\n",
      "step 3138 loss 0.958420873 fisher_loss 0.23683995 triplet loss 0.721580923 l2_loss 17.9432621 fraction B 0.198653966 lossA 2.34604621 fraction A 0.0505645871\n",
      "step 3139 loss 1.11606681 fisher_loss 0.236992985 triplet loss 0.879073858 l2_loss 17.9506226 fraction B 0.291579366 lossA 2.2846818 fraction A 0.0455242731\n",
      "step 3140 loss 1.15737808 fisher_loss 0.236544594 triplet loss 0.920833468 l2_loss 17.956131 fraction B 0.210517243 lossA 2.22739148 fraction A 0.0411795452\n",
      "step 3141 loss 1.25552762 fisher_loss 0.236238956 triplet loss 1.01928866 l2_loss 17.9621468 fraction B 0.212524489 lossA 2.19293213 fraction A 0.0394050293\n",
      "step 3142 loss 1.17507207 fisher_loss 0.236083403 triplet loss 0.938988626 l2_loss 17.9684658 fraction B 0.27020061 lossA 2.1458807 fraction A 0.0364286862\n",
      "step 3143 loss 0.892497182 fisher_loss 0.235721201 triplet loss 0.656775951 l2_loss 17.974308 fraction B 0.129255429 lossA 2.20040035 fraction A 0.0405010208\n",
      "step 3144 loss 0.926118374 fisher_loss 0.23597163 triplet loss 0.690146744 l2_loss 17.9825726 fraction B 0.196053982 lossA 2.33245444 fraction A 0.0527092256\n",
      "step 3145 loss 0.780525804 fisher_loss 0.237093389 triplet loss 0.543432415 l2_loss 17.9938583 fraction B 0.152525529 lossA 2.51642799 fraction A 0.0678286105\n",
      "step 3146 loss 1.01699328 fisher_loss 0.238921136 triplet loss 0.778072119 l2_loss 18.0071621 fraction B 0.255076 lossA 2.59826732 fraction A 0.0732064471\n",
      "step 3147 loss 1.12887418 fisher_loss 0.23988983 triplet loss 0.888984323 l2_loss 18.0178108 fraction B 0.184832275 lossA 2.55565858 fraction A 0.0697656944\n",
      "step 3148 loss 0.929565787 fisher_loss 0.239574865 triplet loss 0.689990938 l2_loss 18.0248985 fraction B 0.195974216 lossA 2.47697639 fraction A 0.0628746152\n",
      "step 3149 loss 0.965569139 fisher_loss 0.238965064 triplet loss 0.726604044 l2_loss 18.0307541 fraction B 0.168633789 lossA 2.40510583 fraction A 0.0547080636\n",
      "step 3150 loss 1.06709659 fisher_loss 0.238425389 triplet loss 0.828671157 l2_loss 18.0359478 fraction B 0.133071154 lossA 2.35478115 fraction A 0.0463432707\n",
      "step 3151 loss 0.970274 fisher_loss 0.237992018 triplet loss 0.732282 l2_loss 18.039957 fraction B 0.221895278 lossA 2.2957232 fraction A 0.0392005183\n",
      "step 3152 loss 0.967414 fisher_loss 0.237505779 triplet loss 0.729908228 l2_loss 18.0433273 fraction B 0.171247587 lossA 2.32752466 fraction A 0.0392915085\n",
      "step 3153 loss 0.982520103 fisher_loss 0.237686113 triplet loss 0.744834 l2_loss 18.0484085 fraction B 0.157915875 lossA 2.42912579 fraction A 0.0449348949\n",
      "step 3154 loss 0.928067684 fisher_loss 0.238256529 triplet loss 0.68981117 l2_loss 18.0560818 fraction B 0.2171451 lossA 2.56508303 fraction A 0.056274876\n",
      "step 3155 loss 1.39684057 fisher_loss 0.239218146 triplet loss 1.15762246 l2_loss 18.0659065 fraction B 0.150321037 lossA 2.64013267 fraction A 0.0634074509\n",
      "step 3156 loss 1.49061608 fisher_loss 0.239497483 triplet loss 1.25111866 l2_loss 18.0744095 fraction B 0.173577502 lossA 2.58488321 fraction A 0.0604559891\n",
      "step 3157 loss 0.950332701 fisher_loss 0.238814965 triplet loss 0.711517751 l2_loss 18.0799618 fraction B 0.136673436 lossA 2.47349429 fraction A 0.0523999929\n",
      "step 3158 loss 0.949657 fisher_loss 0.238121912 triplet loss 0.711535096 l2_loss 18.0858803 fraction B 0.167047 lossA 2.40852642 fraction A 0.0480375141\n",
      "step 3159 loss 1.23542571 fisher_loss 0.238016829 triplet loss 0.997408867 l2_loss 18.0922375 fraction B 0.328983963 lossA 2.38738823 fraction A 0.0468519814\n",
      "step 3160 loss 1.14449918 fisher_loss 0.237923175 triplet loss 0.906576037 l2_loss 18.0977745 fraction B 0.211394563 lossA 2.33940697 fraction A 0.0429681465\n",
      "step 3161 loss 0.99313128 fisher_loss 0.237593099 triplet loss 0.755538166 l2_loss 18.1023331 fraction B 0.265041053 lossA 2.29812765 fraction A 0.039628271\n",
      "step 3162 loss 1.55009818 fisher_loss 0.237462088 triplet loss 1.31263614 l2_loss 18.1069584 fraction B 0.181690723 lossA 2.28580499 fraction A 0.0393382721\n",
      "step 3163 loss 0.843732655 fisher_loss 0.237662971 triplet loss 0.606069684 l2_loss 18.1123238 fraction B 0.145112813 lossA 2.38190174 fraction A 0.0476431921\n",
      "step 3164 loss 1.10504377 fisher_loss 0.239178434 triplet loss 0.86586535 l2_loss 18.1207581 fraction B 0.153406814 lossA 2.48967338 fraction A 0.0569704622\n",
      "step 3165 loss 1.2096771 fisher_loss 0.240983069 triplet loss 0.968694031 l2_loss 18.1295319 fraction B 0.305189401 lossA 2.58945 fraction A 0.0645061582\n",
      "step 3166 loss 1.05560553 fisher_loss 0.242570743 triplet loss 0.813034773 l2_loss 18.1367397 fraction B 0.157559931 lossA 2.60117483 fraction A 0.0654557347\n",
      "step 3167 loss 1.32720709 fisher_loss 0.243099406 triplet loss 1.08410764 l2_loss 18.1428623 fraction B 0.28711763 lossA 2.56792545 fraction A 0.0625063777\n",
      "step 3168 loss 0.947391629 fisher_loss 0.242806435 triplet loss 0.704585195 l2_loss 18.1473789 fraction B 0.147579148 lossA 2.4338665 fraction A 0.0516306944\n",
      "step 3169 loss 0.958839774 fisher_loss 0.241558477 triplet loss 0.717281282 l2_loss 18.1510143 fraction B 0.150014415 lossA 2.3088789 fraction A 0.0417247675\n",
      "step 3170 loss 1.02545083 fisher_loss 0.240415409 triplet loss 0.785035431 l2_loss 18.1544571 fraction B 0.154256418 lossA 2.15232658 fraction A 0.0332673863\n",
      "step 3171 loss 1.19009519 fisher_loss 0.239486128 triplet loss 0.950609 l2_loss 18.1580124 fraction B 0.276417226 lossA 2.06771636 fraction A 0.0298734661\n",
      "step 3172 loss 1.10325432 fisher_loss 0.239134461 triplet loss 0.864119887 l2_loss 18.161314 fraction B 0.258574426 lossA 2.1169641 fraction A 0.031518735\n",
      "step 3173 loss 1.08443689 fisher_loss 0.239271551 triplet loss 0.845165372 l2_loss 18.1657372 fraction B 0.276606292 lossA 2.25559616 fraction A 0.0389257334\n",
      "step 3174 loss 1.18774056 fisher_loss 0.239866599 triplet loss 0.947874 l2_loss 18.1713905 fraction B 0.220642105 lossA 2.43416524 fraction A 0.0539705344\n",
      "step 3175 loss 1.26905262 fisher_loss 0.241398051 triplet loss 1.02765453 l2_loss 18.1787777 fraction B 0.335158944 lossA 2.61876822 fraction A 0.0676645562\n",
      "step 3176 loss 1.16859424 fisher_loss 0.24288936 triplet loss 0.925704896 l2_loss 18.1847382 fraction B 0.255469769 lossA 2.72018147 fraction A 0.0737226903\n",
      "step 3177 loss 1.13523376 fisher_loss 0.243423954 triplet loss 0.891809821 l2_loss 18.188633 fraction B 0.253584445 lossA 2.71544456 fraction A 0.0728958398\n",
      "step 3178 loss 1.06657267 fisher_loss 0.24300696 triplet loss 0.823565662 l2_loss 18.1907768 fraction B 0.192652777 lossA 2.64310169 fraction A 0.0674376562\n",
      "step 3179 loss 1.04299402 fisher_loss 0.24214448 triplet loss 0.800849557 l2_loss 18.1922913 fraction B 0.178409502 lossA 2.515553 fraction A 0.0574064031\n",
      "step 3180 loss 1.14119685 fisher_loss 0.240732372 triplet loss 0.900464475 l2_loss 18.19314 fraction B 0.284470707 lossA 2.39232397 fraction A 0.0460032485\n",
      "step 3181 loss 1.05458558 fisher_loss 0.239246085 triplet loss 0.815339506 l2_loss 18.1925564 fraction B 0.224765822 lossA 2.28553152 fraction A 0.0387691408\n",
      "step 3182 loss 0.844627 fisher_loss 0.238402128 triplet loss 0.606224895 l2_loss 18.1939869 fraction B 0.161369607 lossA 2.24663424 fraction A 0.0369332321\n",
      "step 3183 loss 0.971912444 fisher_loss 0.238121197 triplet loss 0.733791232 l2_loss 18.1988735 fraction B 0.202862516 lossA 2.23583245 fraction A 0.0368195288\n",
      "step 3184 loss 0.993906379 fisher_loss 0.23799926 triplet loss 0.755907118 l2_loss 18.2046547 fraction B 0.192240149 lossA 2.27232909 fraction A 0.039327044\n",
      "step 3185 loss 0.966210186 fisher_loss 0.238285303 triplet loss 0.727924883 l2_loss 18.2114353 fraction B 0.229872629 lossA 2.35356712 fraction A 0.0464166217\n",
      "step 3186 loss 1.18088841 fisher_loss 0.23889266 triplet loss 0.94199574 l2_loss 18.2195 fraction B 0.277730435 lossA 2.39717031 fraction A 0.0515051484\n",
      "step 3187 loss 1.06632233 fisher_loss 0.239285111 triplet loss 0.827037215 l2_loss 18.2260456 fraction B 0.134024367 lossA 2.46292782 fraction A 0.0579859279\n",
      "step 3188 loss 0.970491946 fisher_loss 0.240055755 triplet loss 0.730436206 l2_loss 18.233757 fraction B 0.180451438 lossA 2.49667382 fraction A 0.060956914\n",
      "step 3189 loss 1.16458356 fisher_loss 0.240546629 triplet loss 0.92403692 l2_loss 18.2408962 fraction B 0.243104249 lossA 2.45073724 fraction A 0.0574342944\n",
      "step 3190 loss 1.06494403 fisher_loss 0.240394682 triplet loss 0.824549377 l2_loss 18.2462463 fraction B 0.245140389 lossA 2.36500812 fraction A 0.0502359383\n",
      "step 3191 loss 1.0864166 fisher_loss 0.239847139 triplet loss 0.846569479 l2_loss 18.2504253 fraction B 0.22971414 lossA 2.30824184 fraction A 0.0451758131\n",
      "step 3192 loss 1.03849232 fisher_loss 0.239590719 triplet loss 0.798901558 l2_loss 18.2550831 fraction B 0.185130715 lossA 2.29615736 fraction A 0.0436262786\n",
      "step 3193 loss 1.31415844 fisher_loss 0.239620924 triplet loss 1.07453752 l2_loss 18.260479 fraction B 0.20346193 lossA 2.28051734 fraction A 0.0413963\n",
      "step 3194 loss 1.01487291 fisher_loss 0.239640415 triplet loss 0.775232494 l2_loss 18.2645874 fraction B 0.176449418 lossA 2.31578851 fraction A 0.0426288135\n",
      "step 3195 loss 1.04142261 fisher_loss 0.240002453 triplet loss 0.801420152 l2_loss 18.2691 fraction B 0.236561418 lossA 2.39342785 fraction A 0.0487791\n",
      "step 3196 loss 0.904852092 fisher_loss 0.240740076 triplet loss 0.664112031 l2_loss 18.2752342 fraction B 0.134467125 lossA 2.41441941 fraction A 0.0501629151\n",
      "step 3197 loss 0.983310699 fisher_loss 0.241171375 triplet loss 0.742139339 l2_loss 18.2816257 fraction B 0.21585387 lossA 2.43907237 fraction A 0.0522445254\n",
      "step 3198 loss 1.36294258 fisher_loss 0.241404966 triplet loss 1.12153757 l2_loss 18.2883663 fraction B 0.404665798 lossA 2.45250416 fraction A 0.053071633\n",
      "step 3199 loss 1.25246382 fisher_loss 0.241323277 triplet loss 1.01114058 l2_loss 18.2933502 fraction B 0.216950089 lossA 2.45957112 fraction A 0.0530639887\n",
      "step 3200 loss 1.15632629 fisher_loss 0.241235361 triplet loss 0.915091 l2_loss 18.2973099 fraction B 0.294166714 lossA 2.43566346 fraction A 0.0504457764\n",
      "step 3201 loss 1.37065697 fisher_loss 0.240725622 triplet loss 1.12993133 l2_loss 18.3005848 fraction B 0.161567017 lossA 2.42253232 fraction A 0.0491541848\n",
      "step 3202 loss 0.981303 fisher_loss 0.240273207 triplet loss 0.741029739 l2_loss 18.3046417 fraction B 0.119298324 lossA 2.39213681 fraction A 0.0474131405\n",
      "step 3203 loss 0.99103117 fisher_loss 0.239459 triplet loss 0.751572192 l2_loss 18.3078156 fraction B 0.264251739 lossA 2.34057617 fraction A 0.0440659858\n",
      "step 3204 loss 0.96295917 fisher_loss 0.238715574 triplet loss 0.724243581 l2_loss 18.310915 fraction B 0.183765724 lossA 2.30723619 fraction A 0.0424551629\n",
      "step 3205 loss 1.08393979 fisher_loss 0.238445476 triplet loss 0.84549433 l2_loss 18.3153839 fraction B 0.23222518 lossA 2.29200554 fraction A 0.0422834679\n",
      "step 3206 loss 1.06597328 fisher_loss 0.238357633 triplet loss 0.827615619 l2_loss 18.3200226 fraction B 0.292534858 lossA 2.32649493 fraction A 0.0460213237\n",
      "step 3207 loss 1.00664008 fisher_loss 0.238543719 triplet loss 0.768096328 l2_loss 18.3261604 fraction B 0.171105132 lossA 2.43589091 fraction A 0.0563022271\n",
      "step 3208 loss 0.955882728 fisher_loss 0.239568159 triplet loss 0.716314554 l2_loss 18.3336811 fraction B 0.196214125 lossA 2.52413583 fraction A 0.0633433387\n",
      "step 3209 loss 0.8861413 fisher_loss 0.240880013 triplet loss 0.645261288 l2_loss 18.3408775 fraction B 0.152635604 lossA 2.52693844 fraction A 0.0633920953\n",
      "step 3210 loss 1.4943074 fisher_loss 0.241424605 triplet loss 1.25288284 l2_loss 18.3475266 fraction B 0.206599563 lossA 2.48500872 fraction A 0.0597697161\n",
      "step 3211 loss 0.961979747 fisher_loss 0.241406918 triplet loss 0.720572829 l2_loss 18.3515606 fraction B 0.124825045 lossA 2.42770743 fraction A 0.0550600775\n",
      "step 3212 loss 1.05376351 fisher_loss 0.241303593 triplet loss 0.812459886 l2_loss 18.356678 fraction B 0.142077699 lossA 2.33801556 fraction A 0.0472426042\n",
      "step 3213 loss 1.24114287 fisher_loss 0.241226 triplet loss 0.999916911 l2_loss 18.3624134 fraction B 0.155189037 lossA 2.26225138 fraction A 0.0415639393\n",
      "step 3214 loss 1.12175202 fisher_loss 0.241074443 triplet loss 0.880677521 l2_loss 18.3680286 fraction B 0.275312275 lossA 2.199754 fraction A 0.0381596573\n",
      "step 3215 loss 1.03957891 fisher_loss 0.240812764 triplet loss 0.798766196 l2_loss 18.3738 fraction B 0.216927767 lossA 2.19293594 fraction A 0.0385619849\n",
      "step 3216 loss 1.18752742 fisher_loss 0.24078466 triplet loss 0.946742773 l2_loss 18.3797512 fraction B 0.143660113 lossA 2.27878356 fraction A 0.0466627888\n",
      "step 3217 loss 1.05613327 fisher_loss 0.241510451 triplet loss 0.81462276 l2_loss 18.387331 fraction B 0.176635548 lossA 2.38869953 fraction A 0.0564770736\n",
      "step 3218 loss 1.14056098 fisher_loss 0.24227713 triplet loss 0.898283839 l2_loss 18.3948669 fraction B 0.217260674 lossA 2.49354839 fraction A 0.0642525628\n",
      "step 3219 loss 0.75617981 fisher_loss 0.242670372 triplet loss 0.513509452 l2_loss 18.40131 fraction B 0.108033299 lossA 2.54466987 fraction A 0.0669530332\n",
      "step 3220 loss 1.25950634 fisher_loss 0.24266243 triplet loss 1.01684391 l2_loss 18.4063492 fraction B 0.25804162 lossA 2.53889751 fraction A 0.064938657\n",
      "step 3221 loss 1.26215363 fisher_loss 0.242000446 triplet loss 1.02015316 l2_loss 18.4091358 fraction B 0.132658646 lossA 2.4806025 fraction A 0.0586924143\n",
      "step 3222 loss 1.19871783 fisher_loss 0.240838259 triplet loss 0.957879543 l2_loss 18.4102917 fraction B 0.3150267 lossA 2.40215564 fraction A 0.050884217\n",
      "step 3223 loss 1.10146379 fisher_loss 0.239470884 triplet loss 0.861992896 l2_loss 18.4108639 fraction B 0.227290899 lossA 2.3313756 fraction A 0.0437461734\n",
      "step 3224 loss 1.04289424 fisher_loss 0.238169879 triplet loss 0.804724336 l2_loss 18.412426 fraction B 0.239636779 lossA 2.3204689 fraction A 0.0421332493\n",
      "step 3225 loss 0.889638305 fisher_loss 0.237393945 triplet loss 0.652244389 l2_loss 18.4157066 fraction B 0.165410355 lossA 2.35444665 fraction A 0.0445551723\n",
      "step 3226 loss 1.09166515 fisher_loss 0.237059876 triplet loss 0.854605258 l2_loss 18.4208927 fraction B 0.157953739 lossA 2.41112399 fraction A 0.0504440032\n",
      "step 3227 loss 1.21348548 fisher_loss 0.237107694 triplet loss 0.976377785 l2_loss 18.4289761 fraction B 0.220617592 lossA 2.44021034 fraction A 0.0537164696\n",
      "step 3228 loss 1.09865105 fisher_loss 0.237060472 triplet loss 0.861590624 l2_loss 18.4360867 fraction B 0.293228507 lossA 2.43721795 fraction A 0.0539481118\n",
      "step 3229 loss 1.22924638 fisher_loss 0.236918926 triplet loss 0.992327452 l2_loss 18.4424019 fraction B 0.31239596 lossA 2.40337253 fraction A 0.0510938\n",
      "step 3230 loss 1.09714508 fisher_loss 0.23658222 triplet loss 0.860562801 l2_loss 18.4464836 fraction B 0.236534342 lossA 2.3914032 fraction A 0.0500715263\n",
      "step 3231 loss 0.941094041 fisher_loss 0.236427397 triplet loss 0.704666615 l2_loss 18.4505825 fraction B 0.167764917 lossA 2.3963275 fraction A 0.0499436222\n",
      "step 3232 loss 0.829571545 fisher_loss 0.236650452 triplet loss 0.592921078 l2_loss 18.4552364 fraction B 0.0850125402 lossA 2.40353346 fraction A 0.0507460274\n",
      "step 3233 loss 1.17186248 fisher_loss 0.237401545 triplet loss 0.934460938 l2_loss 18.4612064 fraction B 0.158269435 lossA 2.40912747 fraction A 0.0518147424\n",
      "step 3234 loss 0.955925405 fisher_loss 0.238088 triplet loss 0.717837393 l2_loss 18.4671898 fraction B 0.124692388 lossA 2.4898932 fraction A 0.0590805411\n",
      "step 3235 loss 1.02916789 fisher_loss 0.239312947 triplet loss 0.789855 l2_loss 18.474514 fraction B 0.131806076 lossA 2.54338574 fraction A 0.0645514\n",
      "step 3236 loss 1.17785048 fisher_loss 0.240454286 triplet loss 0.937396169 l2_loss 18.481781 fraction B 0.116623886 lossA 2.52459335 fraction A 0.0636834279\n",
      "step 3237 loss 1.15231574 fisher_loss 0.240965858 triplet loss 0.911349833 l2_loss 18.487299 fraction B 0.251669616 lossA 2.49965382 fraction A 0.0618747585\n",
      "step 3238 loss 0.931075931 fisher_loss 0.241251647 triplet loss 0.689824283 l2_loss 18.4918251 fraction B 0.19676362 lossA 2.352566 fraction A 0.0494758449\n",
      "step 3239 loss 0.914758444 fisher_loss 0.240522161 triplet loss 0.674236298 l2_loss 18.4951305 fraction B 0.14649789 lossA 2.26500702 fraction A 0.0415388383\n",
      "step 3240 loss 1.18547606 fisher_loss 0.240315989 triplet loss 0.945160031 l2_loss 18.4997902 fraction B 0.271799207 lossA 2.24809027 fraction A 0.0397794656\n",
      "step 3241 loss 1.15743327 fisher_loss 0.240447074 triplet loss 0.916986227 l2_loss 18.5048637 fraction B 0.222973794 lossA 2.24970293 fraction A 0.0393759795\n",
      "step 3242 loss 0.917319536 fisher_loss 0.240340084 triplet loss 0.676979482 l2_loss 18.5094261 fraction B 0.137329161 lossA 2.25370455 fraction A 0.0397053547\n",
      "step 3243 loss 0.872266173 fisher_loss 0.240292847 triplet loss 0.631973326 l2_loss 18.5150318 fraction B 0.180264771 lossA 2.24183893 fraction A 0.039225474\n",
      "step 3244 loss 0.920401335 fisher_loss 0.239991933 triplet loss 0.680409431 l2_loss 18.5210876 fraction B 0.175719798 lossA 2.27209401 fraction A 0.0419053026\n",
      "step 3245 loss 1.20060086 fisher_loss 0.240077674 triplet loss 0.960523248 l2_loss 18.5284786 fraction B 0.101113543 lossA 2.35638881 fraction A 0.0496095046\n",
      "step 3246 loss 1.15173054 fisher_loss 0.240941167 triplet loss 0.910789371 l2_loss 18.5372887 fraction B 0.116502114 lossA 2.36119294 fraction A 0.0501339361\n",
      "step 3247 loss 1.07098472 fisher_loss 0.241286665 triplet loss 0.829698 l2_loss 18.5431671 fraction B 0.23835656 lossA 2.34871912 fraction A 0.0488515086\n",
      "step 3248 loss 1.22967315 fisher_loss 0.241428062 triplet loss 0.98824513 l2_loss 18.5485649 fraction B 0.215668097 lossA 2.34774065 fraction A 0.047292158\n",
      "step 3249 loss 0.869142354 fisher_loss 0.241129473 triplet loss 0.628012896 l2_loss 18.5517559 fraction B 0.114291757 lossA 2.39059424 fraction A 0.0488042\n",
      "step 3250 loss 1.15416658 fisher_loss 0.241002336 triplet loss 0.913164198 l2_loss 18.5551262 fraction B 0.166426212 lossA 2.47318244 fraction A 0.0532207973\n",
      "step 3251 loss 1.1758101 fisher_loss 0.241069376 triplet loss 0.934740663 l2_loss 18.5583496 fraction B 0.163587 lossA 2.57137036 fraction A 0.0580951385\n",
      "step 3252 loss 1.12144816 fisher_loss 0.241429314 triplet loss 0.88001883 l2_loss 18.5625267 fraction B 0.177752778 lossA 2.60041094 fraction A 0.0582896918\n",
      "step 3253 loss 1.1463939 fisher_loss 0.241413563 triplet loss 0.904980302 l2_loss 18.566061 fraction B 0.233363047 lossA 2.59532905 fraction A 0.0554521158\n",
      "step 3254 loss 1.0231961 fisher_loss 0.241199747 triplet loss 0.781996369 l2_loss 18.5685062 fraction B 0.203409329 lossA 2.60416627 fraction A 0.0549486913\n",
      "step 3255 loss 1.03594756 fisher_loss 0.240924224 triplet loss 0.795023322 l2_loss 18.572279 fraction B 0.256155789 lossA 2.56792498 fraction A 0.0518227853\n",
      "step 3256 loss 1.06434226 fisher_loss 0.240140796 triplet loss 0.824201524 l2_loss 18.5762749 fraction B 0.201586157 lossA 2.51785064 fraction A 0.0489270687\n",
      "step 3257 loss 1.19810438 fisher_loss 0.238873765 triplet loss 0.959230661 l2_loss 18.5814934 fraction B 0.266169697 lossA 2.5074029 fraction A 0.0496060289\n",
      "step 3258 loss 1.35982764 fisher_loss 0.237878427 triplet loss 1.1219492 l2_loss 18.5876961 fraction B 0.252129078 lossA 2.50687456 fraction A 0.0516032726\n",
      "step 3259 loss 1.16467762 fisher_loss 0.236906648 triplet loss 0.927771032 l2_loss 18.5938683 fraction B 0.143693194 lossA 2.4486711 fraction A 0.0496161\n",
      "step 3260 loss 1.17310441 fisher_loss 0.235849097 triplet loss 0.937255263 l2_loss 18.6002178 fraction B 0.264801085 lossA 2.40377426 fraction A 0.0479518808\n",
      "step 3261 loss 1.0099045 fisher_loss 0.235108778 triplet loss 0.774795771 l2_loss 18.6058502 fraction B 0.204818532 lossA 2.3898654 fraction A 0.0489636548\n",
      "step 3262 loss 0.92547375 fisher_loss 0.234956145 triplet loss 0.690517604 l2_loss 18.6116199 fraction B 0.178698987 lossA 2.37261939 fraction A 0.0490471832\n",
      "step 3263 loss 0.963912606 fisher_loss 0.234891504 triplet loss 0.729021072 l2_loss 18.6164837 fraction B 0.19926919 lossA 2.33364081 fraction A 0.046247676\n",
      "step 3264 loss 1.04340577 fisher_loss 0.234698892 triplet loss 0.80870688 l2_loss 18.6202374 fraction B 0.195973858 lossA 2.28646135 fraction A 0.0419350415\n",
      "step 3265 loss 1.43516123 fisher_loss 0.234614938 triplet loss 1.20054626 l2_loss 18.6236191 fraction B 0.215076044 lossA 2.2325983 fraction A 0.0378246345\n",
      "step 3266 loss 0.999454618 fisher_loss 0.234695464 triplet loss 0.764759183 l2_loss 18.6269131 fraction B 0.241801634 lossA 2.18945909 fraction A 0.0351319872\n",
      "step 3267 loss 1.03783727 fisher_loss 0.234892905 triplet loss 0.802944303 l2_loss 18.6316872 fraction B 0.211980686 lossA 2.22404718 fraction A 0.0367837027\n",
      "step 3268 loss 1.04216099 fisher_loss 0.235517725 triplet loss 0.806643307 l2_loss 18.6371708 fraction B 0.224331 lossA 2.30993056 fraction A 0.0419822372\n",
      "step 3269 loss 1.01875579 fisher_loss 0.236295179 triplet loss 0.78246057 l2_loss 18.6428967 fraction B 0.204911068 lossA 2.40678358 fraction A 0.0497547202\n",
      "step 3270 loss 1.07723618 fisher_loss 0.237288207 triplet loss 0.839948 l2_loss 18.6494617 fraction B 0.221321672 lossA 2.48916578 fraction A 0.0559255816\n",
      "step 3271 loss 1.05775368 fisher_loss 0.237958103 triplet loss 0.819795549 l2_loss 18.6553574 fraction B 0.245637581 lossA 2.54735875 fraction A 0.060355328\n",
      "step 3272 loss 1.10957348 fisher_loss 0.238382801 triplet loss 0.871190727 l2_loss 18.6611404 fraction B 0.190869436 lossA 2.54270649 fraction A 0.0601504892\n",
      "step 3273 loss 1.08280635 fisher_loss 0.237996385 triplet loss 0.844809949 l2_loss 18.6659603 fraction B 0.335320055 lossA 2.46053338 fraction A 0.053761024\n",
      "step 3274 loss 0.961012244 fisher_loss 0.236831948 triplet loss 0.724180281 l2_loss 18.6689529 fraction B 0.168267 lossA 2.42628288 fraction A 0.0510118306\n",
      "step 3275 loss 0.895436525 fisher_loss 0.236277118 triplet loss 0.659159422 l2_loss 18.6734543 fraction B 0.162857145 lossA 2.46192169 fraction A 0.0541266203\n",
      "step 3276 loss 1.0865227 fisher_loss 0.236656174 triplet loss 0.849866509 l2_loss 18.6799011 fraction B 0.260649592 lossA 2.48877716 fraction A 0.0559890792\n",
      "step 3277 loss 1.05781317 fisher_loss 0.236910179 triplet loss 0.820902944 l2_loss 18.685606 fraction B 0.273129553 lossA 2.46983933 fraction A 0.0540330186\n",
      "step 3278 loss 1.23009777 fisher_loss 0.236860037 triplet loss 0.993237734 l2_loss 18.6903286 fraction B 0.197210148 lossA 2.44529438 fraction A 0.050735414\n",
      "step 3279 loss 1.05082369 fisher_loss 0.236543134 triplet loss 0.81428057 l2_loss 18.6939487 fraction B 0.231448904 lossA 2.4684267 fraction A 0.0518874414\n",
      "step 3280 loss 1.03878939 fisher_loss 0.236718431 triplet loss 0.802071 l2_loss 18.698719 fraction B 0.179317713 lossA 2.54697466 fraction A 0.0576009192\n",
      "step 3281 loss 1.39932323 fisher_loss 0.237472326 triplet loss 1.16185093 l2_loss 18.7050953 fraction B 0.233667165 lossA 2.58737183 fraction A 0.0602342337\n",
      "step 3282 loss 1.1605736 fisher_loss 0.237702623 triplet loss 0.922871 l2_loss 18.7096634 fraction B 0.226262584 lossA 2.58967614 fraction A 0.0610694587\n",
      "step 3283 loss 1.13946402 fisher_loss 0.237538159 triplet loss 0.901925862 l2_loss 18.714386 fraction B 0.207411602 lossA 2.58710074 fraction A 0.0616531484\n",
      "step 3284 loss 0.955105305 fisher_loss 0.237329155 triplet loss 0.717776179 l2_loss 18.719202 fraction B 0.178968683 lossA 2.52206302 fraction A 0.0575764328\n",
      "step 3285 loss 1.02584493 fisher_loss 0.236437902 triplet loss 0.789407074 l2_loss 18.7239494 fraction B 0.233508214 lossA 2.45982885 fraction A 0.0536355823\n",
      "step 3286 loss 1.33397949 fisher_loss 0.235581562 triplet loss 1.09839797 l2_loss 18.7288284 fraction B 0.14913933 lossA 2.39595246 fraction A 0.0491222\n",
      "step 3287 loss 1.15516961 fisher_loss 0.234936267 triplet loss 0.920233369 l2_loss 18.7345238 fraction B 0.212433428 lossA 2.37321258 fraction A 0.0471373796\n",
      "step 3288 loss 0.936914921 fisher_loss 0.234491497 triplet loss 0.702423394 l2_loss 18.7402 fraction B 0.164444447 lossA 2.36817598 fraction A 0.0475497022\n",
      "step 3289 loss 1.09952676 fisher_loss 0.234304294 triplet loss 0.865222454 l2_loss 18.7472973 fraction B 0.179734886 lossA 2.34796405 fraction A 0.0461283959\n",
      "step 3290 loss 1.01713729 fisher_loss 0.23417525 triplet loss 0.782962084 l2_loss 18.7529926 fraction B 0.235193893 lossA 2.3612237 fraction A 0.0482376069\n",
      "step 3291 loss 1.3964535 fisher_loss 0.234338254 triplet loss 1.16211522 l2_loss 18.7587814 fraction B 0.291656017 lossA 2.35521078 fraction A 0.0482039191\n",
      "step 3292 loss 0.934370875 fisher_loss 0.234064311 triplet loss 0.700306535 l2_loss 18.7627316 fraction B 0.243170217 lossA 2.3258338 fraction A 0.0462636501\n",
      "step 3293 loss 0.956433296 fisher_loss 0.233823344 triplet loss 0.722609937 l2_loss 18.7662354 fraction B 0.164209813 lossA 2.34789062 fraction A 0.0491489321\n",
      "step 3294 loss 1.11645865 fisher_loss 0.234055579 triplet loss 0.882403 l2_loss 18.771452 fraction B 0.27812621 lossA 2.33464837 fraction A 0.0483865924\n",
      "step 3295 loss 0.992407143 fisher_loss 0.233732477 triplet loss 0.758674681 l2_loss 18.7749348 fraction B 0.144975185 lossA 2.32256365 fraction A 0.047456827\n",
      "step 3296 loss 1.32604122 fisher_loss 0.233650759 triplet loss 1.09239042 l2_loss 18.7793217 fraction B 0.259735554 lossA 2.37324643 fraction A 0.0524345115\n",
      "step 3297 loss 1.11869025 fisher_loss 0.233900413 triplet loss 0.884789884 l2_loss 18.7842102 fraction B 0.275198 lossA 2.41138768 fraction A 0.055616565\n",
      "step 3298 loss 1.07713675 fisher_loss 0.233972982 triplet loss 0.843163729 l2_loss 18.787859 fraction B 0.237047 lossA 2.41305065 fraction A 0.0543751083\n",
      "step 3299 loss 0.863463104 fisher_loss 0.233541325 triplet loss 0.629921794 l2_loss 18.7904491 fraction B 0.11084491 lossA 2.38339257 fraction A 0.0495856702\n",
      "step 3300 loss 1.19213533 fisher_loss 0.23313266 triplet loss 0.959002674 l2_loss 18.7935524 fraction B 0.232850969 lossA 2.3092258 fraction A 0.0418119915\n",
      "step 3301 loss 0.983297825 fisher_loss 0.232506856 triplet loss 0.750790954 l2_loss 18.7955647 fraction B 0.223576382 lossA 2.25784755 fraction A 0.037382137\n",
      "step 3302 loss 1.00081468 fisher_loss 0.23209545 triplet loss 0.768719196 l2_loss 18.7979431 fraction B 0.231017977 lossA 2.23960781 fraction A 0.035618674\n",
      "step 3303 loss 1.0507766 fisher_loss 0.231946588 triplet loss 0.81883 l2_loss 18.8012352 fraction B 0.342120379 lossA 2.25869 fraction A 0.0362940468\n",
      "step 3304 loss 1.04732656 fisher_loss 0.232008189 triplet loss 0.815318406 l2_loss 18.8056927 fraction B 0.23731868 lossA 2.34243679 fraction A 0.0415180102\n",
      "step 3305 loss 0.931359172 fisher_loss 0.232434183 triplet loss 0.698924959 l2_loss 18.8123989 fraction B 0.19617334 lossA 2.40366983 fraction A 0.0476702861\n",
      "step 3306 loss 1.01638889 fisher_loss 0.232853651 triplet loss 0.783535302 l2_loss 18.8199654 fraction B 0.169256687 lossA 2.43779731 fraction A 0.0514731966\n",
      "step 3307 loss 1.00062919 fisher_loss 0.233218789 triplet loss 0.767410338 l2_loss 18.827177 fraction B 0.194486246 lossA 2.39097786 fraction A 0.04795257\n",
      "step 3308 loss 1.12606192 fisher_loss 0.233011812 triplet loss 0.893050075 l2_loss 18.8332138 fraction B 0.217110649 lossA 2.33597302 fraction A 0.0451623388\n",
      "step 3309 loss 1.13316083 fisher_loss 0.232987657 triplet loss 0.900173128 l2_loss 18.8403492 fraction B 0.206427336 lossA 2.29134464 fraction A 0.043240577\n",
      "step 3310 loss 0.993180275 fisher_loss 0.233297095 triplet loss 0.759883165 l2_loss 18.8483505 fraction B 0.225636765 lossA 2.25813413 fraction A 0.0421298817\n",
      "step 3311 loss 0.998959184 fisher_loss 0.233851343 triplet loss 0.76510781 l2_loss 18.8564034 fraction B 0.213510513 lossA 2.29880834 fraction A 0.0466488078\n",
      "step 3312 loss 1.13025546 fisher_loss 0.234986424 triplet loss 0.895269036 l2_loss 18.8650188 fraction B 0.194387749 lossA 2.35946798 fraction A 0.0527391434\n",
      "step 3313 loss 1.22200167 fisher_loss 0.236209318 triplet loss 0.985792398 l2_loss 18.873455 fraction B 0.280179977 lossA 2.43250728 fraction A 0.0598324873\n",
      "step 3314 loss 1.22876084 fisher_loss 0.237366721 triplet loss 0.991394103 l2_loss 18.8812981 fraction B 0.232689455 lossA 2.44413567 fraction A 0.0609682165\n",
      "step 3315 loss 1.22730446 fisher_loss 0.237730905 triplet loss 0.989573598 l2_loss 18.8872051 fraction B 0.260932118 lossA 2.42288733 fraction A 0.058788877\n",
      "step 3316 loss 0.974825621 fisher_loss 0.237496138 triplet loss 0.737329483 l2_loss 18.8911781 fraction B 0.09306 lossA 2.35490012 fraction A 0.0523978919\n",
      "step 3317 loss 1.13924217 fisher_loss 0.236560345 triplet loss 0.902681768 l2_loss 18.8948765 fraction B 0.264201 lossA 2.32929921 fraction A 0.0488498807\n",
      "step 3318 loss 1.06079841 fisher_loss 0.235717431 triplet loss 0.825081 l2_loss 18.8979816 fraction B 0.162469819 lossA 2.30910945 fraction A 0.0458201058\n",
      "step 3319 loss 1.1148541 fisher_loss 0.235050365 triplet loss 0.879803777 l2_loss 18.9009323 fraction B 0.232411623 lossA 2.31166148 fraction A 0.0438649133\n",
      "step 3320 loss 1.10509765 fisher_loss 0.234479055 triplet loss 0.870618582 l2_loss 18.902792 fraction B 0.237875342 lossA 2.32350755 fraction A 0.0430057123\n",
      "step 3321 loss 0.933650792 fisher_loss 0.234095812 triplet loss 0.699555 l2_loss 18.9051113 fraction B 0.188162327 lossA 2.35110569 fraction A 0.0434583127\n",
      "step 3322 loss 1.09972072 fisher_loss 0.234055042 triplet loss 0.865665674 l2_loss 18.9085808 fraction B 0.266275615 lossA 2.3637991 fraction A 0.0434579141\n",
      "step 3323 loss 1.0486145 fisher_loss 0.233955398 triplet loss 0.814659119 l2_loss 18.9116783 fraction B 0.231766447 lossA 2.40761185 fraction A 0.0471114814\n",
      "step 3324 loss 1.04696095 fisher_loss 0.233931929 triplet loss 0.813029051 l2_loss 18.9167595 fraction B 0.160167709 lossA 2.46276331 fraction A 0.0527683422\n",
      "step 3325 loss 1.04465449 fisher_loss 0.234017387 triplet loss 0.810637116 l2_loss 18.9223118 fraction B 0.190528721 lossA 2.50164437 fraction A 0.0567164682\n",
      "step 3326 loss 1.00146079 fisher_loss 0.233905688 triplet loss 0.767555058 l2_loss 18.92836 fraction B 0.109375626 lossA 2.53259182 fraction A 0.0599249639\n",
      "step 3327 loss 1.07867873 fisher_loss 0.233640194 triplet loss 0.845038533 l2_loss 18.9349594 fraction B 0.191715926 lossA 2.53945494 fraction A 0.0608881637\n",
      "step 3328 loss 1.11176598 fisher_loss 0.233318195 triplet loss 0.878447771 l2_loss 18.9405499 fraction B 0.146167234 lossA 2.55585718 fraction A 0.062460117\n",
      "step 3329 loss 0.980905294 fisher_loss 0.233174801 triplet loss 0.747730494 l2_loss 18.9469128 fraction B 0.162676111 lossA 2.54201794 fraction A 0.061953254\n",
      "step 3330 loss 0.956290364 fisher_loss 0.233094916 triplet loss 0.723195434 l2_loss 18.9533844 fraction B 0.210493311 lossA 2.46768236 fraction A 0.0562035926\n",
      "step 3331 loss 1.35354185 fisher_loss 0.232657865 triplet loss 1.12088394 l2_loss 18.9586868 fraction B 0.288227201 lossA 2.37860608 fraction A 0.0489735417\n",
      "step 3332 loss 0.873031616 fisher_loss 0.232025474 triplet loss 0.641006112 l2_loss 18.9626617 fraction B 0.14153564 lossA 2.30637836 fraction A 0.0421707779\n",
      "step 3333 loss 1.06287062 fisher_loss 0.231755495 triplet loss 0.831115127 l2_loss 18.9669971 fraction B 0.170056075 lossA 2.29533911 fraction A 0.0409513414\n",
      "step 3334 loss 1.5438838 fisher_loss 0.231998041 triplet loss 1.31188571 l2_loss 18.9724388 fraction B 0.20142661 lossA 2.304075 fraction A 0.0417802967\n",
      "step 3335 loss 0.973449826 fisher_loss 0.2322478 triplet loss 0.741202056 l2_loss 18.9775524 fraction B 0.163160071 lossA 2.40578651 fraction A 0.0512419529\n",
      "step 3336 loss 0.970147252 fisher_loss 0.233374938 triplet loss 0.736772299 l2_loss 18.9852142 fraction B 0.241565 lossA 2.50445366 fraction A 0.0606116764\n",
      "step 3337 loss 0.995417953 fisher_loss 0.234505296 triplet loss 0.760912657 l2_loss 18.9929829 fraction B 0.145614028 lossA 2.59583521 fraction A 0.068735376\n",
      "step 3338 loss 1.07892179 fisher_loss 0.2357281 triplet loss 0.84319365 l2_loss 19.0010357 fraction B 0.185988277 lossA 2.54823065 fraction A 0.0658570081\n",
      "step 3339 loss 0.879762828 fisher_loss 0.235845491 triplet loss 0.643917322 l2_loss 19.0067234 fraction B 0.126877934 lossA 2.40728927 fraction A 0.0551831611\n",
      "step 3340 loss 0.883276939 fisher_loss 0.235379636 triplet loss 0.647897303 l2_loss 19.012743 fraction B 0.159548685 lossA 2.25508833 fraction A 0.0411408208\n",
      "step 3341 loss 1.15656519 fisher_loss 0.234684348 triplet loss 0.921880782 l2_loss 19.0179672 fraction B 0.268242896 lossA 2.14581895 fraction A 0.0333873183\n",
      "step 3342 loss 0.976100206 fisher_loss 0.234493256 triplet loss 0.741606951 l2_loss 19.0233135 fraction B 0.26567322 lossA 2.06008601 fraction A 0.0292318892\n",
      "step 3343 loss 0.950359702 fisher_loss 0.234575287 triplet loss 0.715784431 l2_loss 19.0291119 fraction B 0.20933044 lossA 2.19225812 fraction A 0.0347666442\n",
      "step 3344 loss 1.06954241 fisher_loss 0.235200047 triplet loss 0.83434236 l2_loss 19.0360794 fraction B 0.252299458 lossA 2.32623839 fraction A 0.0448290147\n",
      "step 3345 loss 1.04458368 fisher_loss 0.236236021 triplet loss 0.808347702 l2_loss 19.0438728 fraction B 0.285437316 lossA 2.46941948 fraction A 0.0569425337\n",
      "step 3346 loss 1.05937624 fisher_loss 0.237678409 triplet loss 0.821697772 l2_loss 19.0519924 fraction B 0.130964175 lossA 2.56905484 fraction A 0.0644417927\n",
      "step 3347 loss 1.16997445 fisher_loss 0.23853375 triplet loss 0.931440651 l2_loss 19.0587044 fraction B 0.213972434 lossA 2.60839677 fraction A 0.0668715686\n",
      "step 3348 loss 1.3051827 fisher_loss 0.238690659 triplet loss 1.06649208 l2_loss 19.0637093 fraction B 0.184464052 lossA 2.59575987 fraction A 0.0652184039\n",
      "step 3349 loss 0.990411758 fisher_loss 0.23821488 triplet loss 0.752196908 l2_loss 19.0670986 fraction B 0.174123079 lossA 2.53008199 fraction A 0.0593269281\n",
      "step 3350 loss 1.15595448 fisher_loss 0.237208039 triplet loss 0.918746412 l2_loss 19.0698013 fraction B 0.263241559 lossA 2.4385004 fraction A 0.0515554585\n",
      "step 3351 loss 0.975983143 fisher_loss 0.235941648 triplet loss 0.740041494 l2_loss 19.0718822 fraction B 0.239106983 lossA 2.35460663 fraction A 0.0435581431\n",
      "step 3352 loss 1.18260801 fisher_loss 0.234860703 triplet loss 0.94774735 l2_loss 19.0749111 fraction B 0.214387134 lossA 2.32081532 fraction A 0.0402164534\n",
      "step 3353 loss 1.11916578 fisher_loss 0.234217733 triplet loss 0.884948 l2_loss 19.0787449 fraction B 0.25994 lossA 2.31400347 fraction A 0.0389450751\n",
      "step 3354 loss 1.10086441 fisher_loss 0.23371467 triplet loss 0.86714977 l2_loss 19.0815125 fraction B 0.23078832 lossA 2.35525727 fraction A 0.0415698811\n",
      "step 3355 loss 1.04884148 fisher_loss 0.233715221 triplet loss 0.81512624 l2_loss 19.0850792 fraction B 0.234843373 lossA 2.43600535 fraction A 0.0472108\n",
      "step 3356 loss 0.93014735 fisher_loss 0.234203413 triplet loss 0.695943952 l2_loss 19.089386 fraction B 0.154912204 lossA 2.50755095 fraction A 0.0529104061\n",
      "step 3357 loss 1.121032 fisher_loss 0.235022292 triplet loss 0.886009753 l2_loss 19.0949955 fraction B 0.154028624 lossA 2.60173535 fraction A 0.0588860624\n",
      "step 3358 loss 0.978514493 fisher_loss 0.236024633 triplet loss 0.742489874 l2_loss 19.1004314 fraction B 0.193914279 lossA 2.6691308 fraction A 0.0636574253\n",
      "step 3359 loss 1.14418375 fisher_loss 0.237191603 triplet loss 0.906992197 l2_loss 19.106678 fraction B 0.24847354 lossA 2.67490745 fraction A 0.0643129423\n",
      "step 3360 loss 1.05000472 fisher_loss 0.237772211 triplet loss 0.812232554 l2_loss 19.1125088 fraction B 0.15040651 lossA 2.64682603 fraction A 0.0632038787\n",
      "step 3361 loss 1.06066108 fisher_loss 0.238017917 triplet loss 0.822643161 l2_loss 19.1194115 fraction B 0.203126982 lossA 2.57240438 fraction A 0.0594238602\n",
      "step 3362 loss 1.30862164 fisher_loss 0.237727314 triplet loss 1.07089436 l2_loss 19.1259689 fraction B 0.193110466 lossA 2.48837113 fraction A 0.0552569851\n",
      "step 3363 loss 1.00802493 fisher_loss 0.237197936 triplet loss 0.770826936 l2_loss 19.1319714 fraction B 0.132227659 lossA 2.42219472 fraction A 0.0511320531\n",
      "step 3364 loss 1.01437593 fisher_loss 0.236958414 triplet loss 0.777417541 l2_loss 19.1394901 fraction B 0.236315876 lossA 2.30728936 fraction A 0.0424938835\n",
      "step 3365 loss 0.972697496 fisher_loss 0.236581117 triplet loss 0.73611635 l2_loss 19.1455746 fraction B 0.192178845 lossA 2.24164534 fraction A 0.0379266031\n",
      "step 3366 loss 1.05074835 fisher_loss 0.236785769 triplet loss 0.813962638 l2_loss 19.1524506 fraction B 0.229729727 lossA 2.23118734 fraction A 0.0378029384\n",
      "step 3367 loss 1.04596269 fisher_loss 0.237119913 triplet loss 0.808842778 l2_loss 19.1592464 fraction B 0.312266111 lossA 2.23921037 fraction A 0.0389646702\n",
      "step 3368 loss 0.941531718 fisher_loss 0.237482861 triplet loss 0.704048872 l2_loss 19.1658478 fraction B 0.189155564 lossA 2.29555321 fraction A 0.0445173196\n",
      "step 3369 loss 1.15025294 fisher_loss 0.238132372 triplet loss 0.912120581 l2_loss 19.1728878 fraction B 0.229404181 lossA 2.36674714 fraction A 0.050545752\n",
      "step 3370 loss 0.977818191 fisher_loss 0.238688663 triplet loss 0.739129543 l2_loss 19.1787872 fraction B 0.157709345 lossA 2.48745823 fraction A 0.0597361\n",
      "step 3371 loss 1.25988352 fisher_loss 0.239737079 triplet loss 1.02014649 l2_loss 19.1849976 fraction B 0.219107062 lossA 2.58317 fraction A 0.0655945763\n",
      "step 3372 loss 1.35019934 fisher_loss 0.240594402 triplet loss 1.10960495 l2_loss 19.1900749 fraction B 0.157488167 lossA 2.5719285 fraction A 0.0636021867\n",
      "step 3373 loss 1.25415695 fisher_loss 0.240574688 triplet loss 1.01358223 l2_loss 19.193512 fraction B 0.288116455 lossA 2.50036192 fraction A 0.0566904955\n",
      "step 3374 loss 0.980841041 fisher_loss 0.239798248 triplet loss 0.741042793 l2_loss 19.1953278 fraction B 0.126631379 lossA 2.42686749 fraction A 0.0498424172\n",
      "step 3375 loss 1.14568627 fisher_loss 0.239075586 triplet loss 0.906610727 l2_loss 19.1988468 fraction B 0.15646258 lossA 2.41828036 fraction A 0.0480161794\n",
      "step 3376 loss 1.13269365 fisher_loss 0.238814697 triplet loss 0.893878937 l2_loss 19.2024326 fraction B 0.182887793 lossA 2.40931535 fraction A 0.0467177741\n",
      "step 3377 loss 1.23581421 fisher_loss 0.238549799 triplet loss 0.997264385 l2_loss 19.2071285 fraction B 0.191822186 lossA 2.43918276 fraction A 0.049841404\n",
      "step 3378 loss 0.971158743 fisher_loss 0.238519281 triplet loss 0.732639492 l2_loss 19.2130489 fraction B 0.146041691 lossA 2.43473172 fraction A 0.0501720794\n",
      "step 3379 loss 1.16218209 fisher_loss 0.238285467 triplet loss 0.92389667 l2_loss 19.2198715 fraction B 0.23711209 lossA 2.40112662 fraction A 0.0480499752\n",
      "step 3380 loss 0.907462239 fisher_loss 0.237727225 triplet loss 0.669735 l2_loss 19.2246952 fraction B 0.13681829 lossA 2.38274598 fraction A 0.0467985533\n",
      "step 3381 loss 1.00717056 fisher_loss 0.237175122 triplet loss 0.769995391 l2_loss 19.2296066 fraction B 0.156848088 lossA 2.37910533 fraction A 0.0466964766\n",
      "step 3382 loss 0.889066458 fisher_loss 0.23672691 triplet loss 0.652339518 l2_loss 19.2353706 fraction B 0.119365677 lossA 2.35771179 fraction A 0.0451415107\n",
      "step 3383 loss 1.03356647 fisher_loss 0.236105263 triplet loss 0.797461212 l2_loss 19.241951 fraction B 0.232600734 lossA 2.37452602 fraction A 0.0465768315\n",
      "step 3384 loss 1.06922102 fisher_loss 0.235741898 triplet loss 0.833479166 l2_loss 19.2492085 fraction B 0.335916817 lossA 2.37891626 fraction A 0.046891246\n",
      "step 3385 loss 1.03939068 fisher_loss 0.235267088 triplet loss 0.80412364 l2_loss 19.2552719 fraction B 0.174173146 lossA 2.45762777 fraction A 0.0533828214\n",
      "step 3386 loss 1.11041331 fisher_loss 0.235483751 triplet loss 0.874929547 l2_loss 19.2633743 fraction B 0.240881294 lossA 2.50040817 fraction A 0.0567199811\n",
      "step 3387 loss 1.48733485 fisher_loss 0.235491231 triplet loss 1.25184357 l2_loss 19.2698555 fraction B 0.191445857 lossA 2.49735188 fraction A 0.0568406396\n",
      "step 3388 loss 1.10698617 fisher_loss 0.235210434 triplet loss 0.871775746 l2_loss 19.2749329 fraction B 0.157584712 lossA 2.4692173 fraction A 0.0546564125\n",
      "step 3389 loss 1.02259099 fisher_loss 0.234656975 triplet loss 0.787934 l2_loss 19.2790966 fraction B 0.202568829 lossA 2.45637488 fraction A 0.0527664199\n",
      "step 3390 loss 1.10855162 fisher_loss 0.234311745 triplet loss 0.874239922 l2_loss 19.2822018 fraction B 0.141757563 lossA 2.42060566 fraction A 0.0498368368\n",
      "step 3391 loss 1.04915559 fisher_loss 0.233720511 triplet loss 0.815435052 l2_loss 19.2858562 fraction B 0.255550891 lossA 2.37773418 fraction A 0.0466813\n",
      "step 3392 loss 0.933655798 fisher_loss 0.232916176 triplet loss 0.700739622 l2_loss 19.2892151 fraction B 0.185055584 lossA 2.32103467 fraction A 0.0421936698\n",
      "step 3393 loss 0.968682289 fisher_loss 0.232086658 triplet loss 0.736595631 l2_loss 19.2929764 fraction B 0.21189639 lossA 2.32793856 fraction A 0.0441507101\n",
      "step 3394 loss 1.12828958 fisher_loss 0.23181203 triplet loss 0.89647752 l2_loss 19.2988262 fraction B 0.235849857 lossA 2.38675356 fraction A 0.0519016758\n",
      "step 3395 loss 1.38472533 fisher_loss 0.232043937 triplet loss 1.15268135 l2_loss 19.3060322 fraction B 0.244274333 lossA 2.44360924 fraction A 0.0584993139\n",
      "step 3396 loss 1.00091863 fisher_loss 0.232342869 triplet loss 0.768575728 l2_loss 19.3124142 fraction B 0.283200175 lossA 2.43911219 fraction A 0.0596167818\n",
      "step 3397 loss 0.903914928 fisher_loss 0.232312515 triplet loss 0.671602428 l2_loss 19.3174648 fraction B 0.170341969 lossA 2.39004159 fraction A 0.0574531294\n",
      "step 3398 loss 1.0730803 fisher_loss 0.232155204 triplet loss 0.840925097 l2_loss 19.3230152 fraction B 0.281797707 lossA 2.29977 fraction A 0.0507687032\n",
      "step 3399 loss 1.11679053 fisher_loss 0.231674552 triplet loss 0.885116 l2_loss 19.3269043 fraction B 0.238025412 lossA 2.21635151 fraction A 0.0437237173\n",
      "step 3400 loss 0.998450756 fisher_loss 0.23119089 triplet loss 0.767259836 l2_loss 19.3299198 fraction B 0.192310944 lossA 2.18035388 fraction A 0.0406621024\n",
      "step 3401 loss 0.889585853 fisher_loss 0.231022239 triplet loss 0.658563614 l2_loss 19.3336563 fraction B 0.186067954 lossA 2.20463634 fraction A 0.0416337401\n",
      "step 3402 loss 1.14747834 fisher_loss 0.231369361 triplet loss 0.916109 l2_loss 19.3383369 fraction B 0.31837222 lossA 2.26942921 fraction A 0.0455685034\n",
      "step 3403 loss 1.20554185 fisher_loss 0.232023567 triplet loss 0.973518252 l2_loss 19.3428497 fraction B 0.274176478 lossA 2.33141208 fraction A 0.0496925637\n",
      "step 3404 loss 0.987035811 fisher_loss 0.232726499 triplet loss 0.754309297 l2_loss 19.347 fraction B 0.156546026 lossA 2.45665598 fraction A 0.0588713549\n",
      "step 3405 loss 1.06215882 fisher_loss 0.233965576 triplet loss 0.828193188 l2_loss 19.3532028 fraction B 0.176973879 lossA 2.5687592 fraction A 0.065843679\n",
      "step 3406 loss 0.960856497 fisher_loss 0.234869465 triplet loss 0.725987 l2_loss 19.3590164 fraction B 0.195008025 lossA 2.70225596 fraction A 0.0751926452\n",
      "step 3407 loss 1.41389775 fisher_loss 0.235731 triplet loss 1.17816675 l2_loss 19.3663311 fraction B 0.198571667 lossA 2.78075886 fraction A 0.0803636909\n",
      "step 3408 loss 1.12147 fisher_loss 0.235941783 triplet loss 0.885528207 l2_loss 19.3721905 fraction B 0.268784314 lossA 2.77647758 fraction A 0.0801482\n",
      "step 3409 loss 1.00540352 fisher_loss 0.235545442 triplet loss 0.769858062 l2_loss 19.376852 fraction B 0.173936769 lossA 2.64330816 fraction A 0.0711559206\n",
      "step 3410 loss 1.11947083 fisher_loss 0.234162033 triplet loss 0.885308802 l2_loss 19.3795109 fraction B 0.223782852 lossA 2.4505 fraction A 0.0573312752\n",
      "step 3411 loss 0.897845924 fisher_loss 0.232765913 triplet loss 0.66508 l2_loss 19.3806667 fraction B 0.0996656641 lossA 2.28136516 fraction A 0.0438741855\n",
      "step 3412 loss 1.15120435 fisher_loss 0.232025594 triplet loss 0.919178724 l2_loss 19.3832722 fraction B 0.258912235 lossA 2.15298986 fraction A 0.0353543572\n",
      "step 3413 loss 1.03857434 fisher_loss 0.231955945 triplet loss 0.806618392 l2_loss 19.3862019 fraction B 0.231934369 lossA 2.17541051 fraction A 0.0370898582\n",
      "step 3414 loss 1.02245343 fisher_loss 0.232198447 triplet loss 0.790255 l2_loss 19.3911266 fraction B 0.234122351 lossA 2.27599096 fraction A 0.045753818\n",
      "step 3415 loss 1.09536684 fisher_loss 0.232843265 triplet loss 0.862523615 l2_loss 19.3976669 fraction B 0.229475752 lossA 2.44501686 fraction A 0.0590863749\n",
      "step 3416 loss 1.33979416 fisher_loss 0.233877078 triplet loss 1.1059171 l2_loss 19.4041424 fraction B 0.312552601 lossA 2.5990603 fraction A 0.0699515864\n",
      "step 3417 loss 1.18551779 fisher_loss 0.234791696 triplet loss 0.950726092 l2_loss 19.4094486 fraction B 0.20268485 lossA 2.74149585 fraction A 0.0786488\n",
      "step 3418 loss 1.11684692 fisher_loss 0.235384047 triplet loss 0.881462872 l2_loss 19.4135876 fraction B 0.224325016 lossA 2.78520513 fraction A 0.0805451646\n",
      "step 3419 loss 1.23170781 fisher_loss 0.235241041 triplet loss 0.996466756 l2_loss 19.4159222 fraction B 0.235685781 lossA 2.72252631 fraction A 0.0749907792\n",
      "step 3420 loss 1.01951432 fisher_loss 0.234261543 triplet loss 0.78525275 l2_loss 19.4161415 fraction B 0.210799918 lossA 2.59042621 fraction A 0.0639513\n",
      "step 3421 loss 1.09711909 fisher_loss 0.233011693 triplet loss 0.86410737 l2_loss 19.415884 fraction B 0.104528859 lossA 2.46448946 fraction A 0.0537282415\n",
      "step 3422 loss 1.25928843 fisher_loss 0.231964156 triplet loss 1.02732432 l2_loss 19.4166126 fraction B 0.27626732 lossA 2.35118461 fraction A 0.0434446558\n",
      "step 3423 loss 1.41828382 fisher_loss 0.231026083 triplet loss 1.18725777 l2_loss 19.4174614 fraction B 0.225259528 lossA 2.28785849 fraction A 0.0386012867\n",
      "step 3424 loss 1.22347152 fisher_loss 0.230415419 triplet loss 0.993056059 l2_loss 19.4189835 fraction B 0.261473536 lossA 2.29930282 fraction A 0.0401174575\n",
      "step 3425 loss 1.02366805 fisher_loss 0.23021844 triplet loss 0.79344964 l2_loss 19.4225922 fraction B 0.234917045 lossA 2.35670614 fraction A 0.0450552255\n",
      "step 3426 loss 1.00379515 fisher_loss 0.230502829 triplet loss 0.773292303 l2_loss 19.4279919 fraction B 0.22566171 lossA 2.4781394 fraction A 0.0569748431\n",
      "step 3427 loss 0.904238164 fisher_loss 0.231545165 triplet loss 0.672693 l2_loss 19.4368191 fraction B 0.229714021 lossA 2.58994722 fraction A 0.0662001818\n",
      "step 3428 loss 1.04356 fisher_loss 0.232870951 triplet loss 0.810689092 l2_loss 19.4445152 fraction B 0.263559401 lossA 2.61629486 fraction A 0.068908371\n",
      "step 3429 loss 1.47449458 fisher_loss 0.233702824 triplet loss 1.2407918 l2_loss 19.4504089 fraction B 0.201721206 lossA 2.60384345 fraction A 0.068806842\n",
      "step 3430 loss 1.30775261 fisher_loss 0.234280974 triplet loss 1.07347167 l2_loss 19.4552326 fraction B 0.195947096 lossA 2.60409927 fraction A 0.0694026351\n",
      "step 3431 loss 1.0225265 fisher_loss 0.234946236 triplet loss 0.787580252 l2_loss 19.4595737 fraction B 0.197532699 lossA 2.52488518 fraction A 0.0635117739\n",
      "step 3432 loss 0.960309267 fisher_loss 0.23483552 triplet loss 0.725473762 l2_loss 19.4634151 fraction B 0.154101431 lossA 2.40669155 fraction A 0.0545139126\n",
      "step 3433 loss 1.24886191 fisher_loss 0.234305844 triplet loss 1.01455605 l2_loss 19.4670525 fraction B 0.199032798 lossA 2.32017875 fraction A 0.0469489843\n",
      "step 3434 loss 1.02446389 fisher_loss 0.2337448 triplet loss 0.790719151 l2_loss 19.4704514 fraction B 0.249380425 lossA 2.31254959 fraction A 0.046980571\n",
      "step 3435 loss 1.13464785 fisher_loss 0.233769536 triplet loss 0.90087837 l2_loss 19.4763107 fraction B 0.249585807 lossA 2.33283234 fraction A 0.0495756716\n",
      "step 3436 loss 1.030689 fisher_loss 0.233826175 triplet loss 0.796862781 l2_loss 19.482214 fraction B 0.159764633 lossA 2.3608923 fraction A 0.0524558127\n",
      "step 3437 loss 0.992186725 fisher_loss 0.234019294 triplet loss 0.758167446 l2_loss 19.4885445 fraction B 0.225400552 lossA 2.44737911 fraction A 0.0596631467\n",
      "step 3438 loss 0.905343711 fisher_loss 0.234553263 triplet loss 0.670790434 l2_loss 19.4947109 fraction B 0.115522876 lossA 2.45540571 fraction A 0.0595749095\n",
      "step 3439 loss 0.950443566 fisher_loss 0.234168395 triplet loss 0.716275156 l2_loss 19.4994106 fraction B 0.1877563 lossA 2.4457562 fraction A 0.0580360591\n",
      "step 3440 loss 1.03841221 fisher_loss 0.233560964 triplet loss 0.804851294 l2_loss 19.5043869 fraction B 0.141729414 lossA 2.44694185 fraction A 0.0567406639\n",
      "step 3441 loss 0.882104516 fisher_loss 0.233023062 triplet loss 0.649081469 l2_loss 19.5091248 fraction B 0.132630482 lossA 2.42239261 fraction A 0.0529239178\n",
      "step 3442 loss 1.13966119 fisher_loss 0.232443377 triplet loss 0.907217801 l2_loss 19.5132198 fraction B 0.333111703 lossA 2.42141747 fraction A 0.0506896637\n",
      "step 3443 loss 1.08404362 fisher_loss 0.232079551 triplet loss 0.851964116 l2_loss 19.5166321 fraction B 0.271149397 lossA 2.44647074 fraction A 0.0516505092\n",
      "step 3444 loss 1.04975903 fisher_loss 0.23206839 triplet loss 0.817690611 l2_loss 19.5205784 fraction B 0.185114667 lossA 2.5248363 fraction A 0.0571856238\n",
      "step 3445 loss 1.17900932 fisher_loss 0.2326819 triplet loss 0.946327388 l2_loss 19.5262871 fraction B 0.260806561 lossA 2.62004113 fraction A 0.0640688\n",
      "step 3446 loss 1.05020559 fisher_loss 0.233444169 triplet loss 0.816761374 l2_loss 19.5324535 fraction B 0.169893786 lossA 2.59033012 fraction A 0.0621373393\n",
      "step 3447 loss 1.13195753 fisher_loss 0.233237892 triplet loss 0.898719668 l2_loss 19.5375423 fraction B 0.312086642 lossA 2.50867367 fraction A 0.0562524907\n",
      "step 3448 loss 0.784048557 fisher_loss 0.232646018 triplet loss 0.551402509 l2_loss 19.5407791 fraction B 0.0848031193 lossA 2.49613595 fraction A 0.055855345\n",
      "step 3449 loss 0.926335 fisher_loss 0.232956499 triplet loss 0.693378508 l2_loss 19.5474224 fraction B 0.18647939 lossA 2.50789547 fraction A 0.0587104186\n",
      "step 3450 loss 0.914156 fisher_loss 0.233372852 triplet loss 0.680783153 l2_loss 19.5561619 fraction B 0.186184779 lossA 2.49085331 fraction A 0.0590843819\n",
      "step 3451 loss 0.922809482 fisher_loss 0.233744398 triplet loss 0.689065099 l2_loss 19.564661 fraction B 0.143824488 lossA 2.42777491 fraction A 0.0557552651\n",
      "step 3452 loss 0.897409856 fisher_loss 0.233936235 triplet loss 0.663473606 l2_loss 19.5722485 fraction B 0.179139987 lossA 2.33700633 fraction A 0.049113363\n",
      "step 3453 loss 1.01108062 fisher_loss 0.234019518 triplet loss 0.777061105 l2_loss 19.5806236 fraction B 0.12984778 lossA 2.2648077 fraction A 0.0449924879\n",
      "step 3454 loss 1.18321013 fisher_loss 0.234437943 triplet loss 0.948772252 l2_loss 19.5903072 fraction B 0.198281243 lossA 2.25203538 fraction A 0.0446562693\n",
      "step 3455 loss 1.01903558 fisher_loss 0.235013336 triplet loss 0.784022272 l2_loss 19.5995445 fraction B 0.156179279 lossA 2.27901506 fraction A 0.0471797958\n",
      "step 3456 loss 1.21977091 fisher_loss 0.235795587 triplet loss 0.983975291 l2_loss 19.609745 fraction B 0.208242431 lossA 2.30228758 fraction A 0.0489947684\n",
      "step 3457 loss 1.16124594 fisher_loss 0.236245677 triplet loss 0.92500025 l2_loss 19.6177197 fraction B 0.248283044 lossA 2.30687094 fraction A 0.0489156581\n",
      "step 3458 loss 1.0855546 fisher_loss 0.236326545 triplet loss 0.849228 l2_loss 19.6246719 fraction B 0.22875692 lossA 2.2743628 fraction A 0.0454862416\n",
      "step 3459 loss 1.13793039 fisher_loss 0.236023068 triplet loss 0.901907265 l2_loss 19.6305561 fraction B 0.181283638 lossA 2.24486399 fraction A 0.0426390655\n",
      "step 3460 loss 0.972618818 fisher_loss 0.235663369 triplet loss 0.736955464 l2_loss 19.6359577 fraction B 0.220355898 lossA 2.26650596 fraction A 0.043932721\n",
      "step 3461 loss 1.00767875 fisher_loss 0.235627949 triplet loss 0.772050738 l2_loss 19.642313 fraction B 0.160070837 lossA 2.34492922 fraction A 0.0510433093\n",
      "step 3462 loss 1.18598914 fisher_loss 0.236201793 triplet loss 0.949787319 l2_loss 19.6502495 fraction B 0.185303986 lossA 2.3636086 fraction A 0.0524402\n",
      "step 3463 loss 0.967221677 fisher_loss 0.236197174 triplet loss 0.731024504 l2_loss 19.6559315 fraction B 0.161544845 lossA 2.38690686 fraction A 0.0549780317\n",
      "step 3464 loss 1.1249367 fisher_loss 0.236237377 triplet loss 0.888699293 l2_loss 19.6630077 fraction B 0.205424264 lossA 2.3949 fraction A 0.0557069406\n",
      "step 3465 loss 1.26680708 fisher_loss 0.236042067 triplet loss 1.03076506 l2_loss 19.6695385 fraction B 0.198060364 lossA 2.35316896 fraction A 0.0527236424\n",
      "step 3466 loss 1.05109906 fisher_loss 0.235439196 triplet loss 0.815659821 l2_loss 19.6753922 fraction B 0.158466399 lossA 2.31441355 fraction A 0.0499922335\n",
      "step 3467 loss 1.32043052 fisher_loss 0.234982222 triplet loss 1.08544827 l2_loss 19.6811085 fraction B 0.191856027 lossA 2.30099511 fraction A 0.0501999706\n",
      "step 3468 loss 1.1978786 fisher_loss 0.234539881 triplet loss 0.963338733 l2_loss 19.6864338 fraction B 0.15281406 lossA 2.29624653 fraction A 0.0511649437\n",
      "step 3469 loss 1.25803292 fisher_loss 0.234259874 triplet loss 1.02377307 l2_loss 19.6919403 fraction B 0.20024845 lossA 2.28159952 fraction A 0.0504053161\n",
      "step 3470 loss 1.21192169 fisher_loss 0.233681425 triplet loss 0.978240252 l2_loss 19.695549 fraction B 0.197938234 lossA 2.26258278 fraction A 0.0487158895\n",
      "step 3471 loss 1.31209087 fisher_loss 0.232937694 triplet loss 1.07915318 l2_loss 19.6983185 fraction B 0.164764658 lossA 2.23846841 fraction A 0.0455219559\n",
      "step 3472 loss 1.44081652 fisher_loss 0.232031509 triplet loss 1.20878506 l2_loss 19.7001858 fraction B 0.199495912 lossA 2.23907614 fraction A 0.0436845981\n",
      "step 3473 loss 0.929080963 fisher_loss 0.231092706 triplet loss 0.697988272 l2_loss 19.6999588 fraction B 0.185970917 lossA 2.19920349 fraction A 0.0380894244\n",
      "step 3474 loss 1.23021841 fisher_loss 0.230153099 triplet loss 1.00006533 l2_loss 19.6986847 fraction B 0.317164332 lossA 2.16731405 fraction A 0.0350165442\n",
      "step 3475 loss 1.36295927 fisher_loss 0.229488164 triplet loss 1.13347113 l2_loss 19.6978512 fraction B 0.211064979 lossA 2.22550344 fraction A 0.0374252424\n",
      "step 3476 loss 1.1662885 fisher_loss 0.229224458 triplet loss 0.937064052 l2_loss 19.6988029 fraction B 0.239064917 lossA 2.34980202 fraction A 0.0441165864\n",
      "step 3477 loss 1.1144129 fisher_loss 0.229326069 triplet loss 0.885086834 l2_loss 19.701334 fraction B 0.291388363 lossA 2.4521153 fraction A 0.051503446\n",
      "step 3478 loss 1.02675 fisher_loss 0.22959052 triplet loss 0.797159493 l2_loss 19.7039871 fraction B 0.317425519 lossA 2.56371164 fraction A 0.0592263\n",
      "step 3479 loss 1.15001655 fisher_loss 0.229956895 triplet loss 0.920059681 l2_loss 19.7069225 fraction B 0.1905123 lossA 2.62934542 fraction A 0.0640804321\n",
      "step 3480 loss 1.3463093 fisher_loss 0.229973987 triplet loss 1.11633527 l2_loss 19.7099628 fraction B 0.358652413 lossA 2.61353445 fraction A 0.0629812926\n",
      "step 3481 loss 1.1194315 fisher_loss 0.229354337 triplet loss 0.890077114 l2_loss 19.7113552 fraction B 0.270739675 lossA 2.56464911 fraction A 0.0590896718\n",
      "step 3482 loss 1.02002311 fisher_loss 0.228561446 triplet loss 0.791461647 l2_loss 19.7124653 fraction B 0.187425628 lossA 2.45634675 fraction A 0.0509184115\n",
      "step 3483 loss 1.13532758 fisher_loss 0.227382675 triplet loss 0.907944918 l2_loss 19.7125702 fraction B 0.24910596 lossA 2.36917019 fraction A 0.0428654552\n",
      "step 3484 loss 1.16826379 fisher_loss 0.226420626 triplet loss 0.941843212 l2_loss 19.7135048 fraction B 0.244809464 lossA 2.25697 fraction A 0.0356731527\n",
      "step 3485 loss 1.14520156 fisher_loss 0.225698322 triplet loss 0.919503272 l2_loss 19.7144833 fraction B 0.276492864 lossA 2.19031572 fraction A 0.032552205\n",
      "step 3486 loss 1.02047169 fisher_loss 0.225380942 triplet loss 0.795090735 l2_loss 19.7162075 fraction B 0.230536059 lossA 2.15257 fraction A 0.0313426219\n",
      "step 3487 loss 0.846240044 fisher_loss 0.22518304 triplet loss 0.621057034 l2_loss 19.7188988 fraction B 0.273201168 lossA 2.27783561 fraction A 0.0378316604\n",
      "step 3488 loss 0.99602145 fisher_loss 0.225617468 triplet loss 0.770404 l2_loss 19.7256489 fraction B 0.223417982 lossA 2.37845254 fraction A 0.0469396412\n",
      "step 3489 loss 0.960324407 fisher_loss 0.226368114 triplet loss 0.733956277 l2_loss 19.7323151 fraction B 0.243586481 lossA 2.43409324 fraction A 0.0537342913\n",
      "step 3490 loss 0.818166137 fisher_loss 0.227125421 triplet loss 0.59104073 l2_loss 19.7382984 fraction B 0.116869099 lossA 2.48923302 fraction A 0.0595400631\n",
      "step 3491 loss 1.10318696 fisher_loss 0.228171855 triplet loss 0.87501514 l2_loss 19.7454967 fraction B 0.283656895 lossA 2.49482298 fraction A 0.0607491024\n",
      "step 3492 loss 1.01693857 fisher_loss 0.228676304 triplet loss 0.788262308 l2_loss 19.7510471 fraction B 0.19962813 lossA 2.45162487 fraction A 0.0574162528\n",
      "step 3493 loss 1.10979092 fisher_loss 0.228711694 triplet loss 0.881079197 l2_loss 19.7559643 fraction B 0.210663751 lossA 2.39318728 fraction A 0.0518677719\n",
      "step 3494 loss 0.992352545 fisher_loss 0.228407085 triplet loss 0.76394546 l2_loss 19.7603302 fraction B 0.173007771 lossA 2.33556819 fraction A 0.0456977077\n",
      "step 3495 loss 1.29836273 fisher_loss 0.228053495 triplet loss 1.07030928 l2_loss 19.7647209 fraction B 0.270953894 lossA 2.26530766 fraction A 0.0398896895\n",
      "step 3496 loss 1.00706434 fisher_loss 0.227502197 triplet loss 0.779562175 l2_loss 19.7677097 fraction B 0.318277478 lossA 2.2661643 fraction A 0.0403254144\n",
      "step 3497 loss 1.20184731 fisher_loss 0.227641657 triplet loss 0.974205613 l2_loss 19.7727 fraction B 0.253760934 lossA 2.30259037 fraction A 0.0438258275\n",
      "step 3498 loss 1.00700521 fisher_loss 0.227995723 triplet loss 0.779009521 l2_loss 19.7776985 fraction B 0.202038437 lossA 2.3649838 fraction A 0.0503739491\n",
      "step 3499 loss 1.31725347 fisher_loss 0.228696942 triplet loss 1.08855653 l2_loss 19.7832 fraction B 0.266040683 lossA 2.40286827 fraction A 0.0541027114\n",
      "step 3500 loss 1.17406404 fisher_loss 0.229163632 triplet loss 0.944900393 l2_loss 19.7878 fraction B 0.0844445303 lossA 2.4016223 fraction A 0.0539861843\n",
      "step 3501 loss 0.888401449 fisher_loss 0.229456469 triplet loss 0.658944964 l2_loss 19.7920322 fraction B 0.181564808 lossA 2.35178065 fraction A 0.0494692512\n",
      "step 3502 loss 1.34310436 fisher_loss 0.229531884 triplet loss 1.11357248 l2_loss 19.7961826 fraction B 0.248397708 lossA 2.31843805 fraction A 0.0462158732\n",
      "step 3503 loss 0.992003322 fisher_loss 0.229615539 triplet loss 0.762387753 l2_loss 19.8004684 fraction B 0.154540032 lossA 2.30586267 fraction A 0.0462242067\n",
      "step 3504 loss 1.01094794 fisher_loss 0.229962289 triplet loss 0.780985713 l2_loss 19.8064117 fraction B 0.161893621 lossA 2.31685543 fraction A 0.0488293394\n",
      "step 3505 loss 1.11984336 fisher_loss 0.230688274 triplet loss 0.88915509 l2_loss 19.8123665 fraction B 0.204818562 lossA 2.36164188 fraction A 0.0545289442\n",
      "step 3506 loss 1.11265016 fisher_loss 0.231555402 triplet loss 0.881094694 l2_loss 19.8185635 fraction B 0.219182864 lossA 2.40300536 fraction A 0.0593195409\n",
      "step 3507 loss 1.37608027 fisher_loss 0.232141793 triplet loss 1.14393842 l2_loss 19.8238316 fraction B 0.205388352 lossA 2.42640805 fraction A 0.0620484464\n",
      "step 3508 loss 1.38596475 fisher_loss 0.232239872 triplet loss 1.15372491 l2_loss 19.8279915 fraction B 0.255601108 lossA 2.42210817 fraction A 0.0617915913\n",
      "step 3509 loss 0.985822856 fisher_loss 0.231635913 triplet loss 0.754186928 l2_loss 19.8304691 fraction B 0.218288705 lossA 2.35692573 fraction A 0.0563494973\n",
      "step 3510 loss 1.20570076 fisher_loss 0.230502278 triplet loss 0.975198507 l2_loss 19.8315811 fraction B 0.0765389651 lossA 2.27124619 fraction A 0.0482181199\n",
      "step 3511 loss 0.94403559 fisher_loss 0.229355335 triplet loss 0.714680254 l2_loss 19.8327656 fraction B 0.174362704 lossA 2.20850658 fraction A 0.0405128673\n",
      "step 3512 loss 0.74204278 fisher_loss 0.228317872 triplet loss 0.513724923 l2_loss 19.8342514 fraction B 0.15184395 lossA 2.27060652 fraction A 0.0442465916\n",
      "step 3513 loss 1.08302104 fisher_loss 0.228633 triplet loss 0.854388 l2_loss 19.8404675 fraction B 0.184933797 lossA 2.37043095 fraction A 0.0507705882\n",
      "step 3514 loss 0.81791091 fisher_loss 0.229347497 triplet loss 0.588563442 l2_loss 19.8466225 fraction B 0.133913368 lossA 2.48109174 fraction A 0.0573499314\n",
      "step 3515 loss 1.06353235 fisher_loss 0.230577365 triplet loss 0.832955 l2_loss 19.854044 fraction B 0.223086596 lossA 2.53982282 fraction A 0.0593830757\n",
      "step 3516 loss 1.00981963 fisher_loss 0.231309608 triplet loss 0.778510034 l2_loss 19.8600273 fraction B 0.140722811 lossA 2.55323219 fraction A 0.0575138368\n",
      "step 3517 loss 1.00820112 fisher_loss 0.231536061 triplet loss 0.776665032 l2_loss 19.8652649 fraction B 0.199625596 lossA 2.49110079 fraction A 0.050183706\n",
      "step 3518 loss 1.00981641 fisher_loss 0.231020808 triplet loss 0.77879566 l2_loss 19.8689327 fraction B 0.166931123 lossA 2.41994524 fraction A 0.0445400663\n",
      "step 3519 loss 0.864713907 fisher_loss 0.2302517 triplet loss 0.634462237 l2_loss 19.8731461 fraction B 0.144895568 lossA 2.35089064 fraction A 0.0398328565\n",
      "step 3520 loss 1.12502253 fisher_loss 0.229356989 triplet loss 0.895665526 l2_loss 19.878582 fraction B 0.240492433 lossA 2.27000737 fraction A 0.0356966965\n",
      "step 3521 loss 0.968776107 fisher_loss 0.228452042 triplet loss 0.74032408 l2_loss 19.8827286 fraction B 0.155500963 lossA 2.27855301 fraction A 0.0358040594\n",
      "step 3522 loss 0.999604 fisher_loss 0.227962568 triplet loss 0.771641433 l2_loss 19.8886337 fraction B 0.260405213 lossA 2.40549588 fraction A 0.043701984\n",
      "step 3523 loss 0.923485577 fisher_loss 0.228005 triplet loss 0.695480585 l2_loss 19.8969803 fraction B 0.229960695 lossA 2.49787664 fraction A 0.053065151\n",
      "step 3524 loss 1.0446775 fisher_loss 0.228527769 triplet loss 0.816149771 l2_loss 19.9064407 fraction B 0.193217292 lossA 2.58846283 fraction A 0.0622009076\n",
      "step 3525 loss 1.25368619 fisher_loss 0.229302436 triplet loss 1.02438378 l2_loss 19.9169064 fraction B 0.267242104 lossA 2.60756493 fraction A 0.0654097348\n",
      "step 3526 loss 0.909363866 fisher_loss 0.229601 triplet loss 0.67976284 l2_loss 19.9250526 fraction B 0.113112785 lossA 2.5142839 fraction A 0.0591418296\n",
      "step 3527 loss 1.00709987 fisher_loss 0.229074523 triplet loss 0.778025329 l2_loss 19.9314423 fraction B 0.242599741 lossA 2.3983779 fraction A 0.0501082204\n",
      "step 3528 loss 1.30888629 fisher_loss 0.228595138 triplet loss 1.08029115 l2_loss 19.937603 fraction B 0.251235 lossA 2.31136322 fraction A 0.0426124409\n",
      "step 3529 loss 1.04819119 fisher_loss 0.228548244 triplet loss 0.819642901 l2_loss 19.9440937 fraction B 0.256751746 lossA 2.2533052 fraction A 0.0377408154\n",
      "step 3530 loss 1.31641495 fisher_loss 0.228819624 triplet loss 1.08759534 l2_loss 19.9505939 fraction B 0.249471456 lossA 2.2675035 fraction A 0.0398255028\n",
      "step 3531 loss 1.23297501 fisher_loss 0.229595959 triplet loss 1.00337911 l2_loss 19.9575977 fraction B 0.265497327 lossA 2.30657101 fraction A 0.0438265875\n",
      "step 3532 loss 1.11186516 fisher_loss 0.230203539 triplet loss 0.881661654 l2_loss 19.9630623 fraction B 0.188177198 lossA 2.39423013 fraction A 0.0520472191\n",
      "step 3533 loss 1.16279387 fisher_loss 0.23137939 triplet loss 0.931414545 l2_loss 19.9689846 fraction B 0.305604815 lossA 2.48985624 fraction A 0.0589217767\n",
      "step 3534 loss 1.13714814 fisher_loss 0.2324218 triplet loss 0.904726326 l2_loss 19.9740105 fraction B 0.244698063 lossA 2.5694809 fraction A 0.0634472594\n",
      "step 3535 loss 1.06680608 fisher_loss 0.233212784 triplet loss 0.833593309 l2_loss 19.9779854 fraction B 0.223481774 lossA 2.54755569 fraction A 0.0601530597\n",
      "step 3536 loss 1.45930719 fisher_loss 0.23310031 triplet loss 1.2262069 l2_loss 19.9808121 fraction B 0.233919188 lossA 2.48628592 fraction A 0.0545384362\n",
      "step 3537 loss 0.986795783 fisher_loss 0.232750952 triplet loss 0.754044831 l2_loss 19.98353 fraction B 0.209810272 lossA 2.39681864 fraction A 0.0472424962\n",
      "step 3538 loss 1.14852536 fisher_loss 0.231952444 triplet loss 0.916572928 l2_loss 19.9864025 fraction B 0.222781047 lossA 2.29076123 fraction A 0.0396557264\n",
      "step 3539 loss 0.699781656 fisher_loss 0.230836987 triplet loss 0.468944699 l2_loss 19.9892902 fraction B 0.131005585 lossA 2.25239086 fraction A 0.0381438285\n",
      "step 3540 loss 1.18260121 fisher_loss 0.230159461 triplet loss 0.952441752 l2_loss 19.9963989 fraction B 0.321090728 lossA 2.28594851 fraction A 0.0420618579\n",
      "step 3541 loss 1.14301372 fisher_loss 0.230223894 triplet loss 0.912789822 l2_loss 20.004591 fraction B 0.297638148 lossA 2.38298345 fraction A 0.0517999642\n",
      "step 3542 loss 1.04470956 fisher_loss 0.230972365 triplet loss 0.813737214 l2_loss 20.0126648 fraction B 0.188019589 lossA 2.45981932 fraction A 0.0587383471\n",
      "step 3543 loss 1.16490483 fisher_loss 0.231578514 triplet loss 0.933326304 l2_loss 20.0203266 fraction B 0.204117 lossA 2.53885865 fraction A 0.0644812\n",
      "step 3544 loss 1.14644098 fisher_loss 0.23189576 triplet loss 0.914545238 l2_loss 20.0267601 fraction B 0.179555476 lossA 2.54016829 fraction A 0.0640637726\n",
      "step 3545 loss 1.25567532 fisher_loss 0.231484935 triplet loss 1.02419043 l2_loss 20.0313892 fraction B 0.257428855 lossA 2.46907043 fraction A 0.0573814437\n",
      "step 3546 loss 0.94472909 fisher_loss 0.230572909 triplet loss 0.714156151 l2_loss 20.0338326 fraction B 0.16531004 lossA 2.37375879 fraction A 0.0466653258\n",
      "step 3547 loss 0.99336046 fisher_loss 0.229631171 triplet loss 0.763729274 l2_loss 20.0355968 fraction B 0.161488533 lossA 2.30027056 fraction A 0.0382534377\n",
      "step 3548 loss 1.02982032 fisher_loss 0.229134887 triplet loss 0.800685406 l2_loss 20.0376987 fraction B 0.227025807 lossA 2.28177667 fraction A 0.0364523754\n",
      "step 3549 loss 0.993355691 fisher_loss 0.229113698 triplet loss 0.764242 l2_loss 20.0412521 fraction B 0.279358089 lossA 2.24897981 fraction A 0.0345016\n",
      "step 3550 loss 1.05030334 fisher_loss 0.229295477 triplet loss 0.821007848 l2_loss 20.0450268 fraction B 0.278613061 lossA 2.31918716 fraction A 0.0376579389\n",
      "step 3551 loss 1.17270017 fisher_loss 0.229748547 triplet loss 0.942951679 l2_loss 20.0494537 fraction B 0.340484113 lossA 2.40561652 fraction A 0.0425186232\n",
      "step 3552 loss 1.31827545 fisher_loss 0.230118364 triplet loss 1.08815706 l2_loss 20.0540466 fraction B 0.278575897 lossA 2.51910496 fraction A 0.0519734696\n",
      "step 3553 loss 1.29571187 fisher_loss 0.230799466 triplet loss 1.06491244 l2_loss 20.0599766 fraction B 0.195994914 lossA 2.62177849 fraction A 0.0601081066\n",
      "step 3554 loss 1.02968121 fisher_loss 0.231235549 triplet loss 0.798445642 l2_loss 20.0653839 fraction B 0.141526788 lossA 2.66504526 fraction A 0.0638815314\n",
      "step 3555 loss 1.1057564 fisher_loss 0.231244 triplet loss 0.874512434 l2_loss 20.0701389 fraction B 0.139552623 lossA 2.63745213 fraction A 0.0624765642\n",
      "step 3556 loss 1.19082475 fisher_loss 0.230641559 triplet loss 0.960183203 l2_loss 20.0738354 fraction B 0.21202898 lossA 2.54605508 fraction A 0.056165196\n",
      "step 3557 loss 0.900857925 fisher_loss 0.229745224 triplet loss 0.671112716 l2_loss 20.0767727 fraction B 0.131284803 lossA 2.43600273 fraction A 0.047366742\n",
      "step 3558 loss 0.998562574 fisher_loss 0.228690132 triplet loss 0.769872427 l2_loss 20.0793934 fraction B 0.160727382 lossA 2.36925626 fraction A 0.0422414131\n",
      "step 3559 loss 1.03492641 fisher_loss 0.228290439 triplet loss 0.806636035 l2_loss 20.0827675 fraction B 0.221158177 lossA 2.3331089 fraction A 0.0398311913\n",
      "step 3560 loss 1.00741112 fisher_loss 0.228281289 triplet loss 0.779129863 l2_loss 20.087204 fraction B 0.179047927 lossA 2.35672259 fraction A 0.0428152531\n",
      "step 3561 loss 1.14867771 fisher_loss 0.228646979 triplet loss 0.920030773 l2_loss 20.093689 fraction B 0.209567443 lossA 2.42046714 fraction A 0.0496007763\n",
      "step 3562 loss 1.12583 fisher_loss 0.229107365 triplet loss 0.896722674 l2_loss 20.1000061 fraction B 0.163484782 lossA 2.49092078 fraction A 0.0563126579\n",
      "step 3563 loss 1.05287755 fisher_loss 0.229428694 triplet loss 0.823448896 l2_loss 20.1047421 fraction B 0.144732624 lossA 2.52905321 fraction A 0.0595079\n",
      "step 3564 loss 1.07237434 fisher_loss 0.229515955 triplet loss 0.842858374 l2_loss 20.1093025 fraction B 0.18371886 lossA 2.5522778 fraction A 0.0613158457\n",
      "step 3565 loss 1.28002524 fisher_loss 0.229469508 triplet loss 1.05055571 l2_loss 20.1133862 fraction B 0.206074491 lossA 2.49986744 fraction A 0.0571264736\n",
      "step 3566 loss 1.32349217 fisher_loss 0.228989229 triplet loss 1.09450293 l2_loss 20.1160622 fraction B 0.281892747 lossA 2.43741512 fraction A 0.0513919517\n",
      "step 3567 loss 0.989222169 fisher_loss 0.228221104 triplet loss 0.76100105 l2_loss 20.1176815 fraction B 0.148037374 lossA 2.40671873 fraction A 0.0478644036\n",
      "step 3568 loss 1.01401615 fisher_loss 0.22763148 triplet loss 0.786384702 l2_loss 20.120945 fraction B 0.168045565 lossA 2.41486979 fraction A 0.0486476123\n",
      "step 3569 loss 1.05696154 fisher_loss 0.227324456 triplet loss 0.82963711 l2_loss 20.1254101 fraction B 0.1810316 lossA 2.40326142 fraction A 0.0478916429\n",
      "step 3570 loss 1.07976055 fisher_loss 0.227024913 triplet loss 0.852735698 l2_loss 20.1298313 fraction B 0.142385855 lossA 2.37899446 fraction A 0.0455227531\n",
      "step 3571 loss 1.15941119 fisher_loss 0.226495907 triplet loss 0.93291533 l2_loss 20.1335106 fraction B 0.224640116 lossA 2.34711266 fraction A 0.043155022\n",
      "step 3572 loss 1.05358493 fisher_loss 0.225920305 triplet loss 0.827664614 l2_loss 20.1372 fraction B 0.157170206 lossA 2.32398796 fraction A 0.0413951427\n",
      "step 3573 loss 0.857291222 fisher_loss 0.225519702 triplet loss 0.631771505 l2_loss 20.1415806 fraction B 0.158423916 lossA 2.36992264 fraction A 0.0450472943\n",
      "step 3574 loss 1.09604609 fisher_loss 0.225660473 triplet loss 0.870385587 l2_loss 20.1478157 fraction B 0.212568149 lossA 2.41842556 fraction A 0.0490961559\n",
      "step 3575 loss 0.900630236 fisher_loss 0.22580272 triplet loss 0.674827516 l2_loss 20.1542377 fraction B 0.154208735 lossA 2.48148823 fraction A 0.0550002\n",
      "step 3576 loss 1.10991323 fisher_loss 0.226307556 triplet loss 0.883605659 l2_loss 20.1617985 fraction B 0.21887365 lossA 2.50205946 fraction A 0.0566655397\n",
      "step 3577 loss 1.0079515 fisher_loss 0.226543233 triplet loss 0.78140831 l2_loss 20.168272 fraction B 0.147045061 lossA 2.46413112 fraction A 0.0534174144\n",
      "step 3578 loss 0.992356777 fisher_loss 0.226252735 triplet loss 0.766104043 l2_loss 20.1728268 fraction B 0.227502614 lossA 2.41926861 fraction A 0.0490543544\n",
      "step 3579 loss 1.1090709 fisher_loss 0.22596094 triplet loss 0.88311 l2_loss 20.1769123 fraction B 0.161768392 lossA 2.35784936 fraction A 0.0433099829\n",
      "step 3580 loss 1.2268858 fisher_loss 0.225682378 triplet loss 1.00120342 l2_loss 20.1800365 fraction B 0.269656539 lossA 2.30084467 fraction A 0.0390485264\n",
      "step 3581 loss 1.00175261 fisher_loss 0.225420564 triplet loss 0.776332 l2_loss 20.1837177 fraction B 0.184425533 lossA 2.24710321 fraction A 0.0353899635\n",
      "step 3582 loss 1.15638769 fisher_loss 0.225189358 triplet loss 0.931198299 l2_loss 20.1871128 fraction B 0.264439434 lossA 2.31796312 fraction A 0.0399463437\n",
      "step 3583 loss 0.985293925 fisher_loss 0.225454867 triplet loss 0.759839058 l2_loss 20.192522 fraction B 0.187107623 lossA 2.44855618 fraction A 0.050488051\n",
      "step 3584 loss 0.995644093 fisher_loss 0.226502806 triplet loss 0.769141316 l2_loss 20.1999512 fraction B 0.195040613 lossA 2.60551643 fraction A 0.0619119257\n",
      "step 3585 loss 0.873826802 fisher_loss 0.227931008 triplet loss 0.645895779 l2_loss 20.2076321 fraction B 0.11954961 lossA 2.68972397 fraction A 0.0680112094\n",
      "step 3586 loss 1.05731332 fisher_loss 0.228944719 triplet loss 0.828368604 l2_loss 20.2147541 fraction B 0.188110679 lossA 2.67543983 fraction A 0.0676101893\n",
      "step 3587 loss 1.08115792 fisher_loss 0.229244232 triplet loss 0.851913691 l2_loss 20.2213478 fraction B 0.225681096 lossA 2.61414027 fraction A 0.0633343533\n",
      "step 3588 loss 1.10027385 fisher_loss 0.229076356 triplet loss 0.871197522 l2_loss 20.2265377 fraction B 0.196989208 lossA 2.51850677 fraction A 0.0566841215\n",
      "step 3589 loss 1.23235083 fisher_loss 0.228506923 triplet loss 1.0038439 l2_loss 20.2311058 fraction B 0.171262905 lossA 2.43393421 fraction A 0.0503484495\n",
      "step 3590 loss 0.876896918 fisher_loss 0.227944613 triplet loss 0.648952305 l2_loss 20.2362785 fraction B 0.1473708 lossA 2.38396358 fraction A 0.0475749858\n",
      "step 3591 loss 0.751597524 fisher_loss 0.227889165 triplet loss 0.523708344 l2_loss 20.2438164 fraction B 0.0829207897 lossA 2.33214831 fraction A 0.0458109416\n",
      "step 3592 loss 0.745700121 fisher_loss 0.228352934 triplet loss 0.517347217 l2_loss 20.2539806 fraction B 0.117356606 lossA 2.36531138 fraction A 0.0523589514\n",
      "step 3593 loss 1.19324064 fisher_loss 0.230394751 triplet loss 0.962845922 l2_loss 20.2686844 fraction B 0.283627212 lossA 2.36027408 fraction A 0.0541789979\n",
      "step 3594 loss 1.07595539 fisher_loss 0.232003644 triplet loss 0.843951762 l2_loss 20.2807903 fraction B 0.18971312 lossA 2.36989689 fraction A 0.0566237755\n",
      "step 3595 loss 0.9000718 fisher_loss 0.233569279 triplet loss 0.666502535 l2_loss 20.2924023 fraction B 0.112287253 lossA 2.3051188 fraction A 0.0511274897\n",
      "step 3596 loss 0.946461558 fisher_loss 0.234170243 triplet loss 0.7122913 l2_loss 20.3015022 fraction B 0.192561641 lossA 2.26874852 fraction A 0.0477143712\n",
      "step 3597 loss 0.987161458 fisher_loss 0.234736487 triplet loss 0.752424955 l2_loss 20.3105984 fraction B 0.244700015 lossA 2.24700856 fraction A 0.0452865437\n",
      "step 3598 loss 1.59735179 fisher_loss 0.23510541 triplet loss 1.36224639 l2_loss 20.3193531 fraction B 0.258489251 lossA 2.22736359 fraction A 0.0428646244\n",
      "step 3599 loss 1.23988569 fisher_loss 0.234909251 triplet loss 1.00497639 l2_loss 20.3255329 fraction B 0.258500725 lossA 2.22836399 fraction A 0.0418717228\n",
      "step 3600 loss 1.0124228 fisher_loss 0.234466359 triplet loss 0.777956426 l2_loss 20.3308315 fraction B 0.196203351 lossA 2.24733758 fraction A 0.0431213342\n",
      "step 3601 loss 1.1348331 fisher_loss 0.23419404 triplet loss 0.900639057 l2_loss 20.3368721 fraction B 0.200994 lossA 2.28766799 fraction A 0.0457004234\n",
      "step 3602 loss 0.789162755 fisher_loss 0.234085232 triplet loss 0.555077553 l2_loss 20.3416576 fraction B 0.0969004557 lossA 2.40221691 fraction A 0.0551518314\n",
      "step 3603 loss 1.06593752 fisher_loss 0.234894678 triplet loss 0.831042826 l2_loss 20.349369 fraction B 0.202809125 lossA 2.41981983 fraction A 0.0555951968\n",
      "step 3604 loss 1.32371688 fisher_loss 0.234984964 triplet loss 1.08873188 l2_loss 20.3537788 fraction B 0.145045117 lossA 2.40935659 fraction A 0.0543693826\n",
      "step 3605 loss 1.01469028 fisher_loss 0.234981984 triplet loss 0.779708326 l2_loss 20.3582535 fraction B 0.217954814 lossA 2.37029934 fraction A 0.0508678444\n",
      "step 3606 loss 1.02593422 fisher_loss 0.235000059 triplet loss 0.790934205 l2_loss 20.3619919 fraction B 0.170619264 lossA 2.38861251 fraction A 0.0524345487\n",
      "step 3607 loss 1.07566583 fisher_loss 0.235376611 triplet loss 0.840289176 l2_loss 20.3681374 fraction B 0.158760279 lossA 2.40260053 fraction A 0.0530636646\n",
      "step 3608 loss 1.22176981 fisher_loss 0.235673085 triplet loss 0.98609674 l2_loss 20.3742752 fraction B 0.167509913 lossA 2.36655879 fraction A 0.0488630272\n",
      "step 3609 loss 1.00787413 fisher_loss 0.235443309 triplet loss 0.772430778 l2_loss 20.3791275 fraction B 0.193650723 lossA 2.33776093 fraction A 0.0451914594\n",
      "step 3610 loss 1.26386929 fisher_loss 0.235121638 triplet loss 1.02874768 l2_loss 20.3840046 fraction B 0.244088843 lossA 2.33739591 fraction A 0.044620119\n",
      "step 3611 loss 1.0794307 fisher_loss 0.234775141 triplet loss 0.844655514 l2_loss 20.389246 fraction B 0.246288776 lossA 2.31856847 fraction A 0.0433141105\n",
      "step 3612 loss 1.09463501 fisher_loss 0.234188601 triplet loss 0.860446393 l2_loss 20.3944836 fraction B 0.181289643 lossA 2.34444618 fraction A 0.0457411408\n",
      "step 3613 loss 1.02229822 fisher_loss 0.233889192 triplet loss 0.788409054 l2_loss 20.4009647 fraction B 0.213253573 lossA 2.41827154 fraction A 0.0517271198\n",
      "step 3614 loss 0.842278123 fisher_loss 0.234205216 triplet loss 0.608072877 l2_loss 20.4093037 fraction B 0.114384241 lossA 2.41992188 fraction A 0.0511270911\n",
      "step 3615 loss 1.06823242 fisher_loss 0.23424162 triplet loss 0.833990753 l2_loss 20.4178181 fraction B 0.208800167 lossA 2.44827056 fraction A 0.0533463806\n",
      "step 3616 loss 1.17462289 fisher_loss 0.23452273 triplet loss 0.940100193 l2_loss 20.4269886 fraction B 0.278406233 lossA 2.45065 fraction A 0.0532246381\n",
      "step 3617 loss 1.1310159 fisher_loss 0.234581739 triplet loss 0.896434188 l2_loss 20.4351673 fraction B 0.179626897 lossA 2.47142339 fraction A 0.0554103516\n",
      "step 3618 loss 1.06792569 fisher_loss 0.234844416 triplet loss 0.833081305 l2_loss 20.4446468 fraction B 0.162874982 lossA 2.44695592 fraction A 0.0534437113\n",
      "step 3619 loss 1.11865091 fisher_loss 0.234698713 triplet loss 0.8839522 l2_loss 20.4530869 fraction B 0.291626513 lossA 2.38890433 fraction A 0.049073264\n",
      "step 3620 loss 1.231305 fisher_loss 0.234089151 triplet loss 0.997215807 l2_loss 20.4602623 fraction B 0.223549709 lossA 2.30920315 fraction A 0.0423275121\n",
      "step 3621 loss 1.25989401 fisher_loss 0.233178854 triplet loss 1.02671516 l2_loss 20.4664421 fraction B 0.19022195 lossA 2.22546911 fraction A 0.0369123667\n",
      "step 3622 loss 0.956467032 fisher_loss 0.232337147 triplet loss 0.724129915 l2_loss 20.4716 fraction B 0.211340204 lossA 2.22193241 fraction A 0.0369173661\n",
      "step 3623 loss 0.932958603 fisher_loss 0.232170805 triplet loss 0.700787783 l2_loss 20.4781265 fraction B 0.174178109 lossA 2.3283298 fraction A 0.0457312502\n",
      "step 3624 loss 0.860450506 fisher_loss 0.232993588 triplet loss 0.627456903 l2_loss 20.4867516 fraction B 0.0729578882 lossA 2.41493297 fraction A 0.0550839119\n",
      "step 3625 loss 1.3373785 fisher_loss 0.233962864 triplet loss 1.10341561 l2_loss 20.4964886 fraction B 0.143702596 lossA 2.52636886 fraction A 0.0639634356\n",
      "step 3626 loss 1.21633923 fisher_loss 0.234600142 triplet loss 0.981739044 l2_loss 20.503437 fraction B 0.222616807 lossA 2.61365223 fraction A 0.0699901283\n",
      "step 3627 loss 1.09417725 fisher_loss 0.23487103 triplet loss 0.859306216 l2_loss 20.5086689 fraction B 0.245314911 lossA 2.62002707 fraction A 0.0700036\n",
      "step 3628 loss 1.06305015 fisher_loss 0.23430264 triplet loss 0.828747511 l2_loss 20.5120983 fraction B 0.358017147 lossA 2.58876491 fraction A 0.0670080185\n",
      "step 3629 loss 1.33240771 fisher_loss 0.233380482 triplet loss 1.09902728 l2_loss 20.5144444 fraction B 0.2499571 lossA 2.50230241 fraction A 0.059795253\n",
      "step 3630 loss 0.944110274 fisher_loss 0.23183915 triplet loss 0.712271094 l2_loss 20.5146236 fraction B 0.172460526 lossA 2.39406919 fraction A 0.0496539511\n",
      "step 3631 loss 1.07493329 fisher_loss 0.230366111 triplet loss 0.844567239 l2_loss 20.5141239 fraction B 0.246788532 lossA 2.30630136 fraction A 0.0401694365\n",
      "step 3632 loss 1.09708703 fisher_loss 0.229227424 triplet loss 0.867859602 l2_loss 20.5137062 fraction B 0.236956805 lossA 2.2398169 fraction A 0.035121046\n",
      "step 3633 loss 1.29385364 fisher_loss 0.228584558 triplet loss 1.06526911 l2_loss 20.5132256 fraction B 0.272952586 lossA 2.20059848 fraction A 0.0332134143\n",
      "step 3634 loss 0.878167689 fisher_loss 0.228187263 triplet loss 0.649980426 l2_loss 20.5123138 fraction B 0.202762574 lossA 2.23893285 fraction A 0.0350945666\n",
      "step 3635 loss 1.02542722 fisher_loss 0.228175953 triplet loss 0.797251225 l2_loss 20.5147953 fraction B 0.2546826 lossA 2.3347652 fraction A 0.040678151\n",
      "step 3636 loss 1.15105164 fisher_loss 0.228478506 triplet loss 0.92257309 l2_loss 20.5188599 fraction B 0.227089882 lossA 2.44624186 fraction A 0.0513044\n",
      "step 3637 loss 1.26572061 fisher_loss 0.229301542 triplet loss 1.03641903 l2_loss 20.52528 fraction B 0.268024892 lossA 2.58828688 fraction A 0.0633227602\n",
      "step 3638 loss 1.03271174 fisher_loss 0.230435356 triplet loss 0.802276433 l2_loss 20.5321102 fraction B 0.189011663 lossA 2.73682189 fraction A 0.0743286237\n",
      "step 3639 loss 1.05606914 fisher_loss 0.231569111 triplet loss 0.824499965 l2_loss 20.5380974 fraction B 0.198161811 lossA 2.81847048 fraction A 0.0793441683\n",
      "step 3640 loss 1.24045658 fisher_loss 0.23204942 triplet loss 1.00840712 l2_loss 20.5428257 fraction B 0.174688205 lossA 2.79278326 fraction A 0.0772436485\n",
      "step 3641 loss 1.13382101 fisher_loss 0.231641874 triplet loss 0.902179122 l2_loss 20.5456352 fraction B 0.317547679 lossA 2.7080338 fraction A 0.0711808726\n",
      "step 3642 loss 1.12796104 fisher_loss 0.23072286 triplet loss 0.897238135 l2_loss 20.5472355 fraction B 0.319487929 lossA 2.55114 fraction A 0.0599175394\n",
      "step 3643 loss 1.34727716 fisher_loss 0.229442626 triplet loss 1.11783457 l2_loss 20.5474415 fraction B 0.153246254 lossA 2.38937259 fraction A 0.0464651957\n",
      "step 3644 loss 0.906082094 fisher_loss 0.228153706 triplet loss 0.677928388 l2_loss 20.5470066 fraction B 0.152935341 lossA 2.31184268 fraction A 0.0391018093\n",
      "step 3645 loss 1.02560151 fisher_loss 0.227731317 triplet loss 0.797870159 l2_loss 20.5488815 fraction B 0.243027806 lossA 2.31420684 fraction A 0.0392450355\n",
      "step 3646 loss 0.997965693 fisher_loss 0.227880269 triplet loss 0.770085394 l2_loss 20.5528526 fraction B 0.300861359 lossA 2.35197 fraction A 0.042214971\n",
      "step 3647 loss 1.19195509 fisher_loss 0.228276044 triplet loss 0.963679075 l2_loss 20.5571728 fraction B 0.319846034 lossA 2.43232965 fraction A 0.048739545\n",
      "step 3648 loss 1.03617775 fisher_loss 0.22901924 triplet loss 0.80715853 l2_loss 20.5615959 fraction B 0.150599852 lossA 2.48778152 fraction A 0.0530100912\n",
      "step 3649 loss 0.968280733 fisher_loss 0.229835197 triplet loss 0.73844552 l2_loss 20.5669727 fraction B 0.157473311 lossA 2.54343843 fraction A 0.0570989437\n",
      "step 3650 loss 1.14424312 fisher_loss 0.230463684 triplet loss 0.913779438 l2_loss 20.5733566 fraction B 0.143924847 lossA 2.60647416 fraction A 0.0618925095\n",
      "step 3651 loss 1.11125135 fisher_loss 0.231252 triplet loss 0.879999399 l2_loss 20.5812225 fraction B 0.121962868 lossA 2.61028862 fraction A 0.0621533841\n",
      "step 3652 loss 0.994254231 fisher_loss 0.231375664 triplet loss 0.762878597 l2_loss 20.5875912 fraction B 0.185714856 lossA 2.56721401 fraction A 0.0586106256\n",
      "step 3653 loss 1.35169423 fisher_loss 0.231089711 triplet loss 1.12060452 l2_loss 20.594099 fraction B 0.167929187 lossA 2.44784832 fraction A 0.0495908149\n",
      "step 3654 loss 1.14309573 fisher_loss 0.230212569 triplet loss 0.912883222 l2_loss 20.5991096 fraction B 0.222614214 lossA 2.38782787 fraction A 0.0440328047\n",
      "step 3655 loss 0.868362725 fisher_loss 0.22979106 triplet loss 0.63857168 l2_loss 20.6040897 fraction B 0.159914359 lossA 2.37028 fraction A 0.0434141569\n",
      "step 3656 loss 1.12911141 fisher_loss 0.229686558 triplet loss 0.899424851 l2_loss 20.6099854 fraction B 0.262732565 lossA 2.36584353 fraction A 0.0436768457\n",
      "step 3657 loss 1.11337137 fisher_loss 0.229464874 triplet loss 0.883906543 l2_loss 20.6150055 fraction B 0.169026554 lossA 2.41423 fraction A 0.0489994772\n",
      "step 3658 loss 1.15115106 fisher_loss 0.229640767 triplet loss 0.921510279 l2_loss 20.6209126 fraction B 0.255587786 lossA 2.43582892 fraction A 0.0512310117\n",
      "step 3659 loss 1.10127115 fisher_loss 0.229553208 triplet loss 0.871718 l2_loss 20.6249847 fraction B 0.202162415 lossA 2.4224093 fraction A 0.0511432104\n",
      "step 3660 loss 1.1486094 fisher_loss 0.229421899 triplet loss 0.919187486 l2_loss 20.6290798 fraction B 0.133463621 lossA 2.38901329 fraction A 0.0499680378\n",
      "step 3661 loss 1.09213364 fisher_loss 0.229284734 triplet loss 0.862848878 l2_loss 20.6338024 fraction B 0.249812141 lossA 2.38619709 fraction A 0.0506698526\n",
      "step 3662 loss 1.29328275 fisher_loss 0.229309693 triplet loss 1.06397307 l2_loss 20.6380119 fraction B 0.266152948 lossA 2.41293263 fraction A 0.0532218106\n",
      "step 3663 loss 0.956301332 fisher_loss 0.229440928 triplet loss 0.726860404 l2_loss 20.6415443 fraction B 0.164669961 lossA 2.44122076 fraction A 0.0539716929\n",
      "step 3664 loss 0.930845261 fisher_loss 0.229705632 triplet loss 0.701139629 l2_loss 20.6450348 fraction B 0.143867925 lossA 2.47836637 fraction A 0.0550979674\n",
      "step 3665 loss 1.4155401 fisher_loss 0.2302773 triplet loss 1.1852628 l2_loss 20.6494446 fraction B 0.162489489 lossA 2.51061201 fraction A 0.056314975\n",
      "step 3666 loss 1.06127644 fisher_loss 0.230837196 triplet loss 0.83043921 l2_loss 20.6533451 fraction B 0.133342594 lossA 2.53374362 fraction A 0.056897074\n",
      "step 3667 loss 1.15496588 fisher_loss 0.231317729 triplet loss 0.923648179 l2_loss 20.657711 fraction B 0.210864976 lossA 2.54178023 fraction A 0.0555414408\n",
      "step 3668 loss 1.04181635 fisher_loss 0.231628895 triplet loss 0.810187459 l2_loss 20.6612301 fraction B 0.306634575 lossA 2.50615788 fraction A 0.0511227064\n",
      "step 3669 loss 0.873865664 fisher_loss 0.231396958 triplet loss 0.642468691 l2_loss 20.6640415 fraction B 0.114186384 lossA 2.4665544 fraction A 0.0482378229\n",
      "step 3670 loss 1.06841373 fisher_loss 0.231179953 triplet loss 0.837233782 l2_loss 20.6688728 fraction B 0.272395074 lossA 2.42130041 fraction A 0.0452905297\n",
      "step 3671 loss 1.14907789 fisher_loss 0.231170848 triplet loss 0.917907 l2_loss 20.6738815 fraction B 0.312825918 lossA 2.39128733 fraction A 0.0436808281\n",
      "step 3672 loss 0.979264438 fisher_loss 0.230961487 triplet loss 0.748302937 l2_loss 20.6787071 fraction B 0.229628697 lossA 2.33671927 fraction A 0.041014839\n",
      "step 3673 loss 1.15276468 fisher_loss 0.229886249 triplet loss 0.922878444 l2_loss 20.681757 fraction B 0.195414931 lossA 2.32386684 fraction A 0.0427153148\n",
      "step 3674 loss 0.934936345 fisher_loss 0.229645744 triplet loss 0.705290616 l2_loss 20.6875668 fraction B 0.161406 lossA 2.37713599 fraction A 0.050983578\n",
      "step 3675 loss 1.12775469 fisher_loss 0.230279058 triplet loss 0.89747566 l2_loss 20.6960411 fraction B 0.201594532 lossA 2.45767117 fraction A 0.0588161163\n",
      "step 3676 loss 0.937837303 fisher_loss 0.231108084 triplet loss 0.706729233 l2_loss 20.7034893 fraction B 0.144085929 lossA 2.51478052 fraction A 0.0635121\n",
      "step 3677 loss 1.2689141 fisher_loss 0.231859326 triplet loss 1.03705478 l2_loss 20.7104416 fraction B 0.257163495 lossA 2.518888 fraction A 0.0646611899\n",
      "step 3678 loss 1.07122838 fisher_loss 0.232061058 triplet loss 0.839167356 l2_loss 20.7159462 fraction B 0.247039348 lossA 2.49321485 fraction A 0.062837556\n",
      "step 3679 loss 1.09941983 fisher_loss 0.231722265 triplet loss 0.867697597 l2_loss 20.719965 fraction B 0.249773785 lossA 2.43030787 fraction A 0.0583624281\n",
      "step 3680 loss 1.08649516 fisher_loss 0.231355548 triplet loss 0.855139554 l2_loss 20.723547 fraction B 0.215759963 lossA 2.34300375 fraction A 0.0510763414\n",
      "step 3681 loss 1.01683795 fisher_loss 0.230734721 triplet loss 0.786103249 l2_loss 20.7271347 fraction B 0.215052 lossA 2.30006981 fraction A 0.0462244935\n",
      "step 3682 loss 1.15407467 fisher_loss 0.230442286 triplet loss 0.923632443 l2_loss 20.7307091 fraction B 0.181996092 lossA 2.2407496 fraction A 0.0409848467\n",
      "step 3683 loss 0.774158418 fisher_loss 0.230022371 triplet loss 0.544136047 l2_loss 20.7335091 fraction B 0.0801657885 lossA 2.27317667 fraction A 0.0433686636\n",
      "step 3684 loss 1.16866922 fisher_loss 0.23019819 triplet loss 0.938471 l2_loss 20.7399235 fraction B 0.280699134 lossA 2.32306409 fraction A 0.0480698943\n",
      "step 3685 loss 1.06075084 fisher_loss 0.230377063 triplet loss 0.830373824 l2_loss 20.746851 fraction B 0.217670932 lossA 2.41031027 fraction A 0.0561793223\n",
      "step 3686 loss 1.0862118 fisher_loss 0.230534658 triplet loss 0.855677187 l2_loss 20.7538414 fraction B 0.172442883 lossA 2.47035146 fraction A 0.0605431423\n",
      "step 3687 loss 1.13101411 fisher_loss 0.230644643 triplet loss 0.900369406 l2_loss 20.7603016 fraction B 0.229613096 lossA 2.47068882 fraction A 0.0599823408\n",
      "step 3688 loss 0.980325103 fisher_loss 0.230256751 triplet loss 0.750068367 l2_loss 20.7652855 fraction B 0.172010869 lossA 2.37987852 fraction A 0.0523470342\n",
      "step 3689 loss 1.00670779 fisher_loss 0.229321137 triplet loss 0.777386606 l2_loss 20.7688217 fraction B 0.171844468 lossA 2.37547183 fraction A 0.0508612506\n",
      "step 3690 loss 0.953014374 fisher_loss 0.229002208 triplet loss 0.724012196 l2_loss 20.7734985 fraction B 0.114411935 lossA 2.36377311 fraction A 0.0488500595\n",
      "step 3691 loss 1.08096516 fisher_loss 0.228717893 triplet loss 0.852247238 l2_loss 20.7794971 fraction B 0.254384488 lossA 2.34953022 fraction A 0.0467987694\n",
      "step 3692 loss 1.04984117 fisher_loss 0.228809014 triplet loss 0.821032166 l2_loss 20.7851143 fraction B 0.204060271 lossA 2.31833792 fraction A 0.0438098907\n",
      "step 3693 loss 1.08643782 fisher_loss 0.228937849 triplet loss 0.857499957 l2_loss 20.7905197 fraction B 0.278849304 lossA 2.27691174 fraction A 0.0403853245\n",
      "step 3694 loss 0.963564515 fisher_loss 0.228952765 triplet loss 0.73461175 l2_loss 20.7955036 fraction B 0.260809958 lossA 2.30328226 fraction A 0.0435751677\n",
      "step 3695 loss 1.04426241 fisher_loss 0.22954157 triplet loss 0.814720869 l2_loss 20.802412 fraction B 0.277127147 lossA 2.37448215 fraction A 0.0521662831\n",
      "step 3696 loss 1.15019667 fisher_loss 0.230380163 triplet loss 0.919816494 l2_loss 20.8096809 fraction B 0.275155604 lossA 2.50049186 fraction A 0.0630056337\n",
      "step 3697 loss 1.03434825 fisher_loss 0.231525108 triplet loss 0.802823186 l2_loss 20.816246 fraction B 0.225969642 lossA 2.58030367 fraction A 0.0696189925\n",
      "step 3698 loss 1.20870316 fisher_loss 0.232354954 triplet loss 0.976348221 l2_loss 20.8220615 fraction B 0.323904574 lossA 2.5553174 fraction A 0.0681550875\n",
      "step 3699 loss 0.966513753 fisher_loss 0.232165262 triplet loss 0.734348476 l2_loss 20.8258038 fraction B 0.106105782 lossA 2.37795401 fraction A 0.0552038439\n",
      "step 3700 loss 1.04134238 fisher_loss 0.231047347 triplet loss 0.810295045 l2_loss 20.8285866 fraction B 0.20340319 lossA 2.24571013 fraction A 0.0428809226\n",
      "step 3701 loss 1.04452527 fisher_loss 0.23012802 triplet loss 0.814397275 l2_loss 20.8315258 fraction B 0.103842385 lossA 2.15512061 fraction A 0.0360425152\n",
      "step 3702 loss 1.05840206 fisher_loss 0.229690373 triplet loss 0.828711689 l2_loss 20.834631 fraction B 0.180859178 lossA 2.12654185 fraction A 0.0339215323\n",
      "step 3703 loss 1.12176847 fisher_loss 0.229742616 triplet loss 0.892025828 l2_loss 20.8384209 fraction B 0.224763408 lossA 2.1614666 fraction A 0.0354722254\n",
      "step 3704 loss 1.21285379 fisher_loss 0.229998276 triplet loss 0.982855499 l2_loss 20.8433208 fraction B 0.306379318 lossA 2.21074224 fraction A 0.0379440263\n",
      "step 3705 loss 1.05520988 fisher_loss 0.230324239 triplet loss 0.824885607 l2_loss 20.8482037 fraction B 0.237208143 lossA 2.31512332 fraction A 0.0455517322\n",
      "step 3706 loss 1.1045984 fisher_loss 0.230996192 triplet loss 0.873602211 l2_loss 20.8543034 fraction B 0.236099496 lossA 2.45543194 fraction A 0.0576913282\n",
      "step 3707 loss 1.13254428 fisher_loss 0.23202166 triplet loss 0.900522649 l2_loss 20.8606453 fraction B 0.138545081 lossA 2.57012272 fraction A 0.0673525333\n",
      "step 3708 loss 1.16071165 fisher_loss 0.232869491 triplet loss 0.9278422 l2_loss 20.867836 fraction B 0.248762846 lossA 2.65114522 fraction A 0.0729514435\n",
      "step 3709 loss 1.01857364 fisher_loss 0.233410567 triplet loss 0.785163105 l2_loss 20.8733368 fraction B 0.207661539 lossA 2.57430363 fraction A 0.0672661066\n",
      "step 3710 loss 1.2096355 fisher_loss 0.232674912 triplet loss 0.97696054 l2_loss 20.8770123 fraction B 0.329557776 lossA 2.45197701 fraction A 0.0566904619\n",
      "step 3711 loss 1.12513924 fisher_loss 0.231555805 triplet loss 0.893583417 l2_loss 20.8792076 fraction B 0.195040017 lossA 2.30393934 fraction A 0.0428858846\n",
      "step 3712 loss 1.34773445 fisher_loss 0.230343074 triplet loss 1.11739135 l2_loss 20.8813 fraction B 0.213097796 lossA 2.20212722 fraction A 0.0356697477\n",
      "step 3713 loss 0.896944404 fisher_loss 0.229940087 triplet loss 0.667004347 l2_loss 20.8848553 fraction B 0.181254923 lossA 2.30026436 fraction A 0.0422769822\n",
      "step 3714 loss 1.16671062 fisher_loss 0.23067522 triplet loss 0.936035395 l2_loss 20.8919487 fraction B 0.199497566 lossA 2.41548848 fraction A 0.0528925471\n",
      "step 3715 loss 0.920341611 fisher_loss 0.231813371 triplet loss 0.68852824 l2_loss 20.8992615 fraction B 0.133899659 lossA 2.48293757 fraction A 0.057945177\n",
      "step 3716 loss 1.11769104 fisher_loss 0.232791811 triplet loss 0.884899199 l2_loss 20.9062138 fraction B 0.20107913 lossA 2.47323203 fraction A 0.0566101894\n",
      "step 3717 loss 1.08417988 fisher_loss 0.23301518 triplet loss 0.851164758 l2_loss 20.9113522 fraction B 0.222006 lossA 2.41446519 fraction A 0.0510759801\n",
      "step 3718 loss 0.901247561 fisher_loss 0.232652903 triplet loss 0.668594658 l2_loss 20.9155807 fraction B 0.136077657 lossA 2.35266757 fraction A 0.0443648957\n",
      "step 3719 loss 1.24621081 fisher_loss 0.232525408 triplet loss 1.01368535 l2_loss 20.9201279 fraction B 0.179 lossA 2.3190906 fraction A 0.0408880599\n",
      "step 3720 loss 1.02428734 fisher_loss 0.23213242 triplet loss 0.792154968 l2_loss 20.9237843 fraction B 0.210727811 lossA 2.32931209 fraction A 0.0414439701\n",
      "step 3721 loss 1.22342134 fisher_loss 0.232140943 triplet loss 0.991280377 l2_loss 20.9290771 fraction B 0.187805384 lossA 2.36395931 fraction A 0.0442707501\n",
      "step 3722 loss 1.07290411 fisher_loss 0.232074589 triplet loss 0.840829551 l2_loss 20.9339542 fraction B 0.139212161 lossA 2.45351529 fraction A 0.0535998307\n",
      "step 3723 loss 1.1453793 fisher_loss 0.232731968 triplet loss 0.912647367 l2_loss 20.9419956 fraction B 0.200707749 lossA 2.52270317 fraction A 0.060153313\n",
      "step 3724 loss 1.18507075 fisher_loss 0.233232841 triplet loss 0.951837897 l2_loss 20.9485645 fraction B 0.177212775 lossA 2.55946636 fraction A 0.0640306622\n",
      "step 3725 loss 0.963105798 fisher_loss 0.233416572 triplet loss 0.72968924 l2_loss 20.9545593 fraction B 0.144864202 lossA 2.55243707 fraction A 0.0638260394\n",
      "step 3726 loss 1.1983918 fisher_loss 0.233117431 triplet loss 0.965274334 l2_loss 20.9592609 fraction B 0.160635531 lossA 2.46080923 fraction A 0.0564140081\n",
      "step 3727 loss 1.26916599 fisher_loss 0.231889904 triplet loss 1.03727603 l2_loss 20.9616489 fraction B 0.228413269 lossA 2.35192513 fraction A 0.0457111821\n",
      "step 3728 loss 1.27436292 fisher_loss 0.230496 triplet loss 1.04386687 l2_loss 20.9623547 fraction B 0.23697181 lossA 2.31455779 fraction A 0.0413537733\n",
      "step 3729 loss 1.22014177 fisher_loss 0.22984387 triplet loss 0.990297914 l2_loss 20.9634457 fraction B 0.301531643 lossA 2.31161141 fraction A 0.0397447981\n",
      "step 3730 loss 1.14579272 fisher_loss 0.229730681 triplet loss 0.916062 l2_loss 20.9643631 fraction B 0.234598398 lossA 2.40689397 fraction A 0.0453472547\n",
      "step 3731 loss 1.17683268 fisher_loss 0.230268866 triplet loss 0.94656384 l2_loss 20.9673405 fraction B 0.216453403 lossA 2.53802395 fraction A 0.0538097434\n",
      "step 3732 loss 1.22641468 fisher_loss 0.231194809 triplet loss 0.995219827 l2_loss 20.9716339 fraction B 0.155407593 lossA 2.678262 fraction A 0.0656868741\n",
      "step 3733 loss 1.01839745 fisher_loss 0.232184 triplet loss 0.786213458 l2_loss 20.9780941 fraction B 0.177966774 lossA 2.81293678 fraction A 0.0765500888\n",
      "step 3734 loss 1.39062715 fisher_loss 0.232889742 triplet loss 1.15773737 l2_loss 20.9850178 fraction B 0.116016798 lossA 2.84517217 fraction A 0.0793575644\n",
      "step 3735 loss 1.03545845 fisher_loss 0.23291187 triplet loss 0.802546561 l2_loss 20.9899921 fraction B 0.184517473 lossA 2.77291775 fraction A 0.0751774311\n",
      "step 3736 loss 1.04743814 fisher_loss 0.232043028 triplet loss 0.815395057 l2_loss 20.9941139 fraction B 0.22290656 lossA 2.63242078 fraction A 0.0660784\n",
      "step 3737 loss 1.23618054 fisher_loss 0.230488658 triplet loss 1.00569189 l2_loss 20.996666 fraction B 0.212021619 lossA 2.44124889 fraction A 0.0531643629\n",
      "step 3738 loss 1.02250874 fisher_loss 0.228839681 triplet loss 0.793669105 l2_loss 20.9985123 fraction B 0.156007126 lossA 2.28239512 fraction A 0.0400671102\n",
      "step 3739 loss 0.931792498 fisher_loss 0.227844447 triplet loss 0.703948081 l2_loss 21.00033 fraction B 0.221669987 lossA 2.16588449 fraction A 0.0323672891\n",
      "step 3740 loss 0.925606847 fisher_loss 0.227622092 triplet loss 0.697984755 l2_loss 21.0027504 fraction B 0.222733945 lossA 2.23836374 fraction A 0.0365778506\n",
      "step 3741 loss 1.03537011 fisher_loss 0.228328332 triplet loss 0.807041764 l2_loss 21.0091228 fraction B 0.295775831 lossA 2.35688782 fraction A 0.047909826\n",
      "step 3742 loss 1.11202872 fisher_loss 0.22965689 triplet loss 0.882371783 l2_loss 21.016655 fraction B 0.199900314 lossA 2.51509953 fraction A 0.0591168\n",
      "step 3743 loss 1.13794219 fisher_loss 0.230886117 triplet loss 0.907056034 l2_loss 21.0231647 fraction B 0.269702733 lossA 2.68026948 fraction A 0.069514744\n",
      "step 3744 loss 1.09638846 fisher_loss 0.232222319 triplet loss 0.864166141 l2_loss 21.0294685 fraction B 0.145654827 lossA 2.76579857 fraction A 0.0744382\n",
      "step 3745 loss 1.12613297 fisher_loss 0.233066872 triplet loss 0.893066108 l2_loss 21.0350075 fraction B 0.250861406 lossA 2.77576065 fraction A 0.0742270201\n",
      "step 3746 loss 1.09785593 fisher_loss 0.233316332 triplet loss 0.864539564 l2_loss 21.0390549 fraction B 0.219629332 lossA 2.67899847 fraction A 0.0670524687\n",
      "step 3747 loss 1.19428921 fisher_loss 0.232798263 triplet loss 0.961490929 l2_loss 21.0415916 fraction B 0.306846648 lossA 2.53838277 fraction A 0.0562514402\n",
      "step 3748 loss 0.938990235 fisher_loss 0.231912211 triplet loss 0.70707804 l2_loss 21.0428486 fraction B 0.127283394 lossA 2.43719363 fraction A 0.0471457839\n",
      "step 3749 loss 1.06023729 fisher_loss 0.231365055 triplet loss 0.828872263 l2_loss 21.0447502 fraction B 0.151864976 lossA 2.34269786 fraction A 0.0401368365\n",
      "step 3750 loss 1.15830898 fisher_loss 0.231137529 triplet loss 0.927171469 l2_loss 21.0471802 fraction B 0.233938426 lossA 2.30985236 fraction A 0.0382739417\n",
      "step 3751 loss 0.727597237 fisher_loss 0.231184721 triplet loss 0.496412516 l2_loss 21.0506573 fraction B 0.0976762772 lossA 2.35464716 fraction A 0.040425241\n",
      "step 3752 loss 1.09950924 fisher_loss 0.231623352 triplet loss 0.867885828 l2_loss 21.0565701 fraction B 0.275523961 lossA 2.42593074 fraction A 0.0452060215\n",
      "step 3753 loss 1.03011823 fisher_loss 0.232065231 triplet loss 0.798052967 l2_loss 21.0630989 fraction B 0.199184775 lossA 2.57852817 fraction A 0.0567938052\n",
      "step 3754 loss 1.39014292 fisher_loss 0.233187467 triplet loss 1.15695548 l2_loss 21.0715427 fraction B 0.155777857 lossA 2.67166781 fraction A 0.0649155825\n",
      "step 3755 loss 0.975010753 fisher_loss 0.233870596 triplet loss 0.741140187 l2_loss 21.0776749 fraction B 0.137595743 lossA 2.71305132 fraction A 0.0684962\n",
      "step 3756 loss 1.11031497 fisher_loss 0.234158725 triplet loss 0.876156271 l2_loss 21.0828896 fraction B 0.236105159 lossA 2.65677929 fraction A 0.0651439279\n",
      "step 3757 loss 1.29715598 fisher_loss 0.233524323 triplet loss 1.06363165 l2_loss 21.0870953 fraction B 0.278990775 lossA 2.58914542 fraction A 0.0605627038\n",
      "step 3758 loss 0.947840333 fisher_loss 0.23263073 triplet loss 0.715209603 l2_loss 21.0905342 fraction B 0.198345527 lossA 2.47696495 fraction A 0.0523590259\n",
      "step 3759 loss 1.42805839 fisher_loss 0.231571823 triplet loss 1.19648659 l2_loss 21.0933971 fraction B 0.213492066 lossA 2.37795424 fraction A 0.0445089526\n",
      "step 3760 loss 1.08231354 fisher_loss 0.230631888 triplet loss 0.851681709 l2_loss 21.0954285 fraction B 0.248542741 lossA 2.31235814 fraction A 0.0397963449\n",
      "step 3761 loss 1.05564797 fisher_loss 0.230232507 triplet loss 0.825415432 l2_loss 21.0990543 fraction B 0.246082708 lossA 2.2411952 fraction A 0.0353462063\n",
      "step 3762 loss 1.02966154 fisher_loss 0.229629934 triplet loss 0.800031602 l2_loss 21.1029434 fraction B 0.231675282 lossA 2.26549411 fraction A 0.0368089862\n",
      "step 3763 loss 1.14674008 fisher_loss 0.229868799 triplet loss 0.916871309 l2_loss 21.1081715 fraction B 0.341409981 lossA 2.31226349 fraction A 0.0399774946\n",
      "step 3764 loss 1.26959336 fisher_loss 0.230087504 triplet loss 1.03950584 l2_loss 21.1136074 fraction B 0.192230582 lossA 2.34235191 fraction A 0.0425191671\n",
      "step 3765 loss 1.04309201 fisher_loss 0.230420604 triplet loss 0.812671363 l2_loss 21.1188183 fraction B 0.127541453 lossA 2.37247729 fraction A 0.0454299152\n",
      "step 3766 loss 1.07803917 fisher_loss 0.230841324 triplet loss 0.84719789 l2_loss 21.1244564 fraction B 0.161089256 lossA 2.381464 fraction A 0.0465243459\n",
      "step 3767 loss 1.49263954 fisher_loss 0.231047466 triplet loss 1.26159203 l2_loss 21.1287727 fraction B 0.207433447 lossA 2.38497615 fraction A 0.0477686673\n",
      "step 3768 loss 0.994547069 fisher_loss 0.231125847 triplet loss 0.763421237 l2_loss 21.1327591 fraction B 0.22139892 lossA 2.37331939 fraction A 0.0489249676\n",
      "step 3769 loss 0.973421395 fisher_loss 0.231191859 triplet loss 0.742229521 l2_loss 21.1380749 fraction B 0.173925161 lossA 2.35840273 fraction A 0.0496701039\n",
      "step 3770 loss 1.11535537 fisher_loss 0.231168523 triplet loss 0.884186804 l2_loss 21.1430225 fraction B 0.192352489 lossA 2.33489299 fraction A 0.0477472246\n",
      "step 3771 loss 1.16022217 fisher_loss 0.230882555 triplet loss 0.929339647 l2_loss 21.1476536 fraction B 0.240436211 lossA 2.30828166 fraction A 0.0453551486\n",
      "step 3772 loss 0.988569856 fisher_loss 0.230467737 triplet loss 0.758102119 l2_loss 21.1511497 fraction B 0.154903874 lossA 2.26784492 fraction A 0.0417668223\n",
      "step 3773 loss 1.04503989 fisher_loss 0.230023548 triplet loss 0.815016329 l2_loss 21.1550751 fraction B 0.17762281 lossA 2.26425719 fraction A 0.0411922224\n",
      "step 3774 loss 1.12370157 fisher_loss 0.229748368 triplet loss 0.893953264 l2_loss 21.1596146 fraction B 0.205765978 lossA 2.30579662 fraction A 0.0434030741\n",
      "step 3775 loss 0.994722 fisher_loss 0.229585618 triplet loss 0.765136421 l2_loss 21.1636868 fraction B 0.176437855 lossA 2.42171812 fraction A 0.0533272922\n",
      "step 3776 loss 0.846531451 fisher_loss 0.23017399 triplet loss 0.616357446 l2_loss 21.1702232 fraction B 0.113358572 lossA 2.50924754 fraction A 0.0594662055\n",
      "step 3777 loss 0.890435636 fisher_loss 0.230756178 triplet loss 0.659679472 l2_loss 21.1775494 fraction B 0.117275223 lossA 2.54347444 fraction A 0.0610394292\n",
      "step 3778 loss 0.99572289 fisher_loss 0.231122077 triplet loss 0.764600813 l2_loss 21.1849098 fraction B 0.20967944 lossA 2.52774477 fraction A 0.0586510487\n",
      "step 3779 loss 1.52698207 fisher_loss 0.230971903 triplet loss 1.29601014 l2_loss 21.1907082 fraction B 0.243314385 lossA 2.44986844 fraction A 0.0517086461\n",
      "step 3780 loss 0.835912228 fisher_loss 0.230464131 triplet loss 0.605448127 l2_loss 21.1950741 fraction B 0.108235627 lossA 2.33806801 fraction A 0.0419561937\n",
      "step 3781 loss 1.39493823 fisher_loss 0.229936853 triplet loss 1.16500139 l2_loss 21.1996384 fraction B 0.172639742 lossA 2.27896476 fraction A 0.037801452\n",
      "step 3782 loss 0.99600023 fisher_loss 0.229975685 triplet loss 0.76602453 l2_loss 21.205389 fraction B 0.171359733 lossA 2.2761817 fraction A 0.0373497903\n",
      "step 3783 loss 1.09535229 fisher_loss 0.230353072 triplet loss 0.864999175 l2_loss 21.2116089 fraction B 0.1896303 lossA 2.30337667 fraction A 0.0390969552\n",
      "step 3784 loss 0.762154 fisher_loss 0.230760977 triplet loss 0.531393 l2_loss 21.2177124 fraction B 0.160733387 lossA 2.41150498 fraction A 0.048132889\n",
      "step 3785 loss 1.13159573 fisher_loss 0.23200956 triplet loss 0.899586141 l2_loss 21.2267952 fraction B 0.272337049 lossA 2.51039767 fraction A 0.0564193353\n",
      "step 3786 loss 0.970385075 fisher_loss 0.233130202 triplet loss 0.737254858 l2_loss 21.2347355 fraction B 0.146666393 lossA 2.56302118 fraction A 0.0610445738\n",
      "step 3787 loss 1.03269064 fisher_loss 0.233823106 triplet loss 0.798867524 l2_loss 21.2425823 fraction B 0.155471876 lossA 2.55990934 fraction A 0.0607793145\n",
      "step 3788 loss 1.11834764 fisher_loss 0.233862281 triplet loss 0.884485424 l2_loss 21.2487411 fraction B 0.166166812 lossA 2.48355675 fraction A 0.0547818504\n",
      "step 3789 loss 1.04458189 fisher_loss 0.233370632 triplet loss 0.811211228 l2_loss 21.2534389 fraction B 0.180392072 lossA 2.36838984 fraction A 0.0464724749\n",
      "step 3790 loss 0.978983641 fisher_loss 0.232640594 triplet loss 0.746343076 l2_loss 21.2587509 fraction B 0.143157035 lossA 2.2925024 fraction A 0.040622402\n",
      "step 3791 loss 0.913110197 fisher_loss 0.232469499 triplet loss 0.680640697 l2_loss 21.2641525 fraction B 0.189312249 lossA 2.14258552 fraction A 0.0321953408\n",
      "step 3792 loss 1.35299587 fisher_loss 0.232268438 triplet loss 1.12072742 l2_loss 21.2681236 fraction B 0.248927668 lossA 1.99682176 fraction A 0.0274872594\n",
      "step 3793 loss 1.00155723 fisher_loss 0.232144058 triplet loss 0.769413173 l2_loss 21.2722797 fraction B 0.198211789 lossA 1.99266994 fraction A 0.0273275897\n",
      "step 3794 loss 0.844395399 fisher_loss 0.232304275 triplet loss 0.612091124 l2_loss 21.2777214 fraction B 0.145473912 lossA 2.16315293 fraction A 0.0331720486\n",
      "step 3795 loss 1.27547276 fisher_loss 0.233099848 triplet loss 1.04237294 l2_loss 21.2860909 fraction B 0.204914927 lossA 2.3424356 fraction A 0.0425790437\n",
      "step 3796 loss 1.11811876 fisher_loss 0.234094173 triplet loss 0.88402462 l2_loss 21.2944584 fraction B 0.263695896 lossA 2.53226161 fraction A 0.0567467511\n",
      "step 3797 loss 1.02861547 fisher_loss 0.235669971 triplet loss 0.792945445 l2_loss 21.3028297 fraction B 0.160761103 lossA 2.69815469 fraction A 0.0697196499\n",
      "step 3798 loss 1.13180184 fisher_loss 0.237036914 triplet loss 0.8947649 l2_loss 21.3115196 fraction B 0.232830673 lossA 2.78271317 fraction A 0.0754351243\n",
      "step 3799 loss 1.24549031 fisher_loss 0.237528503 triplet loss 1.00796175 l2_loss 21.3185368 fraction B 0.31591326 lossA 2.78086591 fraction A 0.0747025535\n",
      "step 3800 loss 1.23094106 fisher_loss 0.237135381 triplet loss 0.993805647 l2_loss 21.3229771 fraction B 0.291416049 lossA 2.67451715 fraction A 0.0656611547\n",
      "step 3801 loss 1.26775837 fisher_loss 0.235574082 triplet loss 1.03218424 l2_loss 21.3250103 fraction B 0.16354467 lossA 2.52124882 fraction A 0.0522823036\n",
      "step 3802 loss 1.06807053 fisher_loss 0.233564436 triplet loss 0.834506094 l2_loss 21.3254967 fraction B 0.170487583 lossA 2.28859115 fraction A 0.036036972\n",
      "step 3803 loss 0.984937489 fisher_loss 0.231821477 triplet loss 0.753116 l2_loss 21.3257713 fraction B 0.289713055 lossA 2.0777 fraction A 0.0284799784\n",
      "step 3804 loss 1.10814643 fisher_loss 0.231090844 triplet loss 0.877055645 l2_loss 21.3282528 fraction B 0.226591453 lossA 2.02957249 fraction A 0.0272182692\n",
      "step 3805 loss 1.0035187 fisher_loss 0.231008604 triplet loss 0.772510052 l2_loss 21.334 fraction B 0.318937838 lossA 2.09031487 fraction A 0.0288480017\n",
      "step 3806 loss 1.08026481 fisher_loss 0.231178373 triplet loss 0.849086404 l2_loss 21.340826 fraction B 0.249400526 lossA 2.26157069 fraction A 0.0348159075\n",
      "step 3807 loss 0.983612 fisher_loss 0.231663361 triplet loss 0.751948655 l2_loss 21.3484402 fraction B 0.23754631 lossA 2.39489746 fraction A 0.042414628\n",
      "step 3808 loss 1.22038591 fisher_loss 0.232402816 triplet loss 0.987983048 l2_loss 21.3564987 fraction B 0.272036076 lossA 2.52520156 fraction A 0.05242984\n",
      "step 3809 loss 1.12075162 fisher_loss 0.233442 triplet loss 0.88730967 l2_loss 21.3647594 fraction B 0.195538431 lossA 2.60157275 fraction A 0.0585809238\n",
      "step 3810 loss 1.01297867 fisher_loss 0.234566107 triplet loss 0.778412521 l2_loss 21.3720703 fraction B 0.165825486 lossA 2.63429427 fraction A 0.0613094345\n",
      "step 3811 loss 1.20026815 fisher_loss 0.235366777 triplet loss 0.964901388 l2_loss 21.3784199 fraction B 0.164313644 lossA 2.5954802 fraction A 0.0588971823\n",
      "step 3812 loss 1.08814824 fisher_loss 0.235511988 triplet loss 0.852636278 l2_loss 21.3833237 fraction B 0.17737633 lossA 2.53394127 fraction A 0.0558202118\n",
      "step 3813 loss 0.976289272 fisher_loss 0.235288426 triplet loss 0.741000831 l2_loss 21.3883152 fraction B 0.184281439 lossA 2.44741 fraction A 0.050538253\n",
      "step 3814 loss 1.11294293 fisher_loss 0.234865546 triplet loss 0.878077328 l2_loss 21.3934784 fraction B 0.365204602 lossA 2.33355546 fraction A 0.0429599248\n",
      "step 3815 loss 1.11713433 fisher_loss 0.234080479 triplet loss 0.883053899 l2_loss 21.3973885 fraction B 0.198174253 lossA 2.3075757 fraction A 0.0424969643\n",
      "step 3816 loss 1.06654966 fisher_loss 0.233924314 triplet loss 0.832625389 l2_loss 21.403511 fraction B 0.184301347 lossA 2.33727217 fraction A 0.0465630665\n",
      "step 3817 loss 1.11003411 fisher_loss 0.234123111 triplet loss 0.875911 l2_loss 21.410759 fraction B 0.193806499 lossA 2.35437608 fraction A 0.0498985276\n",
      "step 3818 loss 1.51602209 fisher_loss 0.234200239 triplet loss 1.28182185 l2_loss 21.4175091 fraction B 0.262666732 lossA 2.33144164 fraction A 0.0489518456\n",
      "step 3819 loss 1.24225414 fisher_loss 0.233809471 triplet loss 1.00844467 l2_loss 21.4221268 fraction B 0.238913462 lossA 2.30116749 fraction A 0.0469646677\n",
      "step 3820 loss 0.879687726 fisher_loss 0.233209252 triplet loss 0.646478474 l2_loss 21.4253883 fraction B 0.171719894 lossA 2.3267169 fraction A 0.0485041328\n",
      "step 3821 loss 0.828827679 fisher_loss 0.232964694 triplet loss 0.595863 l2_loss 21.4292221 fraction B 0.184375 lossA 2.4208982 fraction A 0.0552004054\n",
      "step 3822 loss 1.0017972 fisher_loss 0.233383402 triplet loss 0.768413782 l2_loss 21.4344196 fraction B 0.136099428 lossA 2.51800847 fraction A 0.0610313527\n",
      "step 3823 loss 1.05622673 fisher_loss 0.234092131 triplet loss 0.822134554 l2_loss 21.43964 fraction B 0.152786881 lossA 2.58172822 fraction A 0.0643588\n",
      "step 3824 loss 0.920846581 fisher_loss 0.234639436 triplet loss 0.686207116 l2_loss 21.4444561 fraction B 0.20032607 lossA 2.57202625 fraction A 0.061356198\n",
      "step 3825 loss 1.04438877 fisher_loss 0.234921739 triplet loss 0.809467 l2_loss 21.4485626 fraction B 0.175870463 lossA 2.52729201 fraction A 0.0555472709\n",
      "step 3826 loss 0.97139442 fisher_loss 0.234916225 triplet loss 0.736478209 l2_loss 21.4524326 fraction B 0.105816439 lossA 2.48005342 fraction A 0.0504758805\n",
      "step 3827 loss 0.911796927 fisher_loss 0.234872818 triplet loss 0.676924109 l2_loss 21.4565048 fraction B 0.13132976 lossA 2.46896648 fraction A 0.0492654629\n",
      "step 3828 loss 0.959435582 fisher_loss 0.234774023 triplet loss 0.724661529 l2_loss 21.4634495 fraction B 0.2217623 lossA 2.43943787 fraction A 0.0471540429\n",
      "step 3829 loss 1.24938536 fisher_loss 0.234417871 triplet loss 1.01496744 l2_loss 21.4707699 fraction B 0.151817814 lossA 2.40101385 fraction A 0.0444955491\n",
      "step 3830 loss 1.10896385 fisher_loss 0.234193891 triplet loss 0.87477 l2_loss 21.4783821 fraction B 0.197453 lossA 2.42589021 fraction A 0.0463792756\n",
      "step 3831 loss 1.21081805 fisher_loss 0.234450281 triplet loss 0.976367772 l2_loss 21.4872398 fraction B 0.255862266 lossA 2.47567654 fraction A 0.0501830913\n",
      "step 3832 loss 1.54223859 fisher_loss 0.234829023 triplet loss 1.30740952 l2_loss 21.4955311 fraction B 0.396253407 lossA 2.50584054 fraction A 0.0524713136\n",
      "step 3833 loss 1.04955316 fisher_loss 0.234723508 triplet loss 0.814829707 l2_loss 21.5013809 fraction B 0.199415013 lossA 2.5508821 fraction A 0.0563625358\n",
      "step 3834 loss 0.961090446 fisher_loss 0.234355852 triplet loss 0.726734579 l2_loss 21.5068417 fraction B 0.213041246 lossA 2.5505321 fraction A 0.0570388138\n",
      "step 3835 loss 1.21949744 fisher_loss 0.233694598 triplet loss 0.985802829 l2_loss 21.5121803 fraction B 0.329583108 lossA 2.52759 fraction A 0.0560244694\n",
      "step 3836 loss 1.05348992 fisher_loss 0.232976422 triplet loss 0.820513487 l2_loss 21.5171089 fraction B 0.194059819 lossA 2.47222447 fraction A 0.0525679551\n",
      "step 3837 loss 1.07115102 fisher_loss 0.23229599 triplet loss 0.838855 l2_loss 21.5218029 fraction B 0.173980802 lossA 2.39050841 fraction A 0.0461558886\n",
      "step 3838 loss 1.18839383 fisher_loss 0.231413111 triplet loss 0.956980765 l2_loss 21.5257511 fraction B 0.310906291 lossA 2.26857352 fraction A 0.0369660109\n",
      "step 3839 loss 1.27908683 fisher_loss 0.23063843 triplet loss 1.04844844 l2_loss 21.5286827 fraction B 0.245875075 lossA 2.19028544 fraction A 0.0333638825\n",
      "step 3840 loss 1.0005995 fisher_loss 0.230227947 triplet loss 0.770371556 l2_loss 21.5327301 fraction B 0.303673476 lossA 2.19949412 fraction A 0.0345966518\n",
      "step 3841 loss 0.998393416 fisher_loss 0.229919091 triplet loss 0.76847434 l2_loss 21.5365982 fraction B 0.24533388 lossA 2.26933885 fraction A 0.0397218689\n",
      "step 3842 loss 0.853667498 fisher_loss 0.229909509 triplet loss 0.623758 l2_loss 21.5412445 fraction B 0.181379959 lossA 2.36872029 fraction A 0.0482841544\n",
      "step 3843 loss 0.974877954 fisher_loss 0.230447382 triplet loss 0.744430542 l2_loss 21.5472279 fraction B 0.254340172 lossA 2.44816256 fraction A 0.0548153557\n",
      "step 3844 loss 1.33621645 fisher_loss 0.230918407 triplet loss 1.10529804 l2_loss 21.5531788 fraction B 0.177732438 lossA 2.50278521 fraction A 0.0579073243\n",
      "step 3845 loss 0.986072421 fisher_loss 0.231104523 triplet loss 0.754967868 l2_loss 21.5576687 fraction B 0.130838782 lossA 2.50475526 fraction A 0.0569523498\n",
      "step 3846 loss 1.08650494 fisher_loss 0.231125057 triplet loss 0.855379879 l2_loss 21.5618382 fraction B 0.141407967 lossA 2.48035574 fraction A 0.0540901795\n",
      "step 3847 loss 0.829841912 fisher_loss 0.231052935 triplet loss 0.598789 l2_loss 21.565403 fraction B 0.0917531177 lossA 2.38165402 fraction A 0.0463449359\n",
      "step 3848 loss 1.09228086 fisher_loss 0.230497122 triplet loss 0.861783803 l2_loss 21.57024 fraction B 0.119271852 lossA 2.33088017 fraction A 0.0410935506\n",
      "step 3849 loss 1.26378882 fisher_loss 0.230129093 triplet loss 1.0336597 l2_loss 21.5749722 fraction B 0.193050027 lossA 2.31633568 fraction A 0.0398073196\n",
      "step 3850 loss 1.15764475 fisher_loss 0.229932487 triplet loss 0.927712262 l2_loss 21.5801945 fraction B 0.247703448 lossA 2.29945779 fraction A 0.0383416042\n",
      "step 3851 loss 0.973204136 fisher_loss 0.229751259 triplet loss 0.743452847 l2_loss 21.5845909 fraction B 0.133519053 lossA 2.31444502 fraction A 0.0400154926\n",
      "step 3852 loss 1.22749436 fisher_loss 0.229853034 triplet loss 0.997641325 l2_loss 21.5904388 fraction B 0.187750682 lossA 2.3458755 fraction A 0.0429268889\n",
      "step 3853 loss 1.15587568 fisher_loss 0.229957923 triplet loss 0.925917745 l2_loss 21.5952892 fraction B 0.218560427 lossA 2.40174747 fraction A 0.0476662666\n",
      "step 3854 loss 0.99064517 fisher_loss 0.230078459 triplet loss 0.760566711 l2_loss 21.5990143 fraction B 0.147540018 lossA 2.54973745 fraction A 0.0591980107\n",
      "step 3855 loss 1.15576899 fisher_loss 0.230678767 triplet loss 0.925090194 l2_loss 21.6040344 fraction B 0.223483726 lossA 2.65929389 fraction A 0.0679690838\n",
      "step 3856 loss 1.22585726 fisher_loss 0.230883285 triplet loss 0.994974 l2_loss 21.6089783 fraction B 0.175058275 lossA 2.64816451 fraction A 0.0674152\n",
      "step 3857 loss 0.925365686 fisher_loss 0.230215177 triplet loss 0.695150495 l2_loss 21.6122437 fraction B 0.119159155 lossA 2.55148745 fraction A 0.0604758039\n",
      "step 3858 loss 1.1561085 fisher_loss 0.229037061 triplet loss 0.927071452 l2_loss 21.6153355 fraction B 0.227708161 lossA 2.45962548 fraction A 0.053431794\n",
      "step 3859 loss 1.04163671 fisher_loss 0.227944747 triplet loss 0.813691914 l2_loss 21.6177311 fraction B 0.18597208 lossA 2.39444 fraction A 0.0476607606\n",
      "step 3860 loss 0.807929516 fisher_loss 0.227017045 triplet loss 0.580912471 l2_loss 21.6199932 fraction B 0.137903556 lossA 2.35125971 fraction A 0.0440459885\n",
      "step 3861 loss 1.0888226 fisher_loss 0.226872578 triplet loss 0.86195 l2_loss 21.625042 fraction B 0.236544624 lossA 2.3755765 fraction A 0.0466236696\n",
      "step 3862 loss 0.813737154 fisher_loss 0.227247268 triplet loss 0.586489916 l2_loss 21.6315498 fraction B 0.130148888 lossA 2.45858026 fraction A 0.0542044267\n",
      "step 3863 loss 1.07988656 fisher_loss 0.228450015 triplet loss 0.851436496 l2_loss 21.6403294 fraction B 0.231903598 lossA 2.54087305 fraction A 0.0616994053\n",
      "step 3864 loss 0.8907426 fisher_loss 0.22969538 triplet loss 0.66104722 l2_loss 21.6497688 fraction B 0.221362486 lossA 2.52939391 fraction A 0.0616412684\n",
      "step 3865 loss 1.04417074 fisher_loss 0.2301182 triplet loss 0.814052582 l2_loss 21.6578522 fraction B 0.161571532 lossA 2.51014853 fraction A 0.060974773\n",
      "step 3866 loss 1.08258355 fisher_loss 0.230118513 triplet loss 0.852465034 l2_loss 21.6657181 fraction B 0.196803972 lossA 2.48762846 fraction A 0.0598535687\n",
      "step 3867 loss 1.09814918 fisher_loss 0.230049938 triplet loss 0.868099272 l2_loss 21.6733017 fraction B 0.192524344 lossA 2.41903782 fraction A 0.0543192141\n",
      "step 3868 loss 1.27564216 fisher_loss 0.229715765 triplet loss 1.04592645 l2_loss 21.6795979 fraction B 0.259240508 lossA 2.35463524 fraction A 0.0489508323\n",
      "step 3869 loss 1.20120335 fisher_loss 0.229242414 triplet loss 0.971960902 l2_loss 21.6851444 fraction B 0.279407412 lossA 2.35833883 fraction A 0.0491250642\n",
      "step 3870 loss 1.31553364 fisher_loss 0.228906676 triplet loss 1.08662701 l2_loss 21.6910667 fraction B 0.294055521 lossA 2.38679123 fraction A 0.0517260693\n",
      "step 3871 loss 1.13600397 fisher_loss 0.228632107 triplet loss 0.907371879 l2_loss 21.6965046 fraction B 0.202553049 lossA 2.46590757 fraction A 0.0573999174\n",
      "step 3872 loss 1.08131933 fisher_loss 0.228918836 triplet loss 0.852400482 l2_loss 21.7017956 fraction B 0.196428567 lossA 2.54901385 fraction A 0.0627735183\n",
      "step 3873 loss 1.0401777 fisher_loss 0.229258075 triplet loss 0.810919583 l2_loss 21.7068863 fraction B 0.1773054 lossA 2.56156826 fraction A 0.0627446845\n",
      "step 3874 loss 1.22026694 fisher_loss 0.229353949 triplet loss 0.990913033 l2_loss 21.7110596 fraction B 0.203488365 lossA 2.54854131 fraction A 0.0607977137\n",
      "step 3875 loss 0.862321258 fisher_loss 0.229358763 triplet loss 0.632962465 l2_loss 21.7143021 fraction B 0.129213 lossA 2.46449614 fraction A 0.0537500493\n",
      "step 3876 loss 1.07739413 fisher_loss 0.229067177 triplet loss 0.848327 l2_loss 21.716917 fraction B 0.128372133 lossA 2.40571404 fraction A 0.0478378162\n",
      "step 3877 loss 1.14657354 fisher_loss 0.228759959 triplet loss 0.91781354 l2_loss 21.7192745 fraction B 0.172242433 lossA 2.37233853 fraction A 0.0445473492\n",
      "step 3878 loss 0.869739532 fisher_loss 0.228760079 triplet loss 0.640979469 l2_loss 21.7228756 fraction B 0.109261267 lossA 2.38456154 fraction A 0.0460306667\n",
      "step 3879 loss 1.06184971 fisher_loss 0.229096621 triplet loss 0.832753062 l2_loss 21.7283421 fraction B 0.189833373 lossA 2.39557385 fraction A 0.0474327765\n",
      "step 3880 loss 1.07564723 fisher_loss 0.229590207 triplet loss 0.846057 l2_loss 21.7339306 fraction B 0.194490641 lossA 2.4555397 fraction A 0.0532173216\n",
      "step 3881 loss 1.30129468 fisher_loss 0.230573654 triplet loss 1.07072103 l2_loss 21.7402439 fraction B 0.222835824 lossA 2.48502135 fraction A 0.0563241765\n",
      "step 3882 loss 1.06919789 fisher_loss 0.231340736 triplet loss 0.837857127 l2_loss 21.7455883 fraction B 0.211630449 lossA 2.47866869 fraction A 0.0560532324\n",
      "step 3883 loss 1.08502793 fisher_loss 0.231861398 triplet loss 0.853166521 l2_loss 21.7495499 fraction B 0.191451639 lossA 2.42466402 fraction A 0.0524461381\n",
      "step 3884 loss 1.1163125 fisher_loss 0.232019678 triplet loss 0.884292781 l2_loss 21.7523518 fraction B 0.176799908 lossA 2.39024878 fraction A 0.0498792902\n",
      "step 3885 loss 1.3417865 fisher_loss 0.23231259 triplet loss 1.10947394 l2_loss 21.7553234 fraction B 0.139077812 lossA 2.33532619 fraction A 0.0463544242\n",
      "step 3886 loss 1.14947069 fisher_loss 0.232188627 triplet loss 0.917282045 l2_loss 21.7580223 fraction B 0.146490708 lossA 2.29046345 fraction A 0.0430654436\n",
      "step 3887 loss 0.829520524 fisher_loss 0.23191537 triplet loss 0.597605169 l2_loss 21.7615929 fraction B 0.0996269 lossA 2.30085087 fraction A 0.0448819026\n",
      "step 3888 loss 1.20643413 fisher_loss 0.232126832 triplet loss 0.974307299 l2_loss 21.7676144 fraction B 0.200144067 lossA 2.32885337 fraction A 0.0483243614\n",
      "step 3889 loss 0.809429646 fisher_loss 0.232460245 triplet loss 0.576969385 l2_loss 21.7731438 fraction B 0.0730502903 lossA 2.43529582 fraction A 0.0563024804\n",
      "step 3890 loss 0.96003592 fisher_loss 0.233105198 triplet loss 0.726930737 l2_loss 21.7799664 fraction B 0.181657329 lossA 2.51173663 fraction A 0.0599651\n",
      "step 3891 loss 0.836940944 fisher_loss 0.233414844 triplet loss 0.603526115 l2_loss 21.7859154 fraction B 0.0979019552 lossA 2.43787408 fraction A 0.0515164472\n",
      "step 3892 loss 1.11200011 fisher_loss 0.232664242 triplet loss 0.87933588 l2_loss 21.790266 fraction B 0.186798334 lossA 2.34202194 fraction A 0.0414491855\n",
      "step 3893 loss 0.869590282 fisher_loss 0.232098848 triplet loss 0.637491465 l2_loss 21.7938137 fraction B 0.125581682 lossA 2.2770915 fraction A 0.0362323597\n",
      "step 3894 loss 0.968749106 fisher_loss 0.23241435 triplet loss 0.736334741 l2_loss 21.7988377 fraction B 0.165706694 lossA 2.25090289 fraction A 0.0348051488\n",
      "step 3895 loss 1.13190854 fisher_loss 0.233291805 triplet loss 0.898616731 l2_loss 21.8069859 fraction B 0.22505179 lossA 2.31779027 fraction A 0.0380354151\n",
      "step 3896 loss 0.986771643 fisher_loss 0.233955368 triplet loss 0.75281626 l2_loss 21.8147335 fraction B 0.260662824 lossA 2.4300406 fraction A 0.0453881845\n",
      "step 3897 loss 1.16014051 fisher_loss 0.234856412 triplet loss 0.925284088 l2_loss 21.8241024 fraction B 0.163653 lossA 2.53819108 fraction A 0.0536376834\n",
      "step 3898 loss 0.977413595 fisher_loss 0.235525906 triplet loss 0.741887689 l2_loss 21.8336105 fraction B 0.25087145 lossA 2.54073763 fraction A 0.0564476959\n",
      "step 3899 loss 1.37869465 fisher_loss 0.235621721 triplet loss 1.14307296 l2_loss 21.8431568 fraction B 0.244621217 lossA 2.48097134 fraction A 0.0545931682\n",
      "step 3900 loss 1.12829316 fisher_loss 0.235089272 triplet loss 0.893203855 l2_loss 21.8507404 fraction B 0.148055792 lossA 2.39002848 fraction A 0.0496943034\n",
      "step 3901 loss 1.03097618 fisher_loss 0.234378844 triplet loss 0.796597302 l2_loss 21.8567619 fraction B 0.262889653 lossA 2.30714345 fraction A 0.0446701422\n",
      "step 3902 loss 1.20260072 fisher_loss 0.233813241 triplet loss 0.968787432 l2_loss 21.8626747 fraction B 0.195523083 lossA 2.24396324 fraction A 0.0416703261\n",
      "step 3903 loss 1.0092634 fisher_loss 0.233564332 triplet loss 0.775699079 l2_loss 21.8685493 fraction B 0.187732711 lossA 2.20659828 fraction A 0.0410018712\n",
      "step 3904 loss 1.08932877 fisher_loss 0.233412892 triplet loss 0.855915844 l2_loss 21.8748894 fraction B 0.198991567 lossA 2.25425386 fraction A 0.0473213531\n",
      "step 3905 loss 1.23541689 fisher_loss 0.233505443 triplet loss 1.0019114 l2_loss 21.8815918 fraction B 0.210139051 lossA 2.32662892 fraction A 0.0548922941\n",
      "step 3906 loss 1.12988043 fisher_loss 0.233583808 triplet loss 0.89629662 l2_loss 21.8869648 fraction B 0.238144323 lossA 2.38983822 fraction A 0.0605826229\n",
      "step 3907 loss 0.99373 fisher_loss 0.233558252 triplet loss 0.760171771 l2_loss 21.8904076 fraction B 0.126005813 lossA 2.39709115 fraction A 0.0609597377\n",
      "step 3908 loss 1.02055097 fisher_loss 0.233224362 triplet loss 0.787326634 l2_loss 21.892683 fraction B 0.155318707 lossA 2.38226748 fraction A 0.0596651025\n",
      "step 3909 loss 1.17378914 fisher_loss 0.232753739 triplet loss 0.94103545 l2_loss 21.8952808 fraction B 0.287795871 lossA 2.34421015 fraction A 0.055718895\n",
      "step 3910 loss 1.26397073 fisher_loss 0.232074603 triplet loss 1.03189611 l2_loss 21.896841 fraction B 0.256131589 lossA 2.2944243 fraction A 0.0499730371\n",
      "step 3911 loss 1.14077675 fisher_loss 0.231329188 triplet loss 0.909447551 l2_loss 21.8969135 fraction B 0.274599195 lossA 2.2532177 fraction A 0.0445287302\n",
      "step 3912 loss 1.03153276 fisher_loss 0.230784386 triplet loss 0.800748348 l2_loss 21.897047 fraction B 0.209601238 lossA 2.23060083 fraction A 0.0419143923\n",
      "step 3913 loss 1.01014829 fisher_loss 0.230493531 triplet loss 0.779654741 l2_loss 21.8982754 fraction B 0.182851717 lossA 2.28514099 fraction A 0.0463827141\n",
      "step 3914 loss 0.956384718 fisher_loss 0.230674028 triplet loss 0.72571069 l2_loss 21.9032364 fraction B 0.228875145 lossA 2.38830948 fraction A 0.0566639826\n",
      "step 3915 loss 0.910819769 fisher_loss 0.231236368 triplet loss 0.67958343 l2_loss 21.9113483 fraction B 0.267871946 lossA 2.4471581 fraction A 0.0625561848\n",
      "step 3916 loss 1.00985563 fisher_loss 0.231546417 triplet loss 0.778309166 l2_loss 21.9189663 fraction B 0.222087279 lossA 2.48962951 fraction A 0.0658119097\n",
      "step 3917 loss 1.12332082 fisher_loss 0.231765032 triplet loss 0.891555727 l2_loss 21.9253159 fraction B 0.207629681 lossA 2.47090054 fraction A 0.0648025349\n",
      "step 3918 loss 0.99167335 fisher_loss 0.231660694 triplet loss 0.760012686 l2_loss 21.930172 fraction B 0.193454117 lossA 2.37246752 fraction A 0.0578300618\n",
      "step 3919 loss 0.980376 fisher_loss 0.23113431 triplet loss 0.74924171 l2_loss 21.9346466 fraction B 0.163631171 lossA 2.28053212 fraction A 0.0508717559\n",
      "step 3920 loss 1.14758956 fisher_loss 0.230785951 triplet loss 0.916803598 l2_loss 21.9401188 fraction B 0.244907215 lossA 2.21484 fraction A 0.0443849638\n",
      "step 3921 loss 0.94508 fisher_loss 0.230387866 triplet loss 0.714692116 l2_loss 21.9444332 fraction B 0.0972741321 lossA 2.21395659 fraction A 0.0435765423\n",
      "step 3922 loss 1.20036852 fisher_loss 0.230249569 triplet loss 0.970119 l2_loss 21.9501839 fraction B 0.182525054 lossA 2.26831889 fraction A 0.0473280177\n",
      "step 3923 loss 0.921279073 fisher_loss 0.230537802 triplet loss 0.690741301 l2_loss 21.9561806 fraction B 0.135344431 lossA 2.32592702 fraction A 0.0507118329\n",
      "step 3924 loss 1.05129337 fisher_loss 0.231012225 triplet loss 0.820281208 l2_loss 21.9621716 fraction B 0.231272116 lossA 2.37486935 fraction A 0.0521419793\n",
      "step 3925 loss 0.998368561 fisher_loss 0.231489107 triplet loss 0.766879439 l2_loss 21.9668 fraction B 0.0981618688 lossA 2.44772267 fraction A 0.054534886\n",
      "step 3926 loss 1.27404356 fisher_loss 0.232414305 triplet loss 1.04162931 l2_loss 21.9727726 fraction B 0.183108658 lossA 2.51787281 fraction A 0.057605736\n",
      "step 3927 loss 1.08295238 fisher_loss 0.233350143 triplet loss 0.849602282 l2_loss 21.9788284 fraction B 0.222025454 lossA 2.54217935 fraction A 0.0580256656\n",
      "step 3928 loss 0.889890313 fisher_loss 0.233825058 triplet loss 0.656065226 l2_loss 21.9844513 fraction B 0.153499022 lossA 2.52176 fraction A 0.0550396107\n",
      "step 3929 loss 1.12123382 fisher_loss 0.23383294 triplet loss 0.887400866 l2_loss 21.9897461 fraction B 0.217877924 lossA 2.47311974 fraction A 0.0512400344\n",
      "step 3930 loss 1.17582691 fisher_loss 0.233339682 triplet loss 0.94248718 l2_loss 21.9949207 fraction B 0.119394317 lossA 2.4392755 fraction A 0.0501378477\n",
      "step 3931 loss 1.57905042 fisher_loss 0.232724696 triplet loss 1.34632576 l2_loss 22.0017376 fraction B 0.198703274 lossA 2.3761313 fraction A 0.0466190688\n",
      "step 3932 loss 1.06338871 fisher_loss 0.231801122 triplet loss 0.831587613 l2_loss 22.0069237 fraction B 0.216497883 lossA 2.3742559 fraction A 0.0482423156\n",
      "step 3933 loss 1.23437595 fisher_loss 0.231481045 triplet loss 1.00289488 l2_loss 22.0132236 fraction B 0.21290651 lossA 2.37044787 fraction A 0.0497776158\n",
      "step 3934 loss 1.07345152 fisher_loss 0.231307104 triplet loss 0.84214437 l2_loss 22.0191784 fraction B 0.195065156 lossA 2.35469055 fraction A 0.0497933365\n",
      "step 3935 loss 1.00760126 fisher_loss 0.231151685 triplet loss 0.776449621 l2_loss 22.0246277 fraction B 0.176289305 lossA 2.34364986 fraction A 0.0508369096\n",
      "step 3936 loss 1.01732886 fisher_loss 0.230843142 triplet loss 0.786485672 l2_loss 22.029583 fraction B 0.190475211 lossA 2.38318253 fraction A 0.0551139414\n",
      "step 3937 loss 1.15450692 fisher_loss 0.231017888 triplet loss 0.923489 l2_loss 22.0346622 fraction B 0.228861615 lossA 2.41144824 fraction A 0.0574905127\n",
      "step 3938 loss 1.13840497 fisher_loss 0.231176332 triplet loss 0.907228649 l2_loss 22.0388165 fraction B 0.177227885 lossA 2.44069314 fraction A 0.0588498414\n",
      "step 3939 loss 1.19780266 fisher_loss 0.231263444 triplet loss 0.966539204 l2_loss 22.0423241 fraction B 0.0971492901 lossA 2.38593102 fraction A 0.0536208041\n",
      "step 3940 loss 1.05500126 fisher_loss 0.230765656 triplet loss 0.824235618 l2_loss 22.0455761 fraction B 0.233251363 lossA 2.35202789 fraction A 0.0498364754\n",
      "step 3941 loss 0.968804538 fisher_loss 0.230456248 triplet loss 0.738348305 l2_loss 22.0492496 fraction B 0.16753982 lossA 2.31173873 fraction A 0.045943588\n",
      "step 3942 loss 1.17658424 fisher_loss 0.230091646 triplet loss 0.946492612 l2_loss 22.0533676 fraction B 0.188818395 lossA 2.27238894 fraction A 0.0424205326\n",
      "step 3943 loss 0.997252464 fisher_loss 0.229622096 triplet loss 0.767630339 l2_loss 22.0576916 fraction B 0.167498291 lossA 2.28363323 fraction A 0.0432252213\n",
      "step 3944 loss 1.14934909 fisher_loss 0.229277804 triplet loss 0.920071304 l2_loss 22.0646229 fraction B 0.153149113 lossA 2.32479119 fraction A 0.0476796702\n",
      "step 3945 loss 0.987844765 fisher_loss 0.229341686 triplet loss 0.758503079 l2_loss 22.0721779 fraction B 0.141657546 lossA 2.38892603 fraction A 0.0549694113\n",
      "step 3946 loss 1.30020106 fisher_loss 0.229664773 triplet loss 1.07053626 l2_loss 22.0805683 fraction B 0.171991616 lossA 2.42834711 fraction A 0.0583570674\n",
      "step 3947 loss 0.91349113 fisher_loss 0.229939178 triplet loss 0.683551967 l2_loss 22.0873146 fraction B 0.134174451 lossA 2.53016162 fraction A 0.0662347749\n",
      "step 3948 loss 1.05957389 fisher_loss 0.230938137 triplet loss 0.828635752 l2_loss 22.0946102 fraction B 0.210921943 lossA 2.52744484 fraction A 0.0646065325\n",
      "step 3949 loss 0.955430806 fisher_loss 0.23113586 triplet loss 0.72429496 l2_loss 22.0997353 fraction B 0.125292 lossA 2.50359654 fraction A 0.0602484345\n",
      "step 3950 loss 0.922704577 fisher_loss 0.231128484 triplet loss 0.691576064 l2_loss 22.1035385 fraction B 0.169831812 lossA 2.42583942 fraction A 0.0505186208\n",
      "step 3951 loss 0.965386868 fisher_loss 0.230829015 triplet loss 0.734557867 l2_loss 22.1063671 fraction B 0.151238486 lossA 2.37886691 fraction A 0.044980716\n",
      "step 3952 loss 1.41514683 fisher_loss 0.230794549 triplet loss 1.18435228 l2_loss 22.1101456 fraction B 0.191659987 lossA 2.35816646 fraction A 0.0424903\n",
      "step 3953 loss 0.986080348 fisher_loss 0.231005892 triplet loss 0.755074441 l2_loss 22.1135025 fraction B 0.194129899 lossA 2.37279987 fraction A 0.0433472544\n",
      "step 3954 loss 1.2529676 fisher_loss 0.231224597 triplet loss 1.02174306 l2_loss 22.117609 fraction B 0.247138038 lossA 2.39911747 fraction A 0.0449285917\n",
      "step 3955 loss 1.14272761 fisher_loss 0.23144576 triplet loss 0.911281824 l2_loss 22.1216564 fraction B 0.224881217 lossA 2.45525336 fraction A 0.0485005826\n",
      "step 3956 loss 1.09824324 fisher_loss 0.231837407 triplet loss 0.866405845 l2_loss 22.1261806 fraction B 0.214685977 lossA 2.55359554 fraction A 0.0557525121\n",
      "step 3957 loss 1.45954502 fisher_loss 0.232225284 triplet loss 1.22731972 l2_loss 22.1311 fraction B 0.208320811 lossA 2.64608717 fraction A 0.063523367\n",
      "step 3958 loss 1.10730815 fisher_loss 0.232524112 triplet loss 0.874784 l2_loss 22.135643 fraction B 0.15189375 lossA 2.65331745 fraction A 0.0640913\n",
      "step 3959 loss 1.49766779 fisher_loss 0.23225458 triplet loss 1.26541317 l2_loss 22.1386909 fraction B 0.162050679 lossA 2.59674859 fraction A 0.0599401779\n",
      "step 3960 loss 1.20489097 fisher_loss 0.231588 triplet loss 0.97330296 l2_loss 22.1404076 fraction B 0.330529779 lossA 2.52622485 fraction A 0.0547524393\n",
      "step 3961 loss 1.20093203 fisher_loss 0.230874345 triplet loss 0.970057726 l2_loss 22.1419277 fraction B 0.158118039 lossA 2.44525862 fraction A 0.0496182702\n",
      "step 3962 loss 1.19071925 fisher_loss 0.230154738 triplet loss 0.960564554 l2_loss 22.1446323 fraction B 0.178458467 lossA 2.3539784 fraction A 0.0431507826\n",
      "step 3963 loss 1.41961491 fisher_loss 0.229337841 triplet loss 1.1902771 l2_loss 22.1462135 fraction B 0.21610187 lossA 2.2651763 fraction A 0.0374622643\n",
      "step 3964 loss 1.07405674 fisher_loss 0.228607193 triplet loss 0.845449507 l2_loss 22.1484241 fraction B 0.252675056 lossA 2.23063684 fraction A 0.0362094641\n",
      "step 3965 loss 1.24867952 fisher_loss 0.228343531 triplet loss 1.02033603 l2_loss 22.1520729 fraction B 0.265865445 lossA 2.27306604 fraction A 0.0407121256\n",
      "step 3966 loss 1.0134964 fisher_loss 0.228483409 triplet loss 0.785013 l2_loss 22.1561279 fraction B 0.208222076 lossA 2.35037947 fraction A 0.0496053025\n",
      "step 3967 loss 1.00684488 fisher_loss 0.22906132 triplet loss 0.777783513 l2_loss 22.1609554 fraction B 0.191506 lossA 2.48595285 fraction A 0.061388433\n",
      "step 3968 loss 0.968630254 fisher_loss 0.229853019 triplet loss 0.73877722 l2_loss 22.1653614 fraction B 0.18741861 lossA 2.53644896 fraction A 0.0654343665\n",
      "step 3969 loss 0.979820549 fisher_loss 0.230075791 triplet loss 0.749744773 l2_loss 22.169569 fraction B 0.166611284 lossA 2.50493479 fraction A 0.0619524233\n",
      "step 3970 loss 1.02193916 fisher_loss 0.229560465 triplet loss 0.792378664 l2_loss 22.1717243 fraction B 0.26854673 lossA 2.41714859 fraction A 0.0531648695\n",
      "step 3971 loss 0.928367496 fisher_loss 0.228544086 triplet loss 0.699823439 l2_loss 22.1730862 fraction B 0.132955536 lossA 2.37017179 fraction A 0.0480780825\n",
      "step 3972 loss 1.12309301 fisher_loss 0.227905795 triplet loss 0.895187259 l2_loss 22.1770687 fraction B 0.286676258 lossA 2.3348248 fraction A 0.0439139232\n",
      "step 3973 loss 1.113626 fisher_loss 0.227347434 triplet loss 0.886278629 l2_loss 22.1808014 fraction B 0.241096452 lossA 2.33138895 fraction A 0.0433232374\n",
      "step 3974 loss 1.09826481 fisher_loss 0.227225214 triplet loss 0.871039569 l2_loss 22.185461 fraction B 0.256041199 lossA 2.34694529 fraction A 0.0445095673\n",
      "step 3975 loss 1.1077044 fisher_loss 0.227296665 triplet loss 0.880407751 l2_loss 22.1911011 fraction B 0.256800056 lossA 2.34304214 fraction A 0.0438662544\n",
      "step 3976 loss 1.19156158 fisher_loss 0.227243051 triplet loss 0.964318514 l2_loss 22.1958256 fraction B 0.245309979 lossA 2.2959547 fraction A 0.0397622958\n",
      "step 3977 loss 1.0420903 fisher_loss 0.226914093 triplet loss 0.815176249 l2_loss 22.1998577 fraction B 0.154919088 lossA 2.29245973 fraction A 0.0393219\n",
      "step 3978 loss 1.09908354 fisher_loss 0.22696498 triplet loss 0.872118592 l2_loss 22.2053013 fraction B 0.218214586 lossA 2.37365842 fraction A 0.0460410267\n",
      "step 3979 loss 1.06937087 fisher_loss 0.227403596 triplet loss 0.841967225 l2_loss 22.211874 fraction B 0.235472411 lossA 2.44418764 fraction A 0.0520967357\n",
      "step 3980 loss 1.00686371 fisher_loss 0.227800161 triplet loss 0.779063582 l2_loss 22.2180099 fraction B 0.1919678 lossA 2.53040743 fraction A 0.0597146191\n",
      "step 3981 loss 0.993435 fisher_loss 0.228347242 triplet loss 0.765087783 l2_loss 22.2244511 fraction B 0.147949696 lossA 2.57439923 fraction A 0.063624531\n",
      "step 3982 loss 1.09432542 fisher_loss 0.228639483 triplet loss 0.86568594 l2_loss 22.2308064 fraction B 0.187319145 lossA 2.56367564 fraction A 0.0626789\n",
      "step 3983 loss 1.08300316 fisher_loss 0.228235409 triplet loss 0.85476774 l2_loss 22.2349644 fraction B 0.218705148 lossA 2.52677393 fraction A 0.0590364598\n",
      "step 3984 loss 1.25648952 fisher_loss 0.227599636 triplet loss 1.02888989 l2_loss 22.2381153 fraction B 0.2391195 lossA 2.48540807 fraction A 0.0549339503\n",
      "step 3985 loss 1.09672153 fisher_loss 0.226839393 triplet loss 0.869882166 l2_loss 22.2397861 fraction B 0.25304839 lossA 2.42560244 fraction A 0.0490775034\n",
      "step 3986 loss 1.41050673 fisher_loss 0.225989327 triplet loss 1.18451738 l2_loss 22.2404194 fraction B 0.147968352 lossA 2.30584121 fraction A 0.0388518\n",
      "step 3987 loss 1.08761525 fisher_loss 0.224981129 triplet loss 0.862634182 l2_loss 22.2404442 fraction B 0.209504142 lossA 2.2204175 fraction A 0.0339727141\n",
      "step 3988 loss 1.13138163 fisher_loss 0.224681407 triplet loss 0.906700253 l2_loss 22.2416115 fraction B 0.304344147 lossA 2.16085768 fraction A 0.0323920287\n",
      "step 3989 loss 1.10546303 fisher_loss 0.224874794 triplet loss 0.880588233 l2_loss 22.2433605 fraction B 0.176143795 lossA 2.23907185 fraction A 0.036020346\n",
      "step 3990 loss 0.953218281 fisher_loss 0.225626931 triplet loss 0.727591336 l2_loss 22.2454166 fraction B 0.193629146 lossA 2.36055255 fraction A 0.0426026955\n",
      "step 3991 loss 1.03156948 fisher_loss 0.22681661 triplet loss 0.804752827 l2_loss 22.248148 fraction B 0.14775759 lossA 2.48239088 fraction A 0.0501473024\n",
      "step 3992 loss 0.96654731 fisher_loss 0.227714956 triplet loss 0.738832355 l2_loss 22.2523384 fraction B 0.239934132 lossA 2.57338381 fraction A 0.0562144592\n",
      "step 3993 loss 1.08300948 fisher_loss 0.228347421 triplet loss 0.854662061 l2_loss 22.2567444 fraction B 0.189857766 lossA 2.61476684 fraction A 0.060157951\n",
      "step 3994 loss 1.04533684 fisher_loss 0.228388 triplet loss 0.816948831 l2_loss 22.261261 fraction B 0.12006025 lossA 2.59088135 fraction A 0.05983546\n",
      "step 3995 loss 1.17014539 fisher_loss 0.227689952 triplet loss 0.942455471 l2_loss 22.2651253 fraction B 0.191934317 lossA 2.53449488 fraction A 0.0563239604\n",
      "step 3996 loss 1.17669868 fisher_loss 0.22678864 triplet loss 0.94991 l2_loss 22.267662 fraction B 0.260029942 lossA 2.44302988 fraction A 0.0508577041\n",
      "step 3997 loss 1.05078375 fisher_loss 0.225679502 triplet loss 0.825104237 l2_loss 22.2690048 fraction B 0.135513052 lossA 2.35111904 fraction A 0.0433693863\n",
      "step 3998 loss 1.16686726 fisher_loss 0.224825367 triplet loss 0.942041874 l2_loss 22.2691746 fraction B 0.191376865 lossA 2.26027083 fraction A 0.0361093804\n",
      "step 3999 loss 1.03905582 fisher_loss 0.224356204 triplet loss 0.81469965 l2_loss 22.269846 fraction B 0.220196277 lossA 2.21258235 fraction A 0.0333362818\n",
      "step 4000 loss 1.00908613 fisher_loss 0.224243045 triplet loss 0.784843147 l2_loss 22.2719536 fraction B 0.310011059 lossA 2.2735014 fraction A 0.0374326706\n",
      "step 4001 loss 1.11272764 fisher_loss 0.224546596 triplet loss 0.888181031 l2_loss 22.2761154 fraction B 0.275855601 lossA 2.34727931 fraction A 0.0435763635\n",
      "step 4002 loss 0.981436312 fisher_loss 0.225046411 triplet loss 0.756389916 l2_loss 22.2810059 fraction B 0.145780534 lossA 2.4340539 fraction A 0.0511390083\n",
      "step 4003 loss 1.12223637 fisher_loss 0.225906342 triplet loss 0.896330059 l2_loss 22.2863941 fraction B 0.18006964 lossA 2.46018314 fraction A 0.0529639423\n",
      "step 4004 loss 1.07125425 fisher_loss 0.226534426 triplet loss 0.844719768 l2_loss 22.2900448 fraction B 0.292737544 lossA 2.46872211 fraction A 0.0532006212\n",
      "step 4005 loss 1.07183933 fisher_loss 0.227291048 triplet loss 0.844548225 l2_loss 22.293663 fraction B 0.239663452 lossA 2.44291043 fraction A 0.0510909781\n",
      "step 4006 loss 1.07563853 fisher_loss 0.227886751 triplet loss 0.847751796 l2_loss 22.2975159 fraction B 0.137766331 lossA 2.4028461 fraction A 0.047980316\n",
      "step 4007 loss 1.09161019 fisher_loss 0.228237033 triplet loss 0.86337322 l2_loss 22.3021088 fraction B 0.131063402 lossA 2.37649536 fraction A 0.0461271629\n",
      "step 4008 loss 0.913935 fisher_loss 0.228288457 triplet loss 0.685646534 l2_loss 22.307188 fraction B 0.16691424 lossA 2.37062526 fraction A 0.045429334\n",
      "step 4009 loss 1.114851 fisher_loss 0.22793293 triplet loss 0.886918 l2_loss 22.3135548 fraction B 0.171684697 lossA 2.42588592 fraction A 0.0492191687\n",
      "step 4010 loss 0.955793262 fisher_loss 0.227980688 triplet loss 0.727812588 l2_loss 22.3212585 fraction B 0.151313365 lossA 2.52571654 fraction A 0.057302516\n",
      "step 4011 loss 1.11697292 fisher_loss 0.228308082 triplet loss 0.888664782 l2_loss 22.3310776 fraction B 0.272685468 lossA 2.58052373 fraction A 0.0618918948\n",
      "step 4012 loss 1.17658782 fisher_loss 0.228579298 triplet loss 0.948008478 l2_loss 22.3397293 fraction B 0.168923199 lossA 2.5553236 fraction A 0.0603967644\n",
      "step 4013 loss 0.993250608 fisher_loss 0.228440553 triplet loss 0.764810085 l2_loss 22.3468361 fraction B 0.214887053 lossA 2.47869325 fraction A 0.0556685813\n",
      "step 4014 loss 1.17850232 fisher_loss 0.228264615 triplet loss 0.950237691 l2_loss 22.353138 fraction B 0.255546391 lossA 2.37867022 fraction A 0.0483544245\n",
      "step 4015 loss 0.962311387 fisher_loss 0.227989569 triplet loss 0.734321833 l2_loss 22.3586407 fraction B 0.169548199 lossA 2.32442284 fraction A 0.0442063473\n",
      "step 4016 loss 1.05811763 fisher_loss 0.228211731 triplet loss 0.829905868 l2_loss 22.365324 fraction B 0.152647138 lossA 2.30640602 fraction A 0.0439020768\n",
      "step 4017 loss 1.05709279 fisher_loss 0.228474841 triplet loss 0.828618 l2_loss 22.3720417 fraction B 0.308707237 lossA 2.29036975 fraction A 0.0433349\n",
      "step 4018 loss 1.15647089 fisher_loss 0.228553101 triplet loss 0.927917838 l2_loss 22.3782234 fraction B 0.240921542 lossA 2.31852889 fraction A 0.0468301401\n",
      "step 4019 loss 0.900962114 fisher_loss 0.228818029 triplet loss 0.672144115 l2_loss 22.3843689 fraction B 0.174136907 lossA 2.3644743 fraction A 0.0494389683\n",
      "step 4020 loss 1.14898729 fisher_loss 0.229323491 triplet loss 0.919663787 l2_loss 22.3898888 fraction B 0.220156431 lossA 2.41181445 fraction A 0.051003173\n",
      "step 4021 loss 1.26271391 fisher_loss 0.229945838 triplet loss 1.03276801 l2_loss 22.3952694 fraction B 0.202266902 lossA 2.43017459 fraction A 0.0505123176\n",
      "step 4022 loss 1.06209087 fisher_loss 0.230518863 triplet loss 0.831572056 l2_loss 22.3993454 fraction B 0.195501938 lossA 2.37512159 fraction A 0.0461190492\n",
      "step 4023 loss 1.32141221 fisher_loss 0.23073934 triplet loss 1.09067285 l2_loss 22.4037457 fraction B 0.223883018 lossA 2.33880043 fraction A 0.0435275361\n",
      "step 4024 loss 0.961879253 fisher_loss 0.230748221 triplet loss 0.731131 l2_loss 22.4092216 fraction B 0.24592562 lossA 2.33108354 fraction A 0.0436547138\n",
      "step 4025 loss 1.12128019 fisher_loss 0.230810523 triplet loss 0.89046973 l2_loss 22.4160919 fraction B 0.164332628 lossA 2.33518672 fraction A 0.0451467969\n",
      "step 4026 loss 1.06165385 fisher_loss 0.230709583 triplet loss 0.83094424 l2_loss 22.4229622 fraction B 0.109337449 lossA 2.38955426 fraction A 0.0510713086\n",
      "step 4027 loss 1.06818676 fisher_loss 0.230879679 triplet loss 0.837307036 l2_loss 22.4316463 fraction B 0.240703329 lossA 2.38894773 fraction A 0.0544829406\n",
      "step 4028 loss 1.19974029 fisher_loss 0.230665892 triplet loss 0.969074428 l2_loss 22.4386272 fraction B 0.162827879 lossA 2.36179256 fraction A 0.0551879071\n",
      "step 4029 loss 0.993237734 fisher_loss 0.230187029 triplet loss 0.763050675 l2_loss 22.4449501 fraction B 0.176538736 lossA 2.29140401 fraction A 0.050830137\n",
      "step 4030 loss 0.909862399 fisher_loss 0.229368463 triplet loss 0.680493951 l2_loss 22.4502163 fraction B 0.169915438 lossA 2.22247124 fraction A 0.0455136597\n",
      "step 4031 loss 1.12252891 fisher_loss 0.228792593 triplet loss 0.893736362 l2_loss 22.4556503 fraction B 0.211471409 lossA 2.2017982 fraction A 0.0431696922\n",
      "step 4032 loss 0.858785808 fisher_loss 0.228470907 triplet loss 0.630314887 l2_loss 22.4607658 fraction B 0.121654302 lossA 2.16108656 fraction A 0.0387064032\n",
      "step 4033 loss 1.20768929 fisher_loss 0.228323311 triplet loss 0.979365945 l2_loss 22.466156 fraction B 0.203691393 lossA 2.19067693 fraction A 0.040529456\n",
      "step 4034 loss 1.10727906 fisher_loss 0.228777438 triplet loss 0.878501594 l2_loss 22.4721432 fraction B 0.252680838 lossA 2.23745179 fraction A 0.0438116305\n",
      "step 4035 loss 1.36650646 fisher_loss 0.229397878 triplet loss 1.13710856 l2_loss 22.4777508 fraction B 0.279756725 lossA 2.28297114 fraction A 0.047337655\n",
      "step 4036 loss 1.04078043 fisher_loss 0.230036721 triplet loss 0.810743749 l2_loss 22.4830608 fraction B 0.194561556 lossA 2.31933832 fraction A 0.0497122332\n",
      "step 4037 loss 0.828013122 fisher_loss 0.230526626 triplet loss 0.597486496 l2_loss 22.4879799 fraction B 0.11794652 lossA 2.40060306 fraction A 0.0560066849\n",
      "step 4038 loss 1.14477015 fisher_loss 0.231519908 triplet loss 0.913250208 l2_loss 22.4950924 fraction B 0.224235758 lossA 2.42902327 fraction A 0.0576306209\n",
      "step 4039 loss 1.07114184 fisher_loss 0.231989235 triplet loss 0.839152634 l2_loss 22.501236 fraction B 0.15899168 lossA 2.3927536 fraction A 0.052709803\n",
      "step 4040 loss 1.04855943 fisher_loss 0.231816277 triplet loss 0.816743135 l2_loss 22.5063496 fraction B 0.195048541 lossA 2.37812948 fraction A 0.0508185104\n",
      "step 4041 loss 1.09715867 fisher_loss 0.231619269 triplet loss 0.865539432 l2_loss 22.5125141 fraction B 0.216660321 lossA 2.37072897 fraction A 0.0500075184\n",
      "step 4042 loss 1.13718832 fisher_loss 0.231534049 triplet loss 0.905654252 l2_loss 22.5187874 fraction B 0.2351145 lossA 2.37538624 fraction A 0.0507797152\n",
      "step 4043 loss 1.48162961 fisher_loss 0.231435955 triplet loss 1.2501936 l2_loss 22.5246868 fraction B 0.141731277 lossA 2.3506968 fraction A 0.0489998758\n",
      "step 4044 loss 1.19377398 fisher_loss 0.231134713 triplet loss 0.962639332 l2_loss 22.5294685 fraction B 0.215590864 lossA 2.32238722 fraction A 0.0464166552\n",
      "step 4045 loss 0.955525756 fisher_loss 0.230843961 triplet loss 0.724681795 l2_loss 22.5330505 fraction B 0.193029717 lossA 2.32496119 fraction A 0.04722609\n",
      "step 4046 loss 0.998080611 fisher_loss 0.230897635 triplet loss 0.767182946 l2_loss 22.5385151 fraction B 0.222908959 lossA 2.32091618 fraction A 0.0474953689\n",
      "step 4047 loss 1.1498313 fisher_loss 0.230858684 triplet loss 0.918972552 l2_loss 22.5438614 fraction B 0.260024309 lossA 2.29034281 fraction A 0.0440856181\n",
      "step 4048 loss 0.917127311 fisher_loss 0.230243608 triplet loss 0.686883688 l2_loss 22.5465717 fraction B 0.147445068 lossA 2.25384903 fraction A 0.0407756232\n",
      "step 4049 loss 0.886462927 fisher_loss 0.229621768 triplet loss 0.656841159 l2_loss 22.5494137 fraction B 0.160819188 lossA 2.28630137 fraction A 0.0437798612\n",
      "step 4050 loss 1.01154065 fisher_loss 0.22968173 triplet loss 0.781858921 l2_loss 22.554512 fraction B 0.197487548 lossA 2.32009077 fraction A 0.0462164171\n",
      "step 4051 loss 0.945861459 fisher_loss 0.229680285 triplet loss 0.716181159 l2_loss 22.5597115 fraction B 0.158224598 lossA 2.38813591 fraction A 0.0510076657\n",
      "step 4052 loss 1.20285773 fisher_loss 0.229990155 triplet loss 0.972867608 l2_loss 22.5661049 fraction B 0.230193913 lossA 2.42520165 fraction A 0.0533174388\n",
      "step 4053 loss 1.11058795 fisher_loss 0.230047181 triplet loss 0.880540729 l2_loss 22.5722313 fraction B 0.147892833 lossA 2.44560838 fraction A 0.0541856624\n",
      "step 4054 loss 1.08140624 fisher_loss 0.230178744 triplet loss 0.851227522 l2_loss 22.5780334 fraction B 0.156153068 lossA 2.44244647 fraction A 0.0529420301\n",
      "step 4055 loss 1.13359308 fisher_loss 0.229784951 triplet loss 0.903808117 l2_loss 22.5827904 fraction B 0.231945813 lossA 2.41874433 fraction A 0.0499228314\n",
      "step 4056 loss 1.00983596 fisher_loss 0.229446232 triplet loss 0.780389786 l2_loss 22.5870743 fraction B 0.184501231 lossA 2.36340189 fraction A 0.0451427028\n",
      "step 4057 loss 1.28127682 fisher_loss 0.228874579 triplet loss 1.05240226 l2_loss 22.5918217 fraction B 0.21443741 lossA 2.30272985 fraction A 0.0406071171\n",
      "step 4058 loss 1.25902057 fisher_loss 0.228281572 triplet loss 1.03073895 l2_loss 22.5964165 fraction B 0.240422502 lossA 2.26079941 fraction A 0.0373831168\n",
      "step 4059 loss 1.06875944 fisher_loss 0.227882296 triplet loss 0.840877116 l2_loss 22.6010799 fraction B 0.262925267 lossA 2.24729419 fraction A 0.0360081755\n",
      "step 4060 loss 1.15139616 fisher_loss 0.227953315 triplet loss 0.923442841 l2_loss 22.6061192 fraction B 0.211472601 lossA 2.30348468 fraction A 0.0382258371\n",
      "step 4061 loss 0.820302367 fisher_loss 0.228339821 triplet loss 0.591962576 l2_loss 22.6120071 fraction B 0.0943083093 lossA 2.40266299 fraction A 0.0447118729\n",
      "step 4062 loss 1.31167984 fisher_loss 0.229224741 triplet loss 1.08245516 l2_loss 22.6196232 fraction B 0.178925887 lossA 2.45653963 fraction A 0.0491179265\n",
      "step 4063 loss 1.29393792 fisher_loss 0.229801118 triplet loss 1.06413674 l2_loss 22.6255817 fraction B 0.178150609 lossA 2.57328773 fraction A 0.0587409548\n",
      "step 4064 loss 0.997624338 fisher_loss 0.230506435 triplet loss 0.767117918 l2_loss 22.6309166 fraction B 0.166269 lossA 2.62589717 fraction A 0.0631928667\n",
      "step 4065 loss 1.058411 fisher_loss 0.230753228 triplet loss 0.827657819 l2_loss 22.6359596 fraction B 0.175481468 lossA 2.61765862 fraction A 0.0634654462\n",
      "step 4066 loss 1.05095136 fisher_loss 0.230264455 triplet loss 0.820686877 l2_loss 22.6398563 fraction B 0.137698665 lossA 2.55429888 fraction A 0.0593592748\n",
      "step 4067 loss 1.18736 fisher_loss 0.229105532 triplet loss 0.958254576 l2_loss 22.6423168 fraction B 0.289966345 lossA 2.47024417 fraction A 0.0534225218\n",
      "step 4068 loss 0.851319551 fisher_loss 0.227771163 triplet loss 0.623548388 l2_loss 22.6434956 fraction B 0.127585068 lossA 2.39176702 fraction A 0.0472324267\n",
      "step 4069 loss 1.01795256 fisher_loss 0.22681506 triplet loss 0.791137457 l2_loss 22.6461487 fraction B 0.208874553 lossA 2.29611921 fraction A 0.0393634848\n",
      "step 4070 loss 1.09231269 fisher_loss 0.226033643 triplet loss 0.866279 l2_loss 22.6473942 fraction B 0.317865908 lossA 2.16515803 fraction A 0.0323487446\n",
      "step 4071 loss 1.29310989 fisher_loss 0.225504562 triplet loss 1.06760538 l2_loss 22.6479778 fraction B 0.165019393 lossA 2.09579635 fraction A 0.0301529597\n",
      "step 4072 loss 1.06731343 fisher_loss 0.225510508 triplet loss 0.841802955 l2_loss 22.6503525 fraction B 0.237422034 lossA 2.18114543 fraction A 0.0347227789\n",
      "step 4073 loss 1.39070916 fisher_loss 0.226148218 triplet loss 1.16456091 l2_loss 22.6553154 fraction B 0.228164777 lossA 2.28521514 fraction A 0.0416041836\n",
      "step 4074 loss 1.21190608 fisher_loss 0.226989731 triplet loss 0.984916389 l2_loss 22.6601601 fraction B 0.177706078 lossA 2.33366036 fraction A 0.0458997935\n",
      "step 4075 loss 1.18154097 fisher_loss 0.22750777 triplet loss 0.954033256 l2_loss 22.6640301 fraction B 0.230011016 lossA 2.36706018 fraction A 0.0496287048\n",
      "step 4076 loss 1.21051276 fisher_loss 0.227770343 triplet loss 0.982742429 l2_loss 22.6673298 fraction B 0.189252332 lossA 2.38219333 fraction A 0.0524756238\n",
      "step 4077 loss 1.04077291 fisher_loss 0.227697492 triplet loss 0.813075483 l2_loss 22.6698914 fraction B 0.196801618 lossA 2.34773612 fraction A 0.049917832\n",
      "step 4078 loss 1.04630399 fisher_loss 0.227159157 triplet loss 0.819144785 l2_loss 22.671072 fraction B 0.155872807 lossA 2.31439805 fraction A 0.0480767041\n",
      "step 4079 loss 0.999647 fisher_loss 0.226618722 triplet loss 0.773028314 l2_loss 22.6740532 fraction B 0.0963576809 lossA 2.26258874 fraction A 0.0447476245\n",
      "step 4080 loss 1.13855243 fisher_loss 0.225916579 triplet loss 0.912635863 l2_loss 22.6774578 fraction B 0.238277078 lossA 2.21910334 fraction A 0.0415583253\n",
      "step 4081 loss 1.00346339 fisher_loss 0.225322515 triplet loss 0.778140843 l2_loss 22.6805897 fraction B 0.17657657 lossA 2.21810961 fraction A 0.0411539711\n",
      "step 4082 loss 1.04598391 fisher_loss 0.225124151 triplet loss 0.82085979 l2_loss 22.6841106 fraction B 0.2285285 lossA 2.27165818 fraction A 0.0455331132\n",
      "step 4083 loss 0.975226045 fisher_loss 0.225355119 triplet loss 0.749870956 l2_loss 22.6886749 fraction B 0.102344692 lossA 2.30513096 fraction A 0.0484432429\n",
      "step 4084 loss 0.922796607 fisher_loss 0.225539833 triplet loss 0.697256744 l2_loss 22.6938553 fraction B 0.128066912 lossA 2.31751 fraction A 0.0490050949\n",
      "step 4085 loss 1.03518152 fisher_loss 0.225679532 triplet loss 0.809502 l2_loss 22.6993656 fraction B 0.169201896 lossA 2.31003118 fraction A 0.0482386574\n",
      "step 4086 loss 1.0287565 fisher_loss 0.225743622 triplet loss 0.803012848 l2_loss 22.7052212 fraction B 0.118132524 lossA 2.30589747 fraction A 0.0477167256\n",
      "step 4087 loss 1.03436553 fisher_loss 0.225967273 triplet loss 0.808398306 l2_loss 22.711113 fraction B 0.233103275 lossA 2.28105545 fraction A 0.0443880036\n",
      "step 4088 loss 1.12993371 fisher_loss 0.226026028 triplet loss 0.903907657 l2_loss 22.7158318 fraction B 0.218317851 lossA 2.25457239 fraction A 0.0409251526\n",
      "step 4089 loss 1.08042276 fisher_loss 0.226063 triplet loss 0.854359746 l2_loss 22.7195435 fraction B 0.145743787 lossA 2.25250578 fraction A 0.0392818\n",
      "step 4090 loss 1.10883844 fisher_loss 0.226026133 triplet loss 0.882812262 l2_loss 22.7232647 fraction B 0.171011329 lossA 2.24915624 fraction A 0.037254598\n",
      "step 4091 loss 0.95050621 fisher_loss 0.225859538 triplet loss 0.724646688 l2_loss 22.7263622 fraction B 0.199024782 lossA 2.31502676 fraction A 0.0412311256\n",
      "step 4092 loss 1.25625134 fisher_loss 0.22630614 triplet loss 1.02994514 l2_loss 22.7309532 fraction B 0.23278524 lossA 2.38039947 fraction A 0.0461217314\n",
      "step 4093 loss 1.35651219 fisher_loss 0.226821035 triplet loss 1.12969112 l2_loss 22.7364063 fraction B 0.20893684 lossA 2.44589543 fraction A 0.0501692183\n",
      "step 4094 loss 1.00925589 fisher_loss 0.227184862 triplet loss 0.782071 l2_loss 22.7407761 fraction B 0.227616623 lossA 2.48593092 fraction A 0.0527300909\n",
      "step 4095 loss 1.46896577 fisher_loss 0.227460444 triplet loss 1.24150527 l2_loss 22.7457676 fraction B 0.303374171 lossA 2.52825689 fraction A 0.0563242137\n",
      "step 4096 loss 1.14435959 fisher_loss 0.227735579 triplet loss 0.91662395 l2_loss 22.7495499 fraction B 0.157754466 lossA 2.56676626 fraction A 0.0605760664\n",
      "step 4097 loss 0.976678133 fisher_loss 0.227980196 triplet loss 0.748697937 l2_loss 22.7543812 fraction B 0.146392792 lossA 2.57353091 fraction A 0.0631591827\n",
      "step 4098 loss 0.901549816 fisher_loss 0.227800399 triplet loss 0.673749447 l2_loss 22.7597656 fraction B 0.15010795 lossA 2.48992062 fraction A 0.0588008314\n",
      "step 4099 loss 0.978575706 fisher_loss 0.226900339 triplet loss 0.751675367 l2_loss 22.7649212 fraction B 0.165945873 lossA 2.38242435 fraction A 0.0524759889\n",
      "step 4100 loss 1.44464898 fisher_loss 0.226168022 triplet loss 1.21848094 l2_loss 22.7698345 fraction B 0.183403939 lossA 2.29229116 fraction A 0.0447328798\n",
      "step 4101 loss 1.14085865 fisher_loss 0.225727648 triplet loss 0.915131032 l2_loss 22.7732162 fraction B 0.246689975 lossA 2.22195864 fraction A 0.0377178155\n",
      "step 4102 loss 1.02782547 fisher_loss 0.225405827 triplet loss 0.802419662 l2_loss 22.7755756 fraction B 0.3006 lossA 2.2490983 fraction A 0.0395319536\n",
      "step 4103 loss 0.806934476 fisher_loss 0.225645199 triplet loss 0.581289291 l2_loss 22.7792225 fraction B 0.140556499 lossA 2.37327147 fraction A 0.0510219373\n",
      "step 4104 loss 1.06737411 fisher_loss 0.226572618 triplet loss 0.840801477 l2_loss 22.7854576 fraction B 0.245275289 lossA 2.55305028 fraction A 0.0650743097\n",
      "step 4105 loss 0.99999094 fisher_loss 0.227906317 triplet loss 0.772084594 l2_loss 22.7923222 fraction B 0.211347058 lossA 2.64240599 fraction A 0.0710363835\n",
      "step 4106 loss 1.18008137 fisher_loss 0.228759751 triplet loss 0.951321661 l2_loss 22.7978096 fraction B 0.261281252 lossA 2.67635059 fraction A 0.0725683123\n",
      "step 4107 loss 1.10993075 fisher_loss 0.229031488 triplet loss 0.88089931 l2_loss 22.8016243 fraction B 0.263691127 lossA 2.60559559 fraction A 0.065764606\n",
      "step 4108 loss 1.14332 fisher_loss 0.228465274 triplet loss 0.914854646 l2_loss 22.8036747 fraction B 0.24695608 lossA 2.47525811 fraction A 0.0529550686\n",
      "step 4109 loss 1.22494483 fisher_loss 0.22756578 triplet loss 0.997379 l2_loss 22.8044071 fraction B 0.174627408 lossA 2.42177224 fraction A 0.0475788265\n",
      "step 4110 loss 1.21756315 fisher_loss 0.227308765 triplet loss 0.990254402 l2_loss 22.8076668 fraction B 0.216398522 lossA 2.38934255 fraction A 0.0439671315\n",
      "step 4111 loss 0.979561448 fisher_loss 0.227073163 triplet loss 0.752488315 l2_loss 22.8109226 fraction B 0.147427097 lossA 2.42559314 fraction A 0.0465535782\n",
      "step 4112 loss 1.26146626 fisher_loss 0.227205396 triplet loss 1.03426087 l2_loss 22.8164425 fraction B 0.13481918 lossA 2.44209075 fraction A 0.0476756468\n",
      "step 4113 loss 1.10334945 fisher_loss 0.227288857 triplet loss 0.876060545 l2_loss 22.8211212 fraction B 0.253953308 lossA 2.47429323 fraction A 0.0504063666\n",
      "step 4114 loss 0.874098599 fisher_loss 0.227536306 triplet loss 0.646562278 l2_loss 22.8263741 fraction B 0.0870044082 lossA 2.52256155 fraction A 0.0541353859\n",
      "step 4115 loss 1.00546622 fisher_loss 0.22804828 triplet loss 0.777417958 l2_loss 22.8330822 fraction B 0.153069168 lossA 2.56263423 fraction A 0.0583130196\n",
      "step 4116 loss 1.01246536 fisher_loss 0.228699178 triplet loss 0.78376621 l2_loss 22.8408012 fraction B 0.157941073 lossA 2.53300667 fraction A 0.0562834628\n",
      "step 4117 loss 1.24473321 fisher_loss 0.228617862 triplet loss 1.01611531 l2_loss 22.8465881 fraction B 0.301170558 lossA 2.45929265 fraction A 0.0511121303\n",
      "step 4118 loss 1.18213689 fisher_loss 0.228046462 triplet loss 0.954090416 l2_loss 22.851263 fraction B 0.313467622 lossA 2.37276149 fraction A 0.0437656268\n",
      "step 4119 loss 1.28489184 fisher_loss 0.227235228 triplet loss 1.05765665 l2_loss 22.8540974 fraction B 0.183012038 lossA 2.31829143 fraction A 0.0391975455\n",
      "step 4120 loss 1.06199384 fisher_loss 0.226744622 triplet loss 0.835249186 l2_loss 22.8564587 fraction B 0.226357087 lossA 2.33275676 fraction A 0.0409588031\n",
      "step 4121 loss 0.902089298 fisher_loss 0.226710036 triplet loss 0.675379276 l2_loss 22.8601093 fraction B 0.178156152 lossA 2.42751145 fraction A 0.0487637036\n",
      "step 4122 loss 1.04881787 fisher_loss 0.227752626 triplet loss 0.821065187 l2_loss 22.8661213 fraction B 0.209233835 lossA 2.52125049 fraction A 0.0567829385\n",
      "step 4123 loss 1.38529921 fisher_loss 0.228817612 triplet loss 1.15648162 l2_loss 22.872261 fraction B 0.201427653 lossA 2.58708429 fraction A 0.0631415\n",
      "step 4124 loss 1.0542177 fisher_loss 0.229460597 triplet loss 0.824757099 l2_loss 22.8776207 fraction B 0.250439048 lossA 2.5839045 fraction A 0.0634729043\n",
      "step 4125 loss 1.09442019 fisher_loss 0.229330212 triplet loss 0.86509 l2_loss 22.8816357 fraction B 0.185865536 lossA 2.55697417 fraction A 0.0616087057\n",
      "step 4126 loss 0.99808836 fisher_loss 0.228902966 triplet loss 0.769185364 l2_loss 22.8850479 fraction B 0.0952674523 lossA 2.48983359 fraction A 0.0559134856\n",
      "step 4127 loss 1.25613 fisher_loss 0.227886781 triplet loss 1.02824318 l2_loss 22.8881512 fraction B 0.200366244 lossA 2.45834661 fraction A 0.0535408966\n",
      "step 4128 loss 0.963914692 fisher_loss 0.227255836 triplet loss 0.736658871 l2_loss 22.8911819 fraction B 0.201480567 lossA 2.42226338 fraction A 0.0513441749\n",
      "step 4129 loss 0.955999911 fisher_loss 0.226684794 triplet loss 0.729315102 l2_loss 22.8949108 fraction B 0.216652647 lossA 2.38108468 fraction A 0.047812134\n",
      "step 4130 loss 0.874287367 fisher_loss 0.226311132 triplet loss 0.64797622 l2_loss 22.89921 fraction B 0.165403306 lossA 2.40331626 fraction A 0.0504408143\n",
      "step 4131 loss 1.00229919 fisher_loss 0.226609722 triplet loss 0.775689483 l2_loss 22.9062347 fraction B 0.112850353 lossA 2.42482066 fraction A 0.0539216697\n",
      "step 4132 loss 1.02711523 fisher_loss 0.226973295 triplet loss 0.800141931 l2_loss 22.9143963 fraction B 0.179434717 lossA 2.4202106 fraction A 0.0548281781\n",
      "step 4133 loss 1.10150242 fisher_loss 0.227238104 triplet loss 0.874264359 l2_loss 22.9218349 fraction B 0.239464983 lossA 2.38794041 fraction A 0.0524569713\n",
      "step 4134 loss 1.12377632 fisher_loss 0.227291629 triplet loss 0.896484673 l2_loss 22.9277039 fraction B 0.257272661 lossA 2.37376022 fraction A 0.0514976867\n",
      "step 4135 loss 0.843951583 fisher_loss 0.227334082 triplet loss 0.616617501 l2_loss 22.9333553 fraction B 0.140933305 lossA 2.36529398 fraction A 0.0498136915\n",
      "step 4136 loss 1.07705164 fisher_loss 0.227695063 triplet loss 0.849356532 l2_loss 22.9398136 fraction B 0.156767443 lossA 2.33522224 fraction A 0.0460755117\n",
      "step 4137 loss 0.933068275 fisher_loss 0.228065789 triplet loss 0.705002487 l2_loss 22.9449749 fraction B 0.191776618 lossA 2.33076143 fraction A 0.0453485958\n",
      "step 4138 loss 1.04959846 fisher_loss 0.228707045 triplet loss 0.82089138 l2_loss 22.9514561 fraction B 0.180880874 lossA 2.32090116 fraction A 0.0437617861\n",
      "step 4139 loss 0.937525392 fisher_loss 0.229091704 triplet loss 0.708433688 l2_loss 22.9570179 fraction B 0.12651886 lossA 2.33422041 fraction A 0.0440370068\n",
      "step 4140 loss 1.03062272 fisher_loss 0.229437202 triplet loss 0.801185489 l2_loss 22.9635143 fraction B 0.180873796 lossA 2.36242652 fraction A 0.0467442907\n",
      "step 4141 loss 1.02447522 fisher_loss 0.229345217 triplet loss 0.795129955 l2_loss 22.9716568 fraction B 0.132330984 lossA 2.41133571 fraction A 0.0528212972\n",
      "step 4142 loss 1.20782709 fisher_loss 0.229463503 triplet loss 0.978363574 l2_loss 22.9807911 fraction B 0.209343731 lossA 2.45105505 fraction A 0.0577467494\n",
      "step 4143 loss 0.777592361 fisher_loss 0.229359567 triplet loss 0.548232794 l2_loss 22.9889679 fraction B 0.126424596 lossA 2.45607662 fraction A 0.0587723218\n",
      "step 4144 loss 1.07386434 fisher_loss 0.22948882 triplet loss 0.844375551 l2_loss 22.9971523 fraction B 0.204887137 lossA 2.43451047 fraction A 0.0578114428\n",
      "step 4145 loss 1.10623825 fisher_loss 0.229152694 triplet loss 0.877085507 l2_loss 23.0039139 fraction B 0.122042142 lossA 2.35491824 fraction A 0.05161874\n",
      "step 4146 loss 1.35935163 fisher_loss 0.228412211 triplet loss 1.13093948 l2_loss 23.0095787 fraction B 0.190689012 lossA 2.3325007 fraction A 0.048967205\n",
      "step 4147 loss 1.10985541 fisher_loss 0.228027686 triplet loss 0.881827712 l2_loss 23.0149097 fraction B 0.154068843 lossA 2.32236099 fraction A 0.0479535498\n",
      "step 4148 loss 1.26498461 fisher_loss 0.227847755 triplet loss 1.03713679 l2_loss 23.0195045 fraction B 0.171866089 lossA 2.30167842 fraction A 0.0453921705\n",
      "step 4149 loss 0.946987152 fisher_loss 0.227636427 triplet loss 0.719350696 l2_loss 23.022974 fraction B 0.188761473 lossA 2.31317139 fraction A 0.0457495786\n",
      "step 4150 loss 0.880231857 fisher_loss 0.227781311 triplet loss 0.652450562 l2_loss 23.0263634 fraction B 0.139913857 lossA 2.37577724 fraction A 0.0500283837\n",
      "step 4151 loss 1.1433202 fisher_loss 0.228849977 triplet loss 0.914470196 l2_loss 23.030983 fraction B 0.24519679 lossA 2.44094133 fraction A 0.0534541085\n",
      "step 4152 loss 1.08110142 fisher_loss 0.230058029 triplet loss 0.851043403 l2_loss 23.0351448 fraction B 0.166476503 lossA 2.51019812 fraction A 0.057335332\n",
      "step 4153 loss 1.06302118 fisher_loss 0.231108889 triplet loss 0.831912339 l2_loss 23.0396595 fraction B 0.130336463 lossA 2.54867196 fraction A 0.059486635\n",
      "step 4154 loss 1.02550447 fisher_loss 0.23188208 triplet loss 0.793622375 l2_loss 23.0453205 fraction B 0.139479682 lossA 2.69494772 fraction A 0.0723015666\n",
      "step 4155 loss 0.99432385 fisher_loss 0.232957631 triplet loss 0.761366248 l2_loss 23.0541344 fraction B 0.171400204 lossA 2.72326112 fraction A 0.0751680881\n",
      "step 4156 loss 1.32655621 fisher_loss 0.232950777 triplet loss 1.0936054 l2_loss 23.0618954 fraction B 0.24125874 lossA 2.64685607 fraction A 0.0703965798\n",
      "step 4157 loss 1.16804922 fisher_loss 0.231931359 triplet loss 0.936117828 l2_loss 23.0670757 fraction B 0.303260416 lossA 2.50898457 fraction A 0.0606293865\n",
      "step 4158 loss 1.12516654 fisher_loss 0.230479151 triplet loss 0.894687355 l2_loss 23.0711613 fraction B 0.350122422 lossA 2.3244884 fraction A 0.0457975\n",
      "step 4159 loss 1.08520424 fisher_loss 0.228882506 triplet loss 0.856321692 l2_loss 23.073822 fraction B 0.199018702 lossA 2.17925191 fraction A 0.0352371037\n",
      "step 4160 loss 1.08830214 fisher_loss 0.228038818 triplet loss 0.860263348 l2_loss 23.0769501 fraction B 0.216160223 lossA 2.07304311 fraction A 0.0307234302\n",
      "step 4161 loss 1.04204714 fisher_loss 0.227878764 triplet loss 0.814168394 l2_loss 23.0812778 fraction B 0.306751072 lossA 2.1074369 fraction A 0.0318154357\n",
      "step 4162 loss 0.942163944 fisher_loss 0.22799775 triplet loss 0.714166164 l2_loss 23.0868855 fraction B 0.253394395 lossA 2.27412772 fraction A 0.0405098572\n",
      "step 4163 loss 1.14681709 fisher_loss 0.22855331 triplet loss 0.918263793 l2_loss 23.0956039 fraction B 0.241100386 lossA 2.41965818 fraction A 0.0527584888\n",
      "step 4164 loss 1.28456676 fisher_loss 0.229579911 triplet loss 1.05498683 l2_loss 23.1046066 fraction B 0.267876297 lossA 2.55535245 fraction A 0.0636279359\n",
      "step 4165 loss 1.09042525 fisher_loss 0.23059392 triplet loss 0.859831393 l2_loss 23.1130505 fraction B 0.208423331 lossA 2.66866636 fraction A 0.0714621842\n",
      "step 4166 loss 1.16828728 fisher_loss 0.231325835 triplet loss 0.936961412 l2_loss 23.1202412 fraction B 0.231121525 lossA 2.71320105 fraction A 0.0735047087\n",
      "step 4167 loss 1.09077215 fisher_loss 0.231447637 triplet loss 0.859324455 l2_loss 23.1246376 fraction B 0.251329452 lossA 2.60007215 fraction A 0.0644301251\n",
      "step 4168 loss 1.15398908 fisher_loss 0.230385587 triplet loss 0.923603475 l2_loss 23.1265221 fraction B 0.22629787 lossA 2.49554801 fraction A 0.0550855063\n",
      "step 4169 loss 0.982438684 fisher_loss 0.22954917 triplet loss 0.752889514 l2_loss 23.1290321 fraction B 0.204871878 lossA 2.39299941 fraction A 0.0457825437\n",
      "step 4170 loss 1.00645804 fisher_loss 0.228768855 triplet loss 0.777689159 l2_loss 23.1313496 fraction B 0.26405564 lossA 2.27326274 fraction A 0.0373539589\n",
      "step 4171 loss 1.08519232 fisher_loss 0.228574723 triplet loss 0.85661763 l2_loss 23.1335354 fraction B 0.257832 lossA 2.1500721 fraction A 0.032083448\n",
      "step 4172 loss 0.90429914 fisher_loss 0.228418961 triplet loss 0.675880194 l2_loss 23.137682 fraction B 0.162876397 lossA 2.19579625 fraction A 0.0338904522\n",
      "step 4173 loss 0.817361772 fisher_loss 0.228365853 triplet loss 0.588995934 l2_loss 23.1444817 fraction B 0.163265303 lossA 2.3796773 fraction A 0.0450130627\n",
      "step 4174 loss 1.3034209 fisher_loss 0.22906898 triplet loss 1.07435191 l2_loss 23.1562958 fraction B 0.205725029 lossA 2.55551243 fraction A 0.0600255206\n",
      "step 4175 loss 1.24038839 fisher_loss 0.230403379 triplet loss 1.00998497 l2_loss 23.167963 fraction B 0.315047234 lossA 2.72852945 fraction A 0.0732080787\n",
      "step 4176 loss 0.927530766 fisher_loss 0.231895655 triplet loss 0.69563514 l2_loss 23.1782265 fraction B 0.185231328 lossA 2.74131846 fraction A 0.0744839162\n",
      "step 4177 loss 1.24290109 fisher_loss 0.232536703 triplet loss 1.01036441 l2_loss 23.1868763 fraction B 0.202869952 lossA 2.63442421 fraction A 0.0670123324\n",
      "step 4178 loss 1.12443566 fisher_loss 0.232087299 triplet loss 0.892348409 l2_loss 23.1929855 fraction B 0.153638259 lossA 2.43699574 fraction A 0.0498303548\n",
      "step 4179 loss 1.04621816 fisher_loss 0.231028527 triplet loss 0.8151896 l2_loss 23.1967545 fraction B 0.249491483 lossA 2.24269867 fraction A 0.0359316021\n",
      "step 4180 loss 1.08698595 fisher_loss 0.23031342 triplet loss 0.856672525 l2_loss 23.1998863 fraction B 0.197648421 lossA 2.09061217 fraction A 0.0296908673\n",
      "step 4181 loss 1.30038023 fisher_loss 0.23016037 triplet loss 1.07021987 l2_loss 23.2047234 fraction B 0.2455028 lossA 2.04738712 fraction A 0.0281603858\n",
      "step 4182 loss 1.44023407 fisher_loss 0.230051726 triplet loss 1.21018231 l2_loss 23.2106018 fraction B 0.216513187 lossA 2.08901644 fraction A 0.0296166837\n",
      "step 4183 loss 1.19517517 fisher_loss 0.229943454 triplet loss 0.965231657 l2_loss 23.2159863 fraction B 0.30572632 lossA 2.24106431 fraction A 0.0358850174\n",
      "step 4184 loss 1.39968491 fisher_loss 0.229787558 triplet loss 1.16989732 l2_loss 23.223032 fraction B 0.245983303 lossA 2.39699411 fraction A 0.047592625\n",
      "step 4185 loss 0.910501897 fisher_loss 0.229942963 triplet loss 0.68055892 l2_loss 23.2295399 fraction B 0.100639693 lossA 2.52064061 fraction A 0.0587467141\n",
      "step 4186 loss 1.07705688 fisher_loss 0.230862543 triplet loss 0.846194386 l2_loss 23.2372112 fraction B 0.177407756 lossA 2.63804865 fraction A 0.0683152229\n",
      "step 4187 loss 1.15032899 fisher_loss 0.231833532 triplet loss 0.918495417 l2_loss 23.2443027 fraction B 0.166638583 lossA 2.69935942 fraction A 0.0732109\n",
      "step 4188 loss 0.827802479 fisher_loss 0.232192978 triplet loss 0.595609486 l2_loss 23.2500229 fraction B 0.098476626 lossA 2.59351611 fraction A 0.0658593252\n",
      "step 4189 loss 1.09160829 fisher_loss 0.231483638 triplet loss 0.860124707 l2_loss 23.255249 fraction B 0.167245194 lossA 2.42902231 fraction A 0.0521012619\n",
      "step 4190 loss 1.32204664 fisher_loss 0.230497047 triplet loss 1.09154963 l2_loss 23.2591534 fraction B 0.134719968 lossA 2.30132866 fraction A 0.0399859697\n",
      "step 4191 loss 1.0992732 fisher_loss 0.229388162 triplet loss 0.869885 l2_loss 23.2622833 fraction B 0.187683776 lossA 2.18095207 fraction A 0.0323789902\n",
      "step 4192 loss 0.970588207 fisher_loss 0.228522032 triplet loss 0.742066145 l2_loss 23.265358 fraction B 0.130039603 lossA 2.1493125 fraction A 0.0310101341\n",
      "step 4193 loss 0.883125544 fisher_loss 0.228125125 triplet loss 0.655000448 l2_loss 23.2710152 fraction B 0.205993459 lossA 2.26799512 fraction A 0.0372716598\n",
      "step 4194 loss 0.880619 fisher_loss 0.228458419 triplet loss 0.652160585 l2_loss 23.2785072 fraction B 0.213106483 lossA 2.38569784 fraction A 0.049047403\n",
      "step 4195 loss 0.987272203 fisher_loss 0.229538843 triplet loss 0.757733345 l2_loss 23.2871838 fraction B 0.154137209 lossA 2.50971675 fraction A 0.0602183342\n",
      "step 4196 loss 1.03000665 fisher_loss 0.231046855 triplet loss 0.798959732 l2_loss 23.2958584 fraction B 0.192321286 lossA 2.58688736 fraction A 0.0666742697\n",
      "step 4197 loss 1.20999837 fisher_loss 0.232286692 triplet loss 0.977711618 l2_loss 23.3025589 fraction B 0.220721483 lossA 2.54786563 fraction A 0.0641004667\n",
      "step 4198 loss 1.19352913 fisher_loss 0.23265034 triplet loss 0.960878849 l2_loss 23.3073807 fraction B 0.209514827 lossA 2.47980213 fraction A 0.0589381158\n",
      "step 4199 loss 1.0162245 fisher_loss 0.232513726 triplet loss 0.783710778 l2_loss 23.3105774 fraction B 0.140484467 lossA 2.40961623 fraction A 0.0516877808\n",
      "step 4200 loss 1.03549063 fisher_loss 0.231918305 triplet loss 0.803572357 l2_loss 23.3131561 fraction B 0.212375656 lossA 2.2774384 fraction A 0.0406201221\n",
      "step 4201 loss 1.28341472 fisher_loss 0.230904326 triplet loss 1.05251038 l2_loss 23.3147717 fraction B 0.151722267 lossA 2.1464994 fraction A 0.0336204842\n",
      "step 4202 loss 0.985344112 fisher_loss 0.23013474 triplet loss 0.755209386 l2_loss 23.3167953 fraction B 0.144650325 lossA 2.13854575 fraction A 0.0333127379\n",
      "step 4203 loss 0.948092759 fisher_loss 0.229928687 triplet loss 0.718164086 l2_loss 23.3220081 fraction B 0.154417127 lossA 2.23536706 fraction A 0.038105797\n",
      "step 4204 loss 1.08332217 fisher_loss 0.230266169 triplet loss 0.853056 l2_loss 23.3296337 fraction B 0.167130977 lossA 2.36905503 fraction A 0.0475820117\n",
      "step 4205 loss 1.11118484 fisher_loss 0.23069194 triplet loss 0.880492926 l2_loss 23.338131 fraction B 0.190217838 lossA 2.48608947 fraction A 0.0574354529\n",
      "step 4206 loss 1.3707304 fisher_loss 0.231244728 triplet loss 1.13948572 l2_loss 23.3467827 fraction B 0.190103605 lossA 2.55337572 fraction A 0.0629442334\n",
      "step 4207 loss 0.95737642 fisher_loss 0.231322154 triplet loss 0.726054251 l2_loss 23.3533249 fraction B 0.236128345 lossA 2.50420427 fraction A 0.0591344759\n",
      "step 4208 loss 1.23143387 fisher_loss 0.230474591 triplet loss 1.00095928 l2_loss 23.3575191 fraction B 0.0732326284 lossA 2.40183377 fraction A 0.0514139\n",
      "step 4209 loss 1.20966423 fisher_loss 0.229246363 triplet loss 0.980417907 l2_loss 23.3618431 fraction B 0.267720819 lossA 2.3135078 fraction A 0.043761462\n",
      "step 4210 loss 0.917548239 fisher_loss 0.228396416 triplet loss 0.689151824 l2_loss 23.3662415 fraction B 0.177225247 lossA 2.23476672 fraction A 0.037099421\n",
      "step 4211 loss 1.25996804 fisher_loss 0.227960184 triplet loss 1.03200781 l2_loss 23.3708725 fraction B 0.233493194 lossA 2.18970108 fraction A 0.0341762118\n",
      "step 4212 loss 1.00390399 fisher_loss 0.227746457 triplet loss 0.776157558 l2_loss 23.3754692 fraction B 0.151766852 lossA 2.20512533 fraction A 0.0351038389\n",
      "step 4213 loss 1.1425271 fisher_loss 0.22799173 triplet loss 0.914535344 l2_loss 23.3808155 fraction B 0.153434366 lossA 2.24940109 fraction A 0.0382141359\n",
      "step 4214 loss 0.90864 fisher_loss 0.228336453 triplet loss 0.680303574 l2_loss 23.3859196 fraction B 0.233485341 lossA 2.32120848 fraction A 0.0445716158\n",
      "step 4215 loss 1.12692523 fisher_loss 0.229194596 triplet loss 0.897730649 l2_loss 23.3921604 fraction B 0.160424516 lossA 2.39422131 fraction A 0.0523282364\n",
      "step 4216 loss 1.10330653 fisher_loss 0.230201989 triplet loss 0.873104513 l2_loss 23.3984966 fraction B 0.109298721 lossA 2.50649047 fraction A 0.0626728535\n",
      "step 4217 loss 1.25700831 fisher_loss 0.231528863 triplet loss 1.02547944 l2_loss 23.4060268 fraction B 0.247817278 lossA 2.59943509 fraction A 0.0696967617\n",
      "step 4218 loss 1.05819 fisher_loss 0.232412055 triplet loss 0.825777948 l2_loss 23.4122944 fraction B 0.190852389 lossA 2.59631467 fraction A 0.0691661686\n",
      "step 4219 loss 1.20774031 fisher_loss 0.232407138 triplet loss 0.975333214 l2_loss 23.4167461 fraction B 0.239379883 lossA 2.51357794 fraction A 0.0628829077\n",
      "step 4220 loss 1.33786988 fisher_loss 0.2317608 triplet loss 1.10610902 l2_loss 23.4198875 fraction B 0.159414083 lossA 2.41993237 fraction A 0.0549336597\n",
      "step 4221 loss 0.945630074 fisher_loss 0.230733439 triplet loss 0.714896619 l2_loss 23.4213 fraction B 0.144481108 lossA 2.33445048 fraction A 0.0458925515\n",
      "step 4222 loss 1.1590209 fisher_loss 0.229666829 triplet loss 0.929354072 l2_loss 23.4226513 fraction B 0.149490967 lossA 2.22354913 fraction A 0.0371925868\n",
      "step 4223 loss 1.26521087 fisher_loss 0.228907332 triplet loss 1.03630352 l2_loss 23.4239864 fraction B 0.369648844 lossA 2.19858265 fraction A 0.0356863365\n",
      "step 4224 loss 1.1084547 fisher_loss 0.228388518 triplet loss 0.880066216 l2_loss 23.4256172 fraction B 0.205898538 lossA 2.27482581 fraction A 0.0395925567\n",
      "step 4225 loss 1.18896139 fisher_loss 0.22806482 triplet loss 0.960896611 l2_loss 23.4300137 fraction B 0.223372713 lossA 2.37241268 fraction A 0.0452239513\n",
      "step 4226 loss 1.04587877 fisher_loss 0.227768213 triplet loss 0.818110526 l2_loss 23.4334602 fraction B 0.178606436 lossA 2.46177149 fraction A 0.0515761785\n",
      "step 4227 loss 1.15664625 fisher_loss 0.227703169 triplet loss 0.928943038 l2_loss 23.437891 fraction B 0.230131194 lossA 2.57260919 fraction A 0.0604359955\n",
      "step 4228 loss 1.20491397 fisher_loss 0.227727816 triplet loss 0.977186203 l2_loss 23.4428768 fraction B 0.206554458 lossA 2.66292691 fraction A 0.0670161396\n",
      "step 4229 loss 1.10164022 fisher_loss 0.22772336 triplet loss 0.873916924 l2_loss 23.4469566 fraction B 0.140561923 lossA 2.68936133 fraction A 0.0689060912\n",
      "step 4230 loss 0.953097463 fisher_loss 0.227304101 triplet loss 0.725793362 l2_loss 23.450325 fraction B 0.164891094 lossA 2.64782214 fraction A 0.06580735\n",
      "step 4231 loss 1.18333387 fisher_loss 0.226628169 triplet loss 0.956705749 l2_loss 23.4532757 fraction B 0.135899514 lossA 2.51886249 fraction A 0.0551657751\n",
      "step 4232 loss 1.18003345 fisher_loss 0.225424781 triplet loss 0.954608619 l2_loss 23.4547977 fraction B 0.120615751 lossA 2.37967682 fraction A 0.0431106836\n",
      "step 4233 loss 1.33219469 fisher_loss 0.224396095 triplet loss 1.10779858 l2_loss 23.4557743 fraction B 0.214361385 lossA 2.18052888 fraction A 0.0321661793\n",
      "step 4234 loss 0.815777123 fisher_loss 0.223748907 triplet loss 0.592028201 l2_loss 23.4566498 fraction B 0.121181436 lossA 2.02049327 fraction A 0.0269178748\n",
      "step 4235 loss 0.930462599 fisher_loss 0.223655075 triplet loss 0.706807494 l2_loss 23.4595776 fraction B 0.290471286 lossA 2.12462807 fraction A 0.0298248548\n",
      "step 4236 loss 1.27306783 fisher_loss 0.22414276 triplet loss 1.04892504 l2_loss 23.4656448 fraction B 0.33919242 lossA 2.34716153 fraction A 0.038861692\n",
      "step 4237 loss 0.94981885 fisher_loss 0.225248694 triplet loss 0.724570155 l2_loss 23.4724445 fraction B 0.137692481 lossA 2.48553562 fraction A 0.0503731892\n",
      "step 4238 loss 0.969960093 fisher_loss 0.226607859 triplet loss 0.743352234 l2_loss 23.4808426 fraction B 0.183066607 lossA 2.69470859 fraction A 0.0678075626\n",
      "step 4239 loss 0.820174634 fisher_loss 0.228582785 triplet loss 0.591591835 l2_loss 23.4902592 fraction B 0.169378072 lossA 2.79191375 fraction A 0.075226441\n",
      "step 4240 loss 1.00680375 fisher_loss 0.229545683 triplet loss 0.777258039 l2_loss 23.4979153 fraction B 0.1951603 lossA 2.72808337 fraction A 0.0713860765\n",
      "step 4241 loss 1.34722269 fisher_loss 0.228837639 triplet loss 1.11838508 l2_loss 23.5030365 fraction B 0.253383309 lossA 2.52676249 fraction A 0.0568966046\n",
      "step 4242 loss 1.0828476 fisher_loss 0.226969332 triplet loss 0.855878294 l2_loss 23.5055752 fraction B 0.368277073 lossA 2.33100963 fraction A 0.0414821133\n",
      "step 4243 loss 1.15256059 fisher_loss 0.225523978 triplet loss 0.927036643 l2_loss 23.5069141 fraction B 0.128287554 lossA 2.21668 fraction A 0.0348129347\n",
      "step 4244 loss 1.34753525 fisher_loss 0.224982187 triplet loss 1.12255311 l2_loss 23.5096321 fraction B 0.213877171 lossA 2.18650651 fraction A 0.033554595\n",
      "step 4245 loss 1.02232099 fisher_loss 0.224968091 triplet loss 0.79735291 l2_loss 23.5123787 fraction B 0.224239632 lossA 2.2406435 fraction A 0.0363798216\n",
      "step 4246 loss 0.983216405 fisher_loss 0.225296 triplet loss 0.757920384 l2_loss 23.5165558 fraction B 0.186940327 lossA 2.41102648 fraction A 0.0481217317\n",
      "step 4247 loss 1.08651972 fisher_loss 0.226400614 triplet loss 0.860119104 l2_loss 23.5237 fraction B 0.240034372 lossA 2.58342409 fraction A 0.0620707609\n",
      "step 4248 loss 1.14997983 fisher_loss 0.22789681 triplet loss 0.92208308 l2_loss 23.5308876 fraction B 0.158231556 lossA 2.87627316 fraction A 0.0823593438\n",
      "step 4249 loss 0.950364351 fisher_loss 0.230335221 triplet loss 0.720029116 l2_loss 23.5390511 fraction B 0.149397537 lossA 3.01844454 fraction A 0.0917751417\n",
      "step 4250 loss 1.36709011 fisher_loss 0.231799468 triplet loss 1.13529062 l2_loss 23.5451889 fraction B 0.260789424 lossA 2.97678614 fraction A 0.0896595195\n",
      "step 4251 loss 0.987288833 fisher_loss 0.231566399 triplet loss 0.755722404 l2_loss 23.5480194 fraction B 0.159256265 lossA 2.80974936 fraction A 0.0795324892\n",
      "step 4252 loss 1.10592914 fisher_loss 0.23030214 triplet loss 0.875627041 l2_loss 23.5493927 fraction B 0.184791461 lossA 2.55072808 fraction A 0.0627131313\n",
      "step 4253 loss 1.04047132 fisher_loss 0.228679493 triplet loss 0.811791778 l2_loss 23.5493317 fraction B 0.172757804 lossA 2.28705692 fraction A 0.0428688265\n",
      "step 4254 loss 1.02414882 fisher_loss 0.227257863 triplet loss 0.796890914 l2_loss 23.5491676 fraction B 0.269987434 lossA 2.02018523 fraction A 0.0294982344\n",
      "step 4255 loss 1.10242546 fisher_loss 0.226740047 triplet loss 0.875685453 l2_loss 23.5500584 fraction B 0.273349226 lossA 1.84101319 fraction A 0.0243594665\n",
      "step 4256 loss 1.01645291 fisher_loss 0.226891741 triplet loss 0.789561212 l2_loss 23.5525341 fraction B 0.271273226 lossA 1.92274773 fraction A 0.0269887261\n",
      "step 4257 loss 1.15063965 fisher_loss 0.227429464 triplet loss 0.923210204 l2_loss 23.558363 fraction B 0.365593761 lossA 2.11501098 fraction A 0.0348639376\n",
      "step 4258 loss 1.03292704 fisher_loss 0.228256762 triplet loss 0.804670274 l2_loss 23.5652142 fraction B 0.216773987 lossA 2.32402182 fraction A 0.052990783\n",
      "step 4259 loss 1.02204049 fisher_loss 0.229991749 triplet loss 0.792048752 l2_loss 23.5745106 fraction B 0.150904506 lossA 2.61367273 fraction A 0.0748340786\n",
      "step 4260 loss 1.24735761 fisher_loss 0.232644171 triplet loss 1.01471341 l2_loss 23.5846653 fraction B 0.234661788 lossA 2.84742761 fraction A 0.0899497718\n",
      "step 4261 loss 1.08588326 fisher_loss 0.235042691 triplet loss 0.850840569 l2_loss 23.5925388 fraction B 0.151289567 lossA 2.90819478 fraction A 0.0938247293\n",
      "step 4262 loss 1.1678524 fisher_loss 0.236077651 triplet loss 0.931774795 l2_loss 23.5981617 fraction B 0.380365729 lossA 2.84055 fraction A 0.0890458301\n",
      "step 4263 loss 1.18693006 fisher_loss 0.235734716 triplet loss 0.951195359 l2_loss 23.6011887 fraction B 0.292933345 lossA 2.64279 fraction A 0.074872911\n",
      "step 4264 loss 1.12447476 fisher_loss 0.234363183 triplet loss 0.890111625 l2_loss 23.6019802 fraction B 0.183399096 lossA 2.39794 fraction A 0.0550164282\n",
      "step 4265 loss 1.21073008 fisher_loss 0.232704476 triplet loss 0.978025615 l2_loss 23.6007633 fraction B 0.205544084 lossA 2.21083522 fraction A 0.0380396545\n",
      "step 4266 loss 1.34911108 fisher_loss 0.231656134 triplet loss 1.11745489 l2_loss 23.5996628 fraction B 0.148809016 lossA 2.09543729 fraction A 0.0313103832\n",
      "step 4267 loss 1.04161191 fisher_loss 0.231491268 triplet loss 0.810120642 l2_loss 23.6008053 fraction B 0.180906713 lossA 2.12996387 fraction A 0.0325236619\n",
      "step 4268 loss 1.15778923 fisher_loss 0.2320389 triplet loss 0.925750375 l2_loss 23.6052723 fraction B 0.241036102 lossA 2.25409913 fraction A 0.0381790735\n",
      "step 4269 loss 1.11745536 fisher_loss 0.232860297 triplet loss 0.884595037 l2_loss 23.6105347 fraction B 0.236138925 lossA 2.4266603 fraction A 0.050388474\n",
      "step 4270 loss 1.15493679 fisher_loss 0.234364316 triplet loss 0.920572519 l2_loss 23.6184063 fraction B 0.119491354 lossA 2.60366964 fraction A 0.063816078\n",
      "step 4271 loss 1.219805 fisher_loss 0.236197695 triplet loss 0.983607292 l2_loss 23.6257973 fraction B 0.166142538 lossA 2.73441958 fraction A 0.0725245923\n",
      "step 4272 loss 1.16245401 fisher_loss 0.237646744 triplet loss 0.92480725 l2_loss 23.6318874 fraction B 0.189712465 lossA 2.78587818 fraction A 0.0752496272\n",
      "step 4273 loss 1.23780203 fisher_loss 0.238332465 triplet loss 0.999469578 l2_loss 23.6367073 fraction B 0.234859079 lossA 2.76973224 fraction A 0.0732282177\n",
      "step 4274 loss 1.1381886 fisher_loss 0.238348991 triplet loss 0.89983964 l2_loss 23.6401653 fraction B 0.190008327 lossA 2.71135283 fraction A 0.0689227134\n",
      "step 4275 loss 1.16077685 fisher_loss 0.237646505 triplet loss 0.923130393 l2_loss 23.6432571 fraction B 0.172211856 lossA 2.58502245 fraction A 0.0594071634\n",
      "step 4276 loss 1.10034072 fisher_loss 0.236154228 triplet loss 0.864186466 l2_loss 23.6448383 fraction B 0.207178786 lossA 2.45383167 fraction A 0.0488985255\n",
      "step 4277 loss 1.13881886 fisher_loss 0.23453325 triplet loss 0.90428561 l2_loss 23.6460476 fraction B 0.107494108 lossA 2.29834962 fraction A 0.0368409343\n",
      "step 4278 loss 0.955905557 fisher_loss 0.23320885 triplet loss 0.722696722 l2_loss 23.6471863 fraction B 0.173206136 lossA 2.22297478 fraction A 0.0332891941\n",
      "step 4279 loss 1.07719088 fisher_loss 0.232552275 triplet loss 0.844638646 l2_loss 23.6506062 fraction B 0.217569 lossA 2.24606681 fraction A 0.0344717167\n",
      "step 4280 loss 1.1160934 fisher_loss 0.232429013 triplet loss 0.883664429 l2_loss 23.6556931 fraction B 0.22231032 lossA 2.30155301 fraction A 0.0379366353\n",
      "step 4281 loss 1.08084035 fisher_loss 0.232275307 triplet loss 0.848565 l2_loss 23.661562 fraction B 0.187009797 lossA 2.38534 fraction A 0.0454046652\n",
      "step 4282 loss 1.03748727 fisher_loss 0.232565016 triplet loss 0.804922223 l2_loss 23.668272 fraction B 0.223399684 lossA 2.47357249 fraction A 0.0543224737\n",
      "step 4283 loss 1.03504479 fisher_loss 0.233075276 triplet loss 0.801969469 l2_loss 23.6741066 fraction B 0.217791796 lossA 2.59804416 fraction A 0.0646598116\n",
      "step 4284 loss 1.1109637 fisher_loss 0.234156147 triplet loss 0.87680757 l2_loss 23.6806793 fraction B 0.160935387 lossA 2.66684985 fraction A 0.0698632374\n",
      "step 4285 loss 1.13584661 fisher_loss 0.234890833 triplet loss 0.900955737 l2_loss 23.6862564 fraction B 0.173371449 lossA 2.69069266 fraction A 0.0722059384\n",
      "step 4286 loss 1.47183442 fisher_loss 0.23537679 triplet loss 1.23645759 l2_loss 23.6913872 fraction B 0.211492389 lossA 2.6274128 fraction A 0.0680607259\n",
      "step 4287 loss 1.29807472 fisher_loss 0.235117301 triplet loss 1.06295741 l2_loss 23.6931038 fraction B 0.219020963 lossA 2.5317297 fraction A 0.0611562468\n",
      "step 4288 loss 1.28850722 fisher_loss 0.234544352 triplet loss 1.05396283 l2_loss 23.6941319 fraction B 0.232431278 lossA 2.45248866 fraction A 0.0553108454\n",
      "step 4289 loss 1.21564972 fisher_loss 0.233851045 triplet loss 0.981798708 l2_loss 23.694746 fraction B 0.166031018 lossA 2.38722658 fraction A 0.049474068\n",
      "step 4290 loss 1.17446184 fisher_loss 0.233122766 triplet loss 0.941339076 l2_loss 23.6953259 fraction B 0.336296707 lossA 2.31835127 fraction A 0.0443960465\n",
      "step 4291 loss 1.20263529 fisher_loss 0.232253 triplet loss 0.970382273 l2_loss 23.6964531 fraction B 0.21358937 lossA 2.28607178 fraction A 0.042356275\n",
      "step 4292 loss 1.04024351 fisher_loss 0.231440037 triplet loss 0.808803499 l2_loss 23.6981812 fraction B 0.220065758 lossA 2.29325485 fraction A 0.0427804776\n",
      "step 4293 loss 1.43039966 fisher_loss 0.230879396 triplet loss 1.19952023 l2_loss 23.7004795 fraction B 0.221948162 lossA 2.29842353 fraction A 0.043436\n",
      "step 4294 loss 1.25302362 fisher_loss 0.230388343 triplet loss 1.02263534 l2_loss 23.7030373 fraction B 0.282937467 lossA 2.35617661 fraction A 0.0488761775\n",
      "step 4295 loss 1.13718426 fisher_loss 0.230281308 triplet loss 0.906902969 l2_loss 23.7067699 fraction B 0.115577787 lossA 2.38646579 fraction A 0.0535118096\n",
      "step 4296 loss 1.21977723 fisher_loss 0.230339304 triplet loss 0.989437938 l2_loss 23.7116547 fraction B 0.288520098 lossA 2.37876201 fraction A 0.0542652793\n",
      "step 4297 loss 1.07737827 fisher_loss 0.23008503 triplet loss 0.847293258 l2_loss 23.7150898 fraction B 0.190492749 lossA 2.35906744 fraction A 0.0531925447\n",
      "step 4298 loss 0.945832908 fisher_loss 0.229844585 triplet loss 0.715988338 l2_loss 23.7179699 fraction B 0.168837339 lossA 2.34318733 fraction A 0.052270785\n",
      "step 4299 loss 1.07980359 fisher_loss 0.229868218 triplet loss 0.849935353 l2_loss 23.721632 fraction B 0.179308832 lossA 2.31991172 fraction A 0.049888853\n",
      "step 4300 loss 1.03821719 fisher_loss 0.229656816 triplet loss 0.808560371 l2_loss 23.7242489 fraction B 0.181543976 lossA 2.33033538 fraction A 0.0494641811\n",
      "step 4301 loss 1.10637724 fisher_loss 0.229450479 triplet loss 0.87692678 l2_loss 23.7265968 fraction B 0.206237331 lossA 2.34554553 fraction A 0.0487638488\n",
      "step 4302 loss 0.759627104 fisher_loss 0.228983015 triplet loss 0.530644119 l2_loss 23.7284546 fraction B 0.120826013 lossA 2.37653685 fraction A 0.0498232171\n",
      "step 4303 loss 0.989511847 fisher_loss 0.22890535 triplet loss 0.760606468 l2_loss 23.7326908 fraction B 0.172313154 lossA 2.40314293 fraction A 0.0503980741\n",
      "step 4304 loss 1.64418769 fisher_loss 0.228859484 triplet loss 1.41532815 l2_loss 23.7357903 fraction B 0.104820654 lossA 2.39191365 fraction A 0.0486610867\n",
      "step 4305 loss 1.21238744 fisher_loss 0.228638083 triplet loss 0.98374933 l2_loss 23.7376747 fraction B 0.193469375 lossA 2.36477971 fraction A 0.0456492417\n",
      "step 4306 loss 1.02328157 fisher_loss 0.228411943 triplet loss 0.794869661 l2_loss 23.7391586 fraction B 0.23379831 lossA 2.34034801 fraction A 0.0436196849\n",
      "step 4307 loss 0.984408438 fisher_loss 0.228082895 triplet loss 0.756325543 l2_loss 23.7414169 fraction B 0.18717511 lossA 2.31589365 fraction A 0.0422429703\n",
      "step 4308 loss 1.12413847 fisher_loss 0.227730304 triplet loss 0.896408141 l2_loss 23.7455673 fraction B 0.247176 lossA 2.32188034 fraction A 0.043054577\n",
      "step 4309 loss 0.902597308 fisher_loss 0.227580428 triplet loss 0.67501688 l2_loss 23.7504692 fraction B 0.133879468 lossA 2.37451887 fraction A 0.0479129069\n",
      "step 4310 loss 1.52303958 fisher_loss 0.228146344 triplet loss 1.29489326 l2_loss 23.7582226 fraction B 0.235771164 lossA 2.42257857 fraction A 0.0538483933\n",
      "step 4311 loss 1.05312657 fisher_loss 0.228751197 triplet loss 0.824375391 l2_loss 23.7655544 fraction B 0.157388911 lossA 2.45369029 fraction A 0.0571875088\n",
      "step 4312 loss 0.95931226 fisher_loss 0.229067251 triplet loss 0.730245 l2_loss 23.7713909 fraction B 0.11174468 lossA 2.43294191 fraction A 0.0562991463\n",
      "step 4313 loss 1.11608291 fisher_loss 0.228939816 triplet loss 0.887143135 l2_loss 23.7759247 fraction B 0.235023886 lossA 2.42652178 fraction A 0.0556089245\n",
      "step 4314 loss 1.06794477 fisher_loss 0.228771895 triplet loss 0.8391729 l2_loss 23.7800121 fraction B 0.246726781 lossA 2.37784672 fraction A 0.0508136936\n",
      "step 4315 loss 0.88201046 fisher_loss 0.228170186 triplet loss 0.653840244 l2_loss 23.7827 fraction B 0.101147071 lossA 2.31204033 fraction A 0.0446105562\n",
      "step 4316 loss 1.16132343 fisher_loss 0.227355689 triplet loss 0.93396771 l2_loss 23.7856541 fraction B 0.225570306 lossA 2.28561687 fraction A 0.0416007042\n",
      "step 4317 loss 1.16951835 fisher_loss 0.226734981 triplet loss 0.942783356 l2_loss 23.7879562 fraction B 0.184567079 lossA 2.27563334 fraction A 0.0403228402\n",
      "step 4318 loss 0.94434303 fisher_loss 0.226222113 triplet loss 0.718120933 l2_loss 23.7900276 fraction B 0.119095765 lossA 2.27894521 fraction A 0.0401977263\n",
      "step 4319 loss 1.08500373 fisher_loss 0.226079181 triplet loss 0.858924568 l2_loss 23.7921047 fraction B 0.145679832 lossA 2.34197021 fraction A 0.0448207557\n",
      "step 4320 loss 1.11464953 fisher_loss 0.226696908 triplet loss 0.887952626 l2_loss 23.7952385 fraction B 0.142898306 lossA 2.43563843 fraction A 0.051856108\n",
      "step 4321 loss 1.01614475 fisher_loss 0.227643609 triplet loss 0.788501203 l2_loss 23.7987633 fraction B 0.134054169 lossA 2.48899245 fraction A 0.0551757\n",
      "step 4322 loss 1.57169855 fisher_loss 0.228195429 triplet loss 1.34350312 l2_loss 23.8024902 fraction B 0.214371726 lossA 2.5089519 fraction A 0.0555740036\n",
      "step 4323 loss 1.29806376 fisher_loss 0.228442952 triplet loss 1.06962085 l2_loss 23.8046398 fraction B 0.166779876 lossA 2.51367307 fraction A 0.0556505807\n",
      "step 4324 loss 1.21346581 fisher_loss 0.22849308 triplet loss 0.984972775 l2_loss 23.8067932 fraction B 0.191156462 lossA 2.47906184 fraction A 0.052943334\n",
      "step 4325 loss 1.10628462 fisher_loss 0.227977678 triplet loss 0.878307 l2_loss 23.8082561 fraction B 0.25 lossA 2.47458148 fraction A 0.0537357777\n",
      "step 4326 loss 1.03829467 fisher_loss 0.227532133 triplet loss 0.810762584 l2_loss 23.8110981 fraction B 0.181637228 lossA 2.49587774 fraction A 0.0568494424\n",
      "step 4327 loss 1.30262125 fisher_loss 0.227360204 triplet loss 1.075261 l2_loss 23.8154812 fraction B 0.169637248 lossA 2.50682831 fraction A 0.0589046814\n",
      "step 4328 loss 1.19986355 fisher_loss 0.227174774 triplet loss 0.972688794 l2_loss 23.8195324 fraction B 0.257445037 lossA 2.4829576 fraction A 0.0589214899\n",
      "step 4329 loss 1.02648306 fisher_loss 0.226753697 triplet loss 0.799729347 l2_loss 23.8227272 fraction B 0.1294934 lossA 2.41183853 fraction A 0.05474139\n",
      "step 4330 loss 1.10554814 fisher_loss 0.226373702 triplet loss 0.879174411 l2_loss 23.8259907 fraction B 0.248354852 lossA 2.34180665 fraction A 0.0502436571\n",
      "step 4331 loss 1.17681396 fisher_loss 0.225886479 triplet loss 0.950927496 l2_loss 23.8285809 fraction B 0.133330584 lossA 2.29274154 fraction A 0.0476043262\n",
      "step 4332 loss 1.275769 fisher_loss 0.22577 triplet loss 1.049999 l2_loss 23.8321819 fraction B 0.234218195 lossA 2.28669786 fraction A 0.0487195849\n",
      "step 4333 loss 1.06095278 fisher_loss 0.225865394 triplet loss 0.835087359 l2_loss 23.8354 fraction B 0.29448539 lossA 2.30081391 fraction A 0.050982561\n",
      "step 4334 loss 1.18234277 fisher_loss 0.225767583 triplet loss 0.956575155 l2_loss 23.8385353 fraction B 0.317427903 lossA 2.33617902 fraction A 0.0541511402\n",
      "step 4335 loss 0.977392137 fisher_loss 0.225589573 triplet loss 0.751802564 l2_loss 23.8409538 fraction B 0.185595572 lossA 2.34815049 fraction A 0.0542875566\n",
      "step 4336 loss 1.14587581 fisher_loss 0.225309938 triplet loss 0.920565844 l2_loss 23.8420067 fraction B 0.232055783 lossA 2.34137058 fraction A 0.052220691\n",
      "step 4337 loss 1.20556021 fisher_loss 0.225022852 triplet loss 0.980537355 l2_loss 23.8423138 fraction B 0.258167893 lossA 2.31893659 fraction A 0.0490641706\n",
      "step 4338 loss 1.10495782 fisher_loss 0.224562332 triplet loss 0.880395472 l2_loss 23.8421669 fraction B 0.201148495 lossA 2.2942996 fraction A 0.0459557585\n",
      "step 4339 loss 0.972459674 fisher_loss 0.224264815 triplet loss 0.748194873 l2_loss 23.8426685 fraction B 0.164906397 lossA 2.29952455 fraction A 0.0459764041\n",
      "step 4340 loss 1.05570579 fisher_loss 0.224332497 triplet loss 0.831373334 l2_loss 23.8453293 fraction B 0.216263503 lossA 2.30776572 fraction A 0.0464440398\n",
      "step 4341 loss 1.46780729 fisher_loss 0.224573016 triplet loss 1.24323428 l2_loss 23.8486347 fraction B 0.178574413 lossA 2.3198061 fraction A 0.0472986437\n",
      "step 4342 loss 1.13550293 fisher_loss 0.224725887 triplet loss 0.910777092 l2_loss 23.85145 fraction B 0.151117027 lossA 2.33803034 fraction A 0.0485023931\n",
      "step 4343 loss 1.09027719 fisher_loss 0.225142628 triplet loss 0.865134537 l2_loss 23.8547 fraction B 0.215862498 lossA 2.38543153 fraction A 0.0522853471\n",
      "step 4344 loss 1.15024841 fisher_loss 0.225749508 triplet loss 0.924498856 l2_loss 23.8585663 fraction B 0.189499393 lossA 2.40615392 fraction A 0.0542712212\n",
      "step 4345 loss 1.02101684 fisher_loss 0.226191461 triplet loss 0.794825435 l2_loss 23.862402 fraction B 0.21289742 lossA 2.40433264 fraction A 0.054327257\n",
      "step 4346 loss 1.06985688 fisher_loss 0.226537511 triplet loss 0.843319416 l2_loss 23.8656883 fraction B 0.152272 lossA 2.39584255 fraction A 0.0537952185\n",
      "step 4347 loss 1.01474965 fisher_loss 0.226867363 triplet loss 0.787882328 l2_loss 23.8688126 fraction B 0.137691334 lossA 2.38593793 fraction A 0.0525687523\n",
      "step 4348 loss 1.02508962 fisher_loss 0.227250904 triplet loss 0.797838748 l2_loss 23.8720226 fraction B 0.183823839 lossA 2.36442566 fraction A 0.0510031022\n",
      "step 4349 loss 1.15262508 fisher_loss 0.227605432 triplet loss 0.925019622 l2_loss 23.8766136 fraction B 0.268148869 lossA 2.34857726 fraction A 0.0497425869\n",
      "step 4350 loss 1.00042403 fisher_loss 0.227866277 triplet loss 0.772557735 l2_loss 23.8809605 fraction B 0.17066282 lossA 2.34508181 fraction A 0.0498094894\n",
      "step 4351 loss 1.19461679 fisher_loss 0.22830373 triplet loss 0.966313064 l2_loss 23.8858051 fraction B 0.184493303 lossA 2.34113216 fraction A 0.0497711673\n",
      "step 4352 loss 1.32542562 fisher_loss 0.228684545 triplet loss 1.09674108 l2_loss 23.8908787 fraction B 0.246795967 lossA 2.32392979 fraction A 0.0477727614\n",
      "step 4353 loss 0.934261858 fisher_loss 0.228657112 triplet loss 0.705604732 l2_loss 23.8943195 fraction B 0.172757268 lossA 2.29078603 fraction A 0.0451545864\n",
      "step 4354 loss 0.814218223 fisher_loss 0.228169084 triplet loss 0.586049139 l2_loss 23.8974743 fraction B 0.10139472 lossA 2.34487057 fraction A 0.0507980809\n",
      "step 4355 loss 1.13182378 fisher_loss 0.228447258 triplet loss 0.90337652 l2_loss 23.9035378 fraction B 0.191008642 lossA 2.41165233 fraction A 0.0563422143\n",
      "step 4356 loss 1.35300171 fisher_loss 0.228587195 triplet loss 1.12441456 l2_loss 23.9086819 fraction B 0.191227719 lossA 2.45094633 fraction A 0.0587797128\n",
      "step 4357 loss 0.981481194 fisher_loss 0.228779346 triplet loss 0.752701819 l2_loss 23.9128284 fraction B 0.192274585 lossA 2.43071628 fraction A 0.0565037318\n",
      "step 4358 loss 1.03125179 fisher_loss 0.228658661 triplet loss 0.802593112 l2_loss 23.9165897 fraction B 0.218447387 lossA 2.41692233 fraction A 0.0552382581\n",
      "step 4359 loss 0.99666363 fisher_loss 0.228800729 triplet loss 0.767862916 l2_loss 23.9213982 fraction B 0.181145012 lossA 2.37441015 fraction A 0.0509975962\n",
      "step 4360 loss 1.2681427 fisher_loss 0.228470162 triplet loss 1.03967249 l2_loss 23.9261093 fraction B 0.182369143 lossA 2.36308646 fraction A 0.0501666479\n",
      "step 4361 loss 0.988292 fisher_loss 0.228377655 triplet loss 0.759914339 l2_loss 23.9318409 fraction B 0.162848234 lossA 2.32232904 fraction A 0.0467547588\n",
      "step 4362 loss 0.825458944 fisher_loss 0.22802262 triplet loss 0.597436309 l2_loss 23.9376221 fraction B 0.175122544 lossA 2.25014544 fraction A 0.041041173\n",
      "step 4363 loss 1.0753423 fisher_loss 0.227348566 triplet loss 0.847993731 l2_loss 23.944397 fraction B 0.286132634 lossA 2.19346 fraction A 0.0371060148\n",
      "step 4364 loss 1.0345788 fisher_loss 0.226660699 triplet loss 0.807918131 l2_loss 23.9493732 fraction B 0.130002379 lossA 2.24940968 fraction A 0.0415481105\n",
      "step 4365 loss 0.940058768 fisher_loss 0.226595894 triplet loss 0.713462889 l2_loss 23.9554939 fraction B 0.177808508 lossA 2.39213443 fraction A 0.0538105033\n",
      "step 4366 loss 1.21731174 fisher_loss 0.22744824 triplet loss 0.989863515 l2_loss 23.9627533 fraction B 0.130032942 lossA 2.48486614 fraction A 0.0612162314\n",
      "step 4367 loss 1.07857811 fisher_loss 0.228193283 triplet loss 0.850384831 l2_loss 23.9691277 fraction B 0.197203338 lossA 2.51465893 fraction A 0.062819846\n",
      "step 4368 loss 1.16875577 fisher_loss 0.228534952 triplet loss 0.940220773 l2_loss 23.9743137 fraction B 0.278336793 lossA 2.49419498 fraction A 0.060477\n",
      "step 4369 loss 0.751388907 fisher_loss 0.228469253 triplet loss 0.522919655 l2_loss 23.9787483 fraction B 0.120730042 lossA 2.37532067 fraction A 0.0491235405\n",
      "step 4370 loss 1.12430501 fisher_loss 0.227953985 triplet loss 0.896351039 l2_loss 23.9816322 fraction B 0.152598888 lossA 2.26014781 fraction A 0.0399646722\n",
      "step 4371 loss 1.25793111 fisher_loss 0.227690905 triplet loss 1.03024018 l2_loss 23.9840355 fraction B 0.246471778 lossA 2.17043138 fraction A 0.0348627418\n",
      "step 4372 loss 1.04190898 fisher_loss 0.227550805 triplet loss 0.814358115 l2_loss 23.9857769 fraction B 0.207387388 lossA 2.17474508 fraction A 0.0348302871\n",
      "step 4373 loss 1.04983449 fisher_loss 0.22746554 triplet loss 0.822369 l2_loss 23.988575 fraction B 0.205803677 lossA 2.28160334 fraction A 0.0411726236\n",
      "step 4374 loss 1.08693278 fisher_loss 0.227721348 triplet loss 0.859211445 l2_loss 23.9953461 fraction B 0.250833154 lossA 2.3727715 fraction A 0.0487848967\n",
      "step 4375 loss 0.932249308 fisher_loss 0.227940902 triplet loss 0.704308391 l2_loss 24.0032063 fraction B 0.151396692 lossA 2.43691182 fraction A 0.0552984215\n",
      "step 4376 loss 1.07321942 fisher_loss 0.228386179 triplet loss 0.844833195 l2_loss 24.0113869 fraction B 0.205180049 lossA 2.52089429 fraction A 0.0623237751\n",
      "step 4377 loss 1.09609568 fisher_loss 0.228983551 triplet loss 0.86711216 l2_loss 24.0189877 fraction B 0.218147188 lossA 2.525002 fraction A 0.0624339655\n",
      "step 4378 loss 0.923342347 fisher_loss 0.229013786 triplet loss 0.694328547 l2_loss 24.0247059 fraction B 0.156329542 lossA 2.47787023 fraction A 0.0586794131\n",
      "step 4379 loss 1.02710319 fisher_loss 0.22866945 triplet loss 0.798433781 l2_loss 24.0298958 fraction B 0.190163225 lossA 2.42051816 fraction A 0.053414084\n",
      "step 4380 loss 0.816775918 fisher_loss 0.228049964 triplet loss 0.588725924 l2_loss 24.0341511 fraction B 0.10513214 lossA 2.35489321 fraction A 0.0473639518\n",
      "step 4381 loss 1.35778761 fisher_loss 0.227649763 triplet loss 1.1301378 l2_loss 24.0389538 fraction B 0.295525253 lossA 2.28995609 fraction A 0.0419968367\n",
      "step 4382 loss 1.08242548 fisher_loss 0.227163643 triplet loss 0.855261803 l2_loss 24.0435143 fraction B 0.228335217 lossA 2.28063035 fraction A 0.0412727073\n",
      "step 4383 loss 0.918328166 fisher_loss 0.22697404 triplet loss 0.691354156 l2_loss 24.0484829 fraction B 0.179112032 lossA 2.34493017 fraction A 0.0467049517\n",
      "step 4384 loss 1.01346409 fisher_loss 0.227229983 triplet loss 0.78623414 l2_loss 24.0545692 fraction B 0.152141139 lossA 2.4692533 fraction A 0.0567476936\n",
      "step 4385 loss 0.993258059 fisher_loss 0.227735341 triplet loss 0.765522718 l2_loss 24.0613213 fraction B 0.180840164 lossA 2.55661 fraction A 0.063402921\n",
      "step 4386 loss 1.04738665 fisher_loss 0.228117257 triplet loss 0.819269419 l2_loss 24.0682335 fraction B 0.247269824 lossA 2.53567553 fraction A 0.0618255325\n",
      "step 4387 loss 1.07510722 fisher_loss 0.227867991 triplet loss 0.847239196 l2_loss 24.0735664 fraction B 0.228291541 lossA 2.47541428 fraction A 0.0570965186\n",
      "step 4388 loss 1.03541768 fisher_loss 0.227307782 triplet loss 0.808109939 l2_loss 24.0780621 fraction B 0.159628883 lossA 2.37283444 fraction A 0.048874367\n",
      "step 4389 loss 0.970082879 fisher_loss 0.226522684 triplet loss 0.743560195 l2_loss 24.0818176 fraction B 0.176615641 lossA 2.28689432 fraction A 0.0416678973\n",
      "step 4390 loss 0.980956554 fisher_loss 0.226107866 triplet loss 0.754848659 l2_loss 24.0867195 fraction B 0.234379649 lossA 2.23306203 fraction A 0.0377762765\n",
      "step 4391 loss 0.866843641 fisher_loss 0.226003051 triplet loss 0.64084059 l2_loss 24.0920315 fraction B 0.150009796 lossA 2.3021841 fraction A 0.0424131453\n",
      "step 4392 loss 1.05729032 fisher_loss 0.226580471 triplet loss 0.830709875 l2_loss 24.0998459 fraction B 0.14658758 lossA 2.40152621 fraction A 0.0495265201\n",
      "step 4393 loss 1.08641183 fisher_loss 0.227541655 triplet loss 0.858870208 l2_loss 24.1075306 fraction B 0.244133145 lossA 2.48568106 fraction A 0.0543305166\n",
      "step 4394 loss 1.21821344 fisher_loss 0.228555977 triplet loss 0.989657462 l2_loss 24.1138916 fraction B 0.211172506 lossA 2.50509191 fraction A 0.0539763309\n",
      "step 4395 loss 1.21965051 fisher_loss 0.229322568 triplet loss 0.990327954 l2_loss 24.1187305 fraction B 0.237365589 lossA 2.50277328 fraction A 0.0526055209\n",
      "step 4396 loss 1.08186817 fisher_loss 0.229876742 triplet loss 0.851991415 l2_loss 24.1228676 fraction B 0.2077564 lossA 2.50375652 fraction A 0.0521204248\n",
      "step 4397 loss 1.19141459 fisher_loss 0.230433136 triplet loss 0.960981429 l2_loss 24.1285629 fraction B 0.173373565 lossA 2.48839068 fraction A 0.0501867123\n",
      "step 4398 loss 1.12748396 fisher_loss 0.230849683 triplet loss 0.896634281 l2_loss 24.1334515 fraction B 0.105361745 lossA 2.45416617 fraction A 0.0474467203\n",
      "step 4399 loss 1.22481561 fisher_loss 0.231165543 triplet loss 0.99365 l2_loss 24.1403408 fraction B 0.143293187 lossA 2.44850016 fraction A 0.0470552258\n",
      "step 4400 loss 1.14238393 fisher_loss 0.231418848 triplet loss 0.910965085 l2_loss 24.1480827 fraction B 0.20023717 lossA 2.43806458 fraction A 0.0462282263\n",
      "step 4401 loss 1.20763338 fisher_loss 0.231462583 triplet loss 0.976170778 l2_loss 24.1546154 fraction B 0.136255726 lossA 2.42326975 fraction A 0.0452112\n",
      "step 4402 loss 1.12409496 fisher_loss 0.231315523 triplet loss 0.89277941 l2_loss 24.1616116 fraction B 0.240619555 lossA 2.41746354 fraction A 0.0449054465\n",
      "step 4403 loss 1.31456089 fisher_loss 0.231037721 triplet loss 1.08352315 l2_loss 24.1685276 fraction B 0.245175 lossA 2.42060375 fraction A 0.0456101224\n",
      "step 4404 loss 1.04730344 fisher_loss 0.23062484 triplet loss 0.816678643 l2_loss 24.1746769 fraction B 0.198640868 lossA 2.43309164 fraction A 0.0475147106\n",
      "step 4405 loss 1.05393672 fisher_loss 0.23017785 triplet loss 0.823758841 l2_loss 24.1816406 fraction B 0.159458667 lossA 2.47657061 fraction A 0.0519831069\n",
      "step 4406 loss 0.991991401 fisher_loss 0.22980997 triplet loss 0.762181461 l2_loss 24.1886959 fraction B 0.144199714 lossA 2.52770281 fraction A 0.0564880855\n",
      "step 4407 loss 1.16671312 fisher_loss 0.229657426 triplet loss 0.937055707 l2_loss 24.1948795 fraction B 0.234294891 lossA 2.57530141 fraction A 0.06152061\n",
      "step 4408 loss 1.35555565 fisher_loss 0.229595378 triplet loss 1.12596023 l2_loss 24.2018 fraction B 0.199478894 lossA 2.51233816 fraction A 0.0565880947\n",
      "step 4409 loss 1.50183427 fisher_loss 0.228956819 triplet loss 1.27287745 l2_loss 24.2066116 fraction B 0.244481117 lossA 2.42137742 fraction A 0.0496522486\n",
      "step 4410 loss 1.22155035 fisher_loss 0.228070959 triplet loss 0.993479431 l2_loss 24.2097282 fraction B 0.161441475 lossA 2.34115291 fraction A 0.0439311638\n",
      "step 4411 loss 1.06093502 fisher_loss 0.227436051 triplet loss 0.833498955 l2_loss 24.2129364 fraction B 0.211548448 lossA 2.2710855 fraction A 0.0393919535\n",
      "step 4412 loss 1.0406208 fisher_loss 0.226981252 triplet loss 0.813639581 l2_loss 24.2173538 fraction B 0.178417444 lossA 2.22861958 fraction A 0.0370242223\n",
      "step 4413 loss 0.876352191 fisher_loss 0.226910025 triplet loss 0.649442196 l2_loss 24.2212219 fraction B 0.168548822 lossA 2.29152799 fraction A 0.0419344231\n",
      "step 4414 loss 0.97798413 fisher_loss 0.227306947 triplet loss 0.750677168 l2_loss 24.2283878 fraction B 0.138082743 lossA 2.4035542 fraction A 0.0520017222\n",
      "step 4415 loss 1.03127778 fisher_loss 0.228150219 triplet loss 0.803127527 l2_loss 24.2374668 fraction B 0.216915056 lossA 2.47824907 fraction A 0.058232896\n",
      "step 4416 loss 1.12868416 fisher_loss 0.228876874 triplet loss 0.899807274 l2_loss 24.2442207 fraction B 0.22284016 lossA 2.52661204 fraction A 0.0613767356\n",
      "step 4417 loss 1.22222292 fisher_loss 0.22933273 triplet loss 0.992890179 l2_loss 24.2491245 fraction B 0.148616269 lossA 2.51893353 fraction A 0.0594039038\n",
      "step 4418 loss 1.10253799 fisher_loss 0.229373723 triplet loss 0.873164296 l2_loss 24.2531013 fraction B 0.369992912 lossA 2.42603827 fraction A 0.0502371\n",
      "step 4419 loss 0.917205215 fisher_loss 0.228891432 triplet loss 0.688313782 l2_loss 24.255661 fraction B 0.141913861 lossA 2.36917257 fraction A 0.0451176\n",
      "step 4420 loss 1.13591158 fisher_loss 0.228870466 triplet loss 0.907041132 l2_loss 24.2597828 fraction B 0.253312379 lossA 2.30265546 fraction A 0.0396333076\n",
      "step 4421 loss 1.10400236 fisher_loss 0.228714854 triplet loss 0.875287533 l2_loss 24.2635708 fraction B 0.179204836 lossA 2.25330305 fraction A 0.0361763202\n",
      "step 4422 loss 1.02886796 fisher_loss 0.228710249 triplet loss 0.800157726 l2_loss 24.266737 fraction B 0.216638789 lossA 2.23022819 fraction A 0.0352447107\n",
      "step 4423 loss 1.23055363 fisher_loss 0.229122549 triplet loss 1.00143111 l2_loss 24.2706299 fraction B 0.167278036 lossA 2.28200054 fraction A 0.0383057073\n",
      "step 4424 loss 1.0246923 fisher_loss 0.229742169 triplet loss 0.794950128 l2_loss 24.2759 fraction B 0.119801894 lossA 2.37447286 fraction A 0.0447601564\n",
      "step 4425 loss 0.951644778 fisher_loss 0.230404347 triplet loss 0.721240461 l2_loss 24.2822113 fraction B 0.132091343 lossA 2.46322536 fraction A 0.0513383411\n",
      "step 4426 loss 1.17403293 fisher_loss 0.231058195 triplet loss 0.942974746 l2_loss 24.2890129 fraction B 0.133732751 lossA 2.49499559 fraction A 0.0541422293\n",
      "step 4427 loss 1.3446548 fisher_loss 0.231164277 triplet loss 1.11349046 l2_loss 24.295 fraction B 0.180363968 lossA 2.48375773 fraction A 0.0536310561\n",
      "step 4428 loss 1.18738174 fisher_loss 0.230801612 triplet loss 0.956580162 l2_loss 24.2993488 fraction B 0.174312487 lossA 2.47010183 fraction A 0.0523494259\n",
      "step 4429 loss 1.12753391 fisher_loss 0.230361402 triplet loss 0.897172511 l2_loss 24.3033543 fraction B 0.229666367 lossA 2.41938663 fraction A 0.048106119\n",
      "step 4430 loss 1.10449934 fisher_loss 0.229651704 triplet loss 0.874847591 l2_loss 24.3068638 fraction B 0.164886788 lossA 2.38461661 fraction A 0.0448915\n",
      "step 4431 loss 1.12549198 fisher_loss 0.228983134 triplet loss 0.896508873 l2_loss 24.3096027 fraction B 0.214473128 lossA 2.34439063 fraction A 0.0416587703\n",
      "step 4432 loss 1.0274241 fisher_loss 0.228235528 triplet loss 0.799188554 l2_loss 24.3120327 fraction B 0.191220835 lossA 2.31439424 fraction A 0.0391690396\n",
      "step 4433 loss 0.999543428 fisher_loss 0.22747834 triplet loss 0.772065103 l2_loss 24.3155899 fraction B 0.223129615 lossA 2.30314708 fraction A 0.0383862332\n",
      "step 4434 loss 1.00655699 fisher_loss 0.226950824 triplet loss 0.779606223 l2_loss 24.3204174 fraction B 0.168012589 lossA 2.34654045 fraction A 0.0429319628\n",
      "step 4435 loss 1.10928237 fisher_loss 0.22699149 triplet loss 0.8822909 l2_loss 24.3271351 fraction B 0.223904595 lossA 2.40059233 fraction A 0.0485773049\n",
      "step 4436 loss 1.22899413 fisher_loss 0.227007553 triplet loss 1.00198662 l2_loss 24.3328114 fraction B 0.187573239 lossA 2.45789528 fraction A 0.0537442155\n",
      "step 4437 loss 1.28283143 fisher_loss 0.226975486 triplet loss 1.05585599 l2_loss 24.3375244 fraction B 0.19025743 lossA 2.46053123 fraction A 0.0541537143\n",
      "step 4438 loss 1.31546 fisher_loss 0.226432174 triplet loss 1.08902776 l2_loss 24.3409653 fraction B 0.173631936 lossA 2.42996359 fraction A 0.0510993786\n",
      "step 4439 loss 0.9421646 fisher_loss 0.225800052 triplet loss 0.716364563 l2_loss 24.3435345 fraction B 0.17932862 lossA 2.35241365 fraction A 0.044945363\n",
      "step 4440 loss 1.20014846 fisher_loss 0.225109786 triplet loss 0.975038648 l2_loss 24.3452835 fraction B 0.186608925 lossA 2.28293276 fraction A 0.040109165\n",
      "step 4441 loss 0.938271642 fisher_loss 0.224482507 triplet loss 0.713789165 l2_loss 24.34692 fraction B 0.197139531 lossA 2.24755716 fraction A 0.0380552635\n",
      "step 4442 loss 1.12835765 fisher_loss 0.22417295 triplet loss 0.904184699 l2_loss 24.3508015 fraction B 0.181402266 lossA 2.23676181 fraction A 0.037638884\n",
      "step 4443 loss 1.36466086 fisher_loss 0.223889813 triplet loss 1.14077103 l2_loss 24.3550091 fraction B 0.330741048 lossA 2.25312209 fraction A 0.038871\n",
      "step 4444 loss 1.01585042 fisher_loss 0.223547369 triplet loss 0.792303 l2_loss 24.3580456 fraction B 0.19487673 lossA 2.33074355 fraction A 0.0458815023\n",
      "step 4445 loss 0.929825664 fisher_loss 0.223749071 triplet loss 0.706076622 l2_loss 24.3627262 fraction B 0.136218533 lossA 2.42651749 fraction A 0.0555675589\n",
      "step 4446 loss 1.00787544 fisher_loss 0.224402919 triplet loss 0.783472538 l2_loss 24.3698578 fraction B 0.136703283 lossA 2.46753812 fraction A 0.0590846352\n",
      "step 4447 loss 1.0459832 fisher_loss 0.224527568 triplet loss 0.821455657 l2_loss 24.3749809 fraction B 0.215849206 lossA 2.4620049 fraction A 0.0581950434\n",
      "step 4448 loss 1.06631231 fisher_loss 0.224251747 triplet loss 0.842060506 l2_loss 24.379467 fraction B 0.166292578 lossA 2.40538907 fraction A 0.0536122583\n",
      "step 4449 loss 1.62591529 fisher_loss 0.223661721 triplet loss 1.40225363 l2_loss 24.3837948 fraction B 0.245743379 lossA 2.3414402 fraction A 0.0482568033\n",
      "step 4450 loss 1.03550875 fisher_loss 0.222939163 triplet loss 0.812569618 l2_loss 24.3872662 fraction B 0.186225563 lossA 2.26859021 fraction A 0.0410325527\n",
      "step 4451 loss 0.858174801 fisher_loss 0.222380176 triplet loss 0.63579464 l2_loss 24.390089 fraction B 0.11587929 lossA 2.25817704 fraction A 0.040841043\n",
      "step 4452 loss 1.33534038 fisher_loss 0.222560689 triplet loss 1.11277974 l2_loss 24.3969269 fraction B 0.187257618 lossA 2.24276924 fraction A 0.0397757\n",
      "step 4453 loss 1.02019596 fisher_loss 0.222786099 triplet loss 0.797409832 l2_loss 24.4026814 fraction B 0.198664904 lossA 2.25215864 fraction A 0.0413026661\n",
      "step 4454 loss 1.06157386 fisher_loss 0.223080024 triplet loss 0.838493884 l2_loss 24.4091797 fraction B 0.17437838 lossA 2.29948187 fraction A 0.0466637686\n",
      "step 4455 loss 0.959561467 fisher_loss 0.223612592 triplet loss 0.735948861 l2_loss 24.4155025 fraction B 0.180732653 lossA 2.33364129 fraction A 0.0507838801\n",
      "step 4456 loss 1.2580775 fisher_loss 0.224170312 triplet loss 1.03390718 l2_loss 24.4219131 fraction B 0.272695869 lossA 2.3611443 fraction A 0.0530980378\n",
      "step 4457 loss 0.9239977 fisher_loss 0.224434376 triplet loss 0.699563324 l2_loss 24.4267159 fraction B 0.104554862 lossA 2.3619194 fraction A 0.0532351062\n",
      "step 4458 loss 1.06890428 fisher_loss 0.224583551 triplet loss 0.844320714 l2_loss 24.43153 fraction B 0.184618905 lossA 2.33093023 fraction A 0.0514392592\n",
      "step 4459 loss 0.994094431 fisher_loss 0.224531278 triplet loss 0.769563138 l2_loss 24.4365082 fraction B 0.165369898 lossA 2.28411222 fraction A 0.048333995\n",
      "step 4460 loss 1.48331022 fisher_loss 0.224321887 triplet loss 1.25898838 l2_loss 24.4413929 fraction B 0.183055937 lossA 2.21328974 fraction A 0.0424731635\n",
      "step 4461 loss 1.22779465 fisher_loss 0.223824531 triplet loss 1.00397015 l2_loss 24.4455643 fraction B 0.150516957 lossA 2.15636611 fraction A 0.0378260463\n",
      "step 4462 loss 0.74303633 fisher_loss 0.223522887 triplet loss 0.519513428 l2_loss 24.4494648 fraction B 0.115493841 lossA 2.15619302 fraction A 0.0374086909\n",
      "step 4463 loss 1.00597465 fisher_loss 0.223611221 triplet loss 0.782363474 l2_loss 24.4546051 fraction B 0.162346333 lossA 2.15922117 fraction A 0.0365376808\n",
      "step 4464 loss 0.997976303 fisher_loss 0.223668963 triplet loss 0.774307311 l2_loss 24.4586964 fraction B 0.201360404 lossA 2.20442724 fraction A 0.0394844301\n",
      "step 4465 loss 1.11768579 fisher_loss 0.223952562 triplet loss 0.893733263 l2_loss 24.462904 fraction B 0.374609023 lossA 2.23063302 fraction A 0.0410453044\n",
      "step 4466 loss 1.02434623 fisher_loss 0.224177152 triplet loss 0.80016911 l2_loss 24.4659 fraction B 0.203669503 lossA 2.26258779 fraction A 0.0433817394\n",
      "step 4467 loss 1.14280283 fisher_loss 0.224471167 triplet loss 0.918331623 l2_loss 24.4693356 fraction B 0.249659628 lossA 2.28597641 fraction A 0.0449737273\n",
      "step 4468 loss 0.995190203 fisher_loss 0.224669755 triplet loss 0.770520449 l2_loss 24.4724884 fraction B 0.171005234 lossA 2.28472185 fraction A 0.0439248234\n",
      "step 4469 loss 0.834128499 fisher_loss 0.224978209 triplet loss 0.60915029 l2_loss 24.476265 fraction B 0.135305226 lossA 2.2697556 fraction A 0.0423035696\n",
      "step 4470 loss 1.28985178 fisher_loss 0.225254446 triplet loss 1.06459737 l2_loss 24.4814358 fraction B 0.191285849 lossA 2.26009083 fraction A 0.0414212942\n",
      "step 4471 loss 1.07927978 fisher_loss 0.225633 triplet loss 0.853646815 l2_loss 24.4871197 fraction B 0.203923285 lossA 2.29252172 fraction A 0.043438863\n",
      "step 4472 loss 0.976401448 fisher_loss 0.226373732 triplet loss 0.750027716 l2_loss 24.4937744 fraction B 0.133339 lossA 2.35264015 fraction A 0.0479683653\n",
      "step 4473 loss 1.02177322 fisher_loss 0.227152184 triplet loss 0.79462105 l2_loss 24.5010529 fraction B 0.14197582 lossA 2.45362663 fraction A 0.0575338341\n",
      "step 4474 loss 1.23046327 fisher_loss 0.228131518 triplet loss 1.00233173 l2_loss 24.5105267 fraction B 0.266456217 lossA 2.50926781 fraction A 0.0631415397\n",
      "step 4475 loss 1.12113166 fisher_loss 0.228637636 triplet loss 0.892494 l2_loss 24.5179729 fraction B 0.128865972 lossA 2.49921703 fraction A 0.0630268231\n",
      "step 4476 loss 1.32545209 fisher_loss 0.228688762 triplet loss 1.09676337 l2_loss 24.5246525 fraction B 0.16960445 lossA 2.49588251 fraction A 0.0627830774\n",
      "step 4477 loss 1.17204571 fisher_loss 0.22869201 triplet loss 0.943353713 l2_loss 24.530201 fraction B 0.232874975 lossA 2.50092292 fraction A 0.0634438545\n",
      "step 4478 loss 0.897052109 fisher_loss 0.228662729 triplet loss 0.66838938 l2_loss 24.535099 fraction B 0.159329757 lossA 2.42250061 fraction A 0.0567670725\n",
      "step 4479 loss 1.12140775 fisher_loss 0.22804293 triplet loss 0.893364787 l2_loss 24.5383759 fraction B 0.181242988 lossA 2.3456161 fraction A 0.0484395139\n",
      "step 4480 loss 0.883710384 fisher_loss 0.227112651 triplet loss 0.656597733 l2_loss 24.5400867 fraction B 0.16342923 lossA 2.24959159 fraction A 0.0393897817\n",
      "step 4481 loss 1.21849 fisher_loss 0.226122856 triplet loss 0.992367148 l2_loss 24.5410748 fraction B 0.245052412 lossA 2.19260216 fraction A 0.0361420549\n",
      "step 4482 loss 1.34579778 fisher_loss 0.225640401 triplet loss 1.12015736 l2_loss 24.5433121 fraction B 0.241943061 lossA 2.15560842 fraction A 0.0344522297\n",
      "step 4483 loss 1.03975976 fisher_loss 0.225261211 triplet loss 0.814498544 l2_loss 24.5457115 fraction B 0.219556272 lossA 2.25444245 fraction A 0.039746359\n",
      "step 4484 loss 1.26380861 fisher_loss 0.22546792 triplet loss 1.03834069 l2_loss 24.551527 fraction B 0.196696699 lossA 2.31420612 fraction A 0.0443665609\n",
      "step 4485 loss 1.03916311 fisher_loss 0.225551724 triplet loss 0.813611448 l2_loss 24.5565109 fraction B 0.160821974 lossA 2.3764534 fraction A 0.0505775921\n",
      "step 4486 loss 1.25281298 fisher_loss 0.225692064 triplet loss 1.02712095 l2_loss 24.562397 fraction B 0.210955709 lossA 2.45054102 fraction A 0.0576496\n",
      "step 4487 loss 1.03574085 fisher_loss 0.22587797 triplet loss 0.809862912 l2_loss 24.567524 fraction B 0.146980241 lossA 2.48711634 fraction A 0.061062973\n",
      "step 4488 loss 1.15298903 fisher_loss 0.225880876 triplet loss 0.927108169 l2_loss 24.5720234 fraction B 0.190744191 lossA 2.48324656 fraction A 0.0615164079\n",
      "step 4489 loss 1.10233486 fisher_loss 0.225603193 triplet loss 0.876731634 l2_loss 24.5757866 fraction B 0.190086484 lossA 2.45346451 fraction A 0.0596508682\n",
      "step 4490 loss 1.15488183 fisher_loss 0.225030228 triplet loss 0.929851651 l2_loss 24.5784283 fraction B 0.183317721 lossA 2.41843891 fraction A 0.0575922243\n",
      "step 4491 loss 1.16595149 fisher_loss 0.224447787 triplet loss 0.941503644 l2_loss 24.5819912 fraction B 0.131588638 lossA 2.39088917 fraction A 0.0562293455\n",
      "step 4492 loss 1.06579816 fisher_loss 0.224028409 triplet loss 0.841769755 l2_loss 24.5854359 fraction B 0.165491536 lossA 2.37406039 fraction A 0.0556812249\n",
      "step 4493 loss 1.22314513 fisher_loss 0.223771393 triplet loss 0.999373734 l2_loss 24.5892792 fraction B 0.225905806 lossA 2.34730506 fraction A 0.053534884\n",
      "step 4494 loss 1.07407248 fisher_loss 0.223304376 triplet loss 0.850768089 l2_loss 24.5921154 fraction B 0.202282518 lossA 2.33600616 fraction A 0.0521001033\n",
      "step 4495 loss 1.2504698 fisher_loss 0.222990602 triplet loss 1.02747917 l2_loss 24.5945663 fraction B 0.319700718 lossA 2.32827735 fraction A 0.05055191\n",
      "step 4496 loss 0.973763108 fisher_loss 0.222692743 triplet loss 0.75107038 l2_loss 24.596489 fraction B 0.140906543 lossA 2.31033015 fraction A 0.0469158776\n",
      "step 4497 loss 1.07825959 fisher_loss 0.222322 triplet loss 0.8559376 l2_loss 24.5982151 fraction B 0.18200393 lossA 2.32005358 fraction A 0.0458603464\n",
      "step 4498 loss 0.960049033 fisher_loss 0.222224802 triplet loss 0.737824261 l2_loss 24.6016045 fraction B 0.197172314 lossA 2.3131783 fraction A 0.0436845236\n",
      "step 4499 loss 1.11179256 fisher_loss 0.222116455 triplet loss 0.889676154 l2_loss 24.6051407 fraction B 0.216636166 lossA 2.30625796 fraction A 0.0421174206\n",
      "step 4500 loss 1.15181828 fisher_loss 0.222173572 triplet loss 0.929644763 l2_loss 24.609066 fraction B 0.253582686 lossA 2.30102491 fraction A 0.0403993428\n",
      "step 4501 loss 1.17921579 fisher_loss 0.22217916 triplet loss 0.957036674 l2_loss 24.6112957 fraction B 0.231186152 lossA 2.26857972 fraction A 0.037439879\n",
      "step 4502 loss 1.20748067 fisher_loss 0.222196668 triplet loss 0.985284 l2_loss 24.6132221 fraction B 0.235791892 lossA 2.33588886 fraction A 0.0418774448\n",
      "step 4503 loss 0.999308646 fisher_loss 0.222675905 triplet loss 0.776632726 l2_loss 24.6170692 fraction B 0.159002766 lossA 2.46914172 fraction A 0.051860854\n",
      "step 4504 loss 1.3330425 fisher_loss 0.223879457 triplet loss 1.10916305 l2_loss 24.6230125 fraction B 0.223363802 lossA 2.5783565 fraction A 0.0599310137\n",
      "step 4505 loss 1.13604665 fisher_loss 0.224886954 triplet loss 0.911159694 l2_loss 24.628109 fraction B 0.238604978 lossA 2.63713908 fraction A 0.0641939938\n",
      "step 4506 loss 1.01022077 fisher_loss 0.225588799 triplet loss 0.784632 l2_loss 24.6329784 fraction B 0.0840419 lossA 2.63908648 fraction A 0.0634892061\n",
      "step 4507 loss 1.21056402 fisher_loss 0.226100132 triplet loss 0.98446393 l2_loss 24.6366405 fraction B 0.239099503 lossA 2.59743619 fraction A 0.0594519712\n",
      "step 4508 loss 0.858661234 fisher_loss 0.226121947 triplet loss 0.632539272 l2_loss 24.6392899 fraction B 0.115891218 lossA 2.45382595 fraction A 0.0486785471\n",
      "step 4509 loss 1.15813446 fisher_loss 0.225506157 triplet loss 0.932628334 l2_loss 24.6415539 fraction B 0.141668037 lossA 2.30086589 fraction A 0.0387044474\n",
      "step 4510 loss 1.09573257 fisher_loss 0.22460641 triplet loss 0.871126115 l2_loss 24.6440239 fraction B 0.1397845 lossA 2.12428284 fraction A 0.0306731891\n",
      "step 4511 loss 1.13888025 fisher_loss 0.224045485 triplet loss 0.914834738 l2_loss 24.6466465 fraction B 0.245972067 lossA 2.03071046 fraction A 0.0280047\n",
      "step 4512 loss 1.13685048 fisher_loss 0.223704264 triplet loss 0.913146257 l2_loss 24.6503353 fraction B 0.254369348 lossA 2.03675508 fraction A 0.0282859709\n",
      "step 4513 loss 1.03605092 fisher_loss 0.223536566 triplet loss 0.812514365 l2_loss 24.6548805 fraction B 0.166625455 lossA 2.20714498 fraction A 0.0348689742\n",
      "step 4514 loss 1.030707 fisher_loss 0.223715335 triplet loss 0.806991637 l2_loss 24.6596317 fraction B 0.150675207 lossA 2.41416693 fraction A 0.0500573255\n",
      "step 4515 loss 0.973177493 fisher_loss 0.224953249 triplet loss 0.748224258 l2_loss 24.6656113 fraction B 0.207134962 lossA 2.55205703 fraction A 0.0605200306\n",
      "step 4516 loss 1.06748343 fisher_loss 0.226276189 triplet loss 0.841207266 l2_loss 24.6712017 fraction B 0.173291475 lossA 2.56984377 fraction A 0.0612636097\n",
      "step 4517 loss 1.49342179 fisher_loss 0.226658583 triplet loss 1.26676321 l2_loss 24.6752243 fraction B 0.281236738 lossA 2.57302904 fraction A 0.0610665232\n",
      "step 4518 loss 0.961740375 fisher_loss 0.226939 triplet loss 0.734801352 l2_loss 24.6785507 fraction B 0.157525018 lossA 2.47723055 fraction A 0.0539430417\n",
      "step 4519 loss 1.24514699 fisher_loss 0.226390183 triplet loss 1.01875687 l2_loss 24.6814175 fraction B 0.219603837 lossA 2.40437555 fraction A 0.0486708321\n",
      "step 4520 loss 1.03330135 fisher_loss 0.225659028 triplet loss 0.807642281 l2_loss 24.6838722 fraction B 0.209177762 lossA 2.36206198 fraction A 0.0458068475\n",
      "step 4521 loss 1.0766151 fisher_loss 0.225272238 triplet loss 0.851342857 l2_loss 24.6873951 fraction B 0.127995238 lossA 2.35062242 fraction A 0.0450715274\n",
      "step 4522 loss 1.12865961 fisher_loss 0.225335196 triplet loss 0.903324366 l2_loss 24.6913719 fraction B 0.228125647 lossA 2.35695791 fraction A 0.0460838042\n",
      "step 4523 loss 0.934703 fisher_loss 0.225598 triplet loss 0.709105 l2_loss 24.6959953 fraction B 0.19760491 lossA 2.38574743 fraction A 0.0494584218\n",
      "step 4524 loss 1.07837808 fisher_loss 0.226086929 triplet loss 0.852291107 l2_loss 24.7023125 fraction B 0.186869115 lossA 2.42765832 fraction A 0.0533083491\n",
      "step 4525 loss 0.861734688 fisher_loss 0.226634145 triplet loss 0.635100543 l2_loss 24.7073078 fraction B 0.122949801 lossA 2.44649911 fraction A 0.0548771881\n",
      "step 4526 loss 1.16782832 fisher_loss 0.227322593 triplet loss 0.940505743 l2_loss 24.7132092 fraction B 0.150755033 lossA 2.48178411 fraction A 0.0571980849\n",
      "step 4527 loss 1.26255536 fisher_loss 0.227867946 triplet loss 1.0346874 l2_loss 24.7193851 fraction B 0.284682512 lossA 2.48220396 fraction A 0.0573584065\n",
      "step 4528 loss 0.980440915 fisher_loss 0.22820352 triplet loss 0.75223738 l2_loss 24.7253284 fraction B 0.142103299 lossA 2.60669184 fraction A 0.0677947402\n",
      "step 4529 loss 0.943822801 fisher_loss 0.229520142 triplet loss 0.714302659 l2_loss 24.7339954 fraction B 0.140706316 lossA 2.65799236 fraction A 0.0722047091\n",
      "step 4530 loss 1.05448985 fisher_loss 0.230514601 triplet loss 0.823975265 l2_loss 24.7420177 fraction B 0.129456475 lossA 2.62878728 fraction A 0.0708699\n",
      "step 4531 loss 1.13840342 fisher_loss 0.230547383 triplet loss 0.907856047 l2_loss 24.7487831 fraction B 0.210361272 lossA 2.55849504 fraction A 0.0663123652\n",
      "step 4532 loss 0.969220281 fisher_loss 0.230056718 triplet loss 0.739163578 l2_loss 24.7545166 fraction B 0.169655085 lossA 2.47334385 fraction A 0.0607458428\n",
      "step 4533 loss 1.1481992 fisher_loss 0.229496375 triplet loss 0.918702841 l2_loss 24.7607193 fraction B 0.139480948 lossA 2.32489753 fraction A 0.0485735722\n",
      "step 4534 loss 1.01575983 fisher_loss 0.228434309 triplet loss 0.787325561 l2_loss 24.7663403 fraction B 0.143298492 lossA 2.25707626 fraction A 0.0418155789\n",
      "step 4535 loss 1.17797625 fisher_loss 0.227930158 triplet loss 0.950046122 l2_loss 24.7735424 fraction B 0.282958895 lossA 2.2375772 fraction A 0.0397740677\n",
      "step 4536 loss 1.29698277 fisher_loss 0.227688923 triplet loss 1.06929386 l2_loss 24.7802982 fraction B 0.304549754 lossA 2.23646569 fraction A 0.0394421592\n",
      "step 4537 loss 1.13750243 fisher_loss 0.227421224 triplet loss 0.910081208 l2_loss 24.7857189 fraction B 0.202674642 lossA 2.27801371 fraction A 0.0446000881\n",
      "step 4538 loss 1.17045367 fisher_loss 0.227691591 triplet loss 0.942762077 l2_loss 24.7923164 fraction B 0.176762402 lossA 2.35600805 fraction A 0.0522150025\n",
      "step 4539 loss 0.938929558 fisher_loss 0.228139803 triplet loss 0.71078974 l2_loss 24.7979317 fraction B 0.128230423 lossA 2.48827386 fraction A 0.062848568\n",
      "step 4540 loss 1.14470196 fisher_loss 0.229079738 triplet loss 0.915622175 l2_loss 24.8044682 fraction B 0.176890865 lossA 2.55495596 fraction A 0.0673019364\n",
      "step 4541 loss 1.130615 fisher_loss 0.229816914 triplet loss 0.900798082 l2_loss 24.8096466 fraction B 0.283928216 lossA 2.54422617 fraction A 0.0654089376\n",
      "step 4542 loss 1.10633421 fisher_loss 0.230112121 triplet loss 0.876222134 l2_loss 24.8139095 fraction B 0.144720346 lossA 2.48586035 fraction A 0.0594626181\n",
      "step 4543 loss 1.2258122 fisher_loss 0.229784817 triplet loss 0.99602741 l2_loss 24.8173809 fraction B 0.23133634 lossA 2.36936045 fraction A 0.04796822\n",
      "step 4544 loss 1.0805583 fisher_loss 0.228662461 triplet loss 0.851895809 l2_loss 24.8191605 fraction B 0.189535618 lossA 2.25552583 fraction A 0.038752079\n",
      "step 4545 loss 0.90246135 fisher_loss 0.227385983 triplet loss 0.675075352 l2_loss 24.821188 fraction B 0.188856184 lossA 2.12667394 fraction A 0.0324681327\n",
      "step 4546 loss 1.33925259 fisher_loss 0.226194695 triplet loss 1.11305785 l2_loss 24.8248615 fraction B 0.251762182 lossA 2.05907559 fraction A 0.030094713\n",
      "step 4547 loss 1.2600224 fisher_loss 0.225058734 triplet loss 1.03496373 l2_loss 24.8286514 fraction B 0.295572847 lossA 2.13279486 fraction A 0.0324556343\n",
      "step 4548 loss 1.11167097 fisher_loss 0.224414527 triplet loss 0.887256503 l2_loss 24.8327217 fraction B 0.233063772 lossA 2.26495647 fraction A 0.0389081277\n",
      "step 4549 loss 0.889435291 fisher_loss 0.224207133 triplet loss 0.665228128 l2_loss 24.8371067 fraction B 0.158318505 lossA 2.41758585 fraction A 0.0510967\n",
      "step 4550 loss 1.10943532 fisher_loss 0.224863023 triplet loss 0.884572327 l2_loss 24.8428822 fraction B 0.179952294 lossA 2.5152 fraction A 0.0589250736\n",
      "step 4551 loss 1.017946 fisher_loss 0.225437298 triplet loss 0.792508721 l2_loss 24.8471737 fraction B 0.184541881 lossA 2.54666448 fraction A 0.0613715537\n",
      "step 4552 loss 0.93578732 fisher_loss 0.225575536 triplet loss 0.710211813 l2_loss 24.8516979 fraction B 0.12774536 lossA 2.5438211 fraction A 0.060807351\n",
      "step 4553 loss 0.994952261 fisher_loss 0.225484714 triplet loss 0.769467533 l2_loss 24.8562279 fraction B 0.196119189 lossA 2.47917986 fraction A 0.0550505519\n",
      "step 4554 loss 1.03268 fisher_loss 0.224896476 triplet loss 0.807783544 l2_loss 24.8604393 fraction B 0.168185443 lossA 2.38808537 fraction A 0.0471138358\n",
      "step 4555 loss 1.05859756 fisher_loss 0.224074662 triplet loss 0.834522963 l2_loss 24.8637142 fraction B 0.224085629 lossA 2.29489541 fraction A 0.0401535369\n",
      "step 4556 loss 0.909639359 fisher_loss 0.223510727 triplet loss 0.686128616 l2_loss 24.8673439 fraction B 0.158397913 lossA 2.24932623 fraction A 0.0375726707\n",
      "step 4557 loss 1.08401096 fisher_loss 0.223495767 triplet loss 0.860515237 l2_loss 24.8731785 fraction B 0.171138659 lossA 2.26301694 fraction A 0.0384142324\n",
      "step 4558 loss 1.20589805 fisher_loss 0.223729029 triplet loss 0.982169032 l2_loss 24.8805046 fraction B 0.305761755 lossA 2.28032112 fraction A 0.0395046063\n",
      "step 4559 loss 1.05554152 fisher_loss 0.22371842 triplet loss 0.831823051 l2_loss 24.887104 fraction B 0.0916634649 lossA 2.35568666 fraction A 0.0456282683\n",
      "step 4560 loss 1.062356 fisher_loss 0.224205583 triplet loss 0.838150382 l2_loss 24.8960094 fraction B 0.20995456 lossA 2.4196763 fraction A 0.0515721589\n",
      "step 4561 loss 1.16499817 fisher_loss 0.224949226 triplet loss 0.940049 l2_loss 24.9041958 fraction B 0.202671692 lossA 2.44397211 fraction A 0.0541076399\n",
      "step 4562 loss 1.16824162 fisher_loss 0.225680873 triplet loss 0.942560792 l2_loss 24.9118633 fraction B 0.209799737 lossA 2.44486833 fraction A 0.0548058301\n",
      "step 4563 loss 1.04341018 fisher_loss 0.226227388 triplet loss 0.817182839 l2_loss 24.9187012 fraction B 0.276919872 lossA 2.40011621 fraction A 0.0509799533\n",
      "step 4564 loss 0.995318711 fisher_loss 0.226303101 triplet loss 0.76901561 l2_loss 24.9244919 fraction B 0.226019487 lossA 2.30653644 fraction A 0.043494463\n",
      "step 4565 loss 1.05514669 fisher_loss 0.225864038 triplet loss 0.829282641 l2_loss 24.9296703 fraction B 0.208152398 lossA 2.27296 fraction A 0.0414916\n",
      "step 4566 loss 0.886580527 fisher_loss 0.225824788 triplet loss 0.660755754 l2_loss 24.9351883 fraction B 0.131383196 lossA 2.28008342 fraction A 0.0435401388\n",
      "step 4567 loss 1.15135658 fisher_loss 0.225996867 triplet loss 0.925359726 l2_loss 24.942028 fraction B 0.207113147 lossA 2.33567429 fraction A 0.0513346456\n",
      "step 4568 loss 1.23964477 fisher_loss 0.226679415 triplet loss 1.01296532 l2_loss 24.949976 fraction B 0.222963154 lossA 2.37680197 fraction A 0.0571851172\n",
      "step 4569 loss 1.0823698 fisher_loss 0.227057621 triplet loss 0.855312169 l2_loss 24.9555244 fraction B 0.190085262 lossA 2.35853124 fraction A 0.0568508543\n",
      "step 4570 loss 1.09916973 fisher_loss 0.226805508 triplet loss 0.872364223 l2_loss 24.9596138 fraction B 0.234075889 lossA 2.2960279 fraction A 0.0513713397\n",
      "step 4571 loss 1.1435442 fisher_loss 0.226091564 triplet loss 0.917452574 l2_loss 24.9622 fraction B 0.215687871 lossA 2.23303962 fraction A 0.0445055105\n",
      "step 4572 loss 0.967961311 fisher_loss 0.225317225 triplet loss 0.742644072 l2_loss 24.9643764 fraction B 0.176782399 lossA 2.1975832 fraction A 0.0416777879\n",
      "step 4573 loss 1.00831163 fisher_loss 0.225038171 triplet loss 0.783273458 l2_loss 24.9681911 fraction B 0.195705622 lossA 2.17716527 fraction A 0.0395693\n",
      "step 4574 loss 1.00669849 fisher_loss 0.224651903 triplet loss 0.782046616 l2_loss 24.9722385 fraction B 0.149266526 lossA 2.1904006 fraction A 0.0399532616\n",
      "step 4575 loss 0.964428902 fisher_loss 0.224306419 triplet loss 0.740122497 l2_loss 24.9769096 fraction B 0.186398253 lossA 2.21357965 fraction A 0.0407221615\n",
      "step 4576 loss 1.03148508 fisher_loss 0.224172384 triplet loss 0.807312667 l2_loss 24.9815559 fraction B 0.162846476 lossA 2.23892713 fraction A 0.0418048576\n",
      "step 4577 loss 1.05978489 fisher_loss 0.224092752 triplet loss 0.835692108 l2_loss 24.9866848 fraction B 0.207024306 lossA 2.22736764 fraction A 0.0398898721\n",
      "step 4578 loss 1.07118058 fisher_loss 0.223875463 triplet loss 0.847305059 l2_loss 24.9913197 fraction B 0.216973171 lossA 2.18073416 fraction A 0.0358471647\n",
      "step 4579 loss 1.10369551 fisher_loss 0.223588586 triplet loss 0.880106926 l2_loss 24.9954758 fraction B 0.233995765 lossA 2.15643716 fraction A 0.0338957421\n",
      "step 4580 loss 1.08826 fisher_loss 0.223367 triplet loss 0.864893 l2_loss 24.9992352 fraction B 0.293077499 lossA 2.22458696 fraction A 0.036808189\n",
      "step 4581 loss 1.07031059 fisher_loss 0.223542735 triplet loss 0.846767902 l2_loss 25.0046597 fraction B 0.158231 lossA 2.34527135 fraction A 0.0444926508\n",
      "step 4582 loss 1.15059507 fisher_loss 0.224086553 triplet loss 0.926508546 l2_loss 25.0118542 fraction B 0.179182261 lossA 2.4110713 fraction A 0.0487999655\n",
      "step 4583 loss 1.28455234 fisher_loss 0.224627808 triplet loss 1.05992448 l2_loss 25.0185566 fraction B 0.142320871 lossA 2.45969224 fraction A 0.0523948483\n",
      "step 4584 loss 1.00960219 fisher_loss 0.225115985 triplet loss 0.784486234 l2_loss 25.0256863 fraction B 0.168756381 lossA 2.47883439 fraction A 0.0543452948\n",
      "step 4585 loss 0.887255847 fisher_loss 0.225561395 triplet loss 0.661694467 l2_loss 25.0336323 fraction B 0.190823436 lossA 2.43441057 fraction A 0.0513635166\n",
      "step 4586 loss 0.855201 fisher_loss 0.225735888 triplet loss 0.629465103 l2_loss 25.0416946 fraction B 0.184726611 lossA 2.29727697 fraction A 0.0413427278\n",
      "step 4587 loss 0.907788 fisher_loss 0.225448266 triplet loss 0.682339728 l2_loss 25.0487251 fraction B 0.138389766 lossA 2.19765711 fraction A 0.0353789516\n",
      "step 4588 loss 1.12559223 fisher_loss 0.225392908 triplet loss 0.900199354 l2_loss 25.0576935 fraction B 0.188167915 lossA 2.17503905 fraction A 0.0347965285\n",
      "step 4589 loss 1.06672716 fisher_loss 0.225696534 triplet loss 0.841030657 l2_loss 25.0665226 fraction B 0.22670266 lossA 2.22946239 fraction A 0.0387939885\n",
      "step 4590 loss 1.17260981 fisher_loss 0.226281911 triplet loss 0.946327865 l2_loss 25.0752678 fraction B 0.230015248 lossA 2.27796459 fraction A 0.0434656292\n",
      "step 4591 loss 1.27541089 fisher_loss 0.226984113 triplet loss 1.04842675 l2_loss 25.0831814 fraction B 0.231672928 lossA 2.31679916 fraction A 0.0471166223\n",
      "step 4592 loss 1.06209862 fisher_loss 0.22760959 triplet loss 0.834489048 l2_loss 25.0902576 fraction B 0.171333849 lossA 2.30945516 fraction A 0.0468133315\n",
      "step 4593 loss 1.09421241 fisher_loss 0.227997974 triplet loss 0.866214454 l2_loss 25.0965519 fraction B 0.207181975 lossA 2.27773333 fraction A 0.0445727408\n",
      "step 4594 loss 0.979878664 fisher_loss 0.22788918 triplet loss 0.751989484 l2_loss 25.1021786 fraction B 0.15119195 lossA 2.3182 fraction A 0.0482519157\n",
      "step 4595 loss 1.32293391 fisher_loss 0.228173271 triplet loss 1.09476066 l2_loss 25.1092033 fraction B 0.212542951 lossA 2.34893203 fraction A 0.0505335107\n",
      "step 4596 loss 1.04724288 fisher_loss 0.228130043 triplet loss 0.819112837 l2_loss 25.115139 fraction B 0.170883015 lossA 2.37330317 fraction A 0.0519996211\n",
      "step 4597 loss 1.09079623 fisher_loss 0.228133455 triplet loss 0.862662792 l2_loss 25.1210556 fraction B 0.164230198 lossA 2.38938689 fraction A 0.0527456664\n",
      "step 4598 loss 1.10800469 fisher_loss 0.228346691 triplet loss 0.879658043 l2_loss 25.1268539 fraction B 0.26752308 lossA 2.41566205 fraction A 0.0544949658\n",
      "step 4599 loss 1.17202377 fisher_loss 0.22868751 triplet loss 0.943336248 l2_loss 25.132719 fraction B 0.312331527 lossA 2.43358588 fraction A 0.0554203466\n",
      "step 4600 loss 0.934830546 fisher_loss 0.228892416 triplet loss 0.70593816 l2_loss 25.1369457 fraction B 0.151913062 lossA 2.47149014 fraction A 0.0582027584\n",
      "step 4601 loss 1.45035958 fisher_loss 0.229489028 triplet loss 1.22087061 l2_loss 25.1429062 fraction B 0.196112782 lossA 2.47184134 fraction A 0.058149837\n",
      "step 4602 loss 1.78444874 fisher_loss 0.229686067 triplet loss 1.55476272 l2_loss 25.1478176 fraction B 0.189890131 lossA 2.44731808 fraction A 0.0558004342\n",
      "step 4603 loss 1.14409244 fisher_loss 0.22945708 triplet loss 0.91463536 l2_loss 25.1504326 fraction B 0.16685912 lossA 2.44224763 fraction A 0.0551633835\n",
      "step 4604 loss 0.908010244 fisher_loss 0.229323059 triplet loss 0.678687155 l2_loss 25.1536617 fraction B 0.151237652 lossA 2.42472076 fraction A 0.0531281047\n",
      "step 4605 loss 1.04102218 fisher_loss 0.229087472 triplet loss 0.81193471 l2_loss 25.1572 fraction B 0.207118019 lossA 2.42337823 fraction A 0.0529035963\n",
      "step 4606 loss 1.11253965 fisher_loss 0.228759497 triplet loss 0.883780181 l2_loss 25.1610641 fraction B 0.221066147 lossA 2.39658 fraction A 0.05083289\n",
      "step 4607 loss 0.958421707 fisher_loss 0.228365451 triplet loss 0.730056226 l2_loss 25.1644917 fraction B 0.0772211254 lossA 2.39734101 fraction A 0.0510687344\n",
      "step 4608 loss 0.910958886 fisher_loss 0.228562027 triplet loss 0.682396829 l2_loss 25.1689682 fraction B 0.121653229 lossA 2.41902184 fraction A 0.0532100052\n",
      "step 4609 loss 1.03344703 fisher_loss 0.228971377 triplet loss 0.804475605 l2_loss 25.1747646 fraction B 0.1750049 lossA 2.43961954 fraction A 0.0551569387\n",
      "step 4610 loss 1.09980607 fisher_loss 0.229420587 triplet loss 0.870385468 l2_loss 25.1809959 fraction B 0.157337487 lossA 2.4509213 fraction A 0.0554123782\n",
      "step 4611 loss 1.36182487 fisher_loss 0.229689986 triplet loss 1.13213491 l2_loss 25.1862392 fraction B 0.233416453 lossA 2.4458456 fraction A 0.0540159941\n",
      "step 4612 loss 1.1694212 fisher_loss 0.229799137 triplet loss 0.939622104 l2_loss 25.1899529 fraction B 0.136991262 lossA 2.46561456 fraction A 0.054384958\n",
      "step 4613 loss 1.53862464 fisher_loss 0.229964659 triplet loss 1.30866 l2_loss 25.1934681 fraction B 0.16594626 lossA 2.42759109 fraction A 0.0512999818\n",
      "step 4614 loss 1.09897351 fisher_loss 0.229568779 triplet loss 0.869404733 l2_loss 25.1961136 fraction B 0.25702104 lossA 2.39796805 fraction A 0.0489008427\n",
      "step 4615 loss 1.03521502 fisher_loss 0.229042143 triplet loss 0.806172907 l2_loss 25.1993217 fraction B 0.18580763 lossA 2.34432507 fraction A 0.0444465764\n",
      "step 4616 loss 1.07393527 fisher_loss 0.228019238 triplet loss 0.845916092 l2_loss 25.2019825 fraction B 0.174221635 lossA 2.31250238 fraction A 0.0417656638\n",
      "step 4617 loss 0.912844598 fisher_loss 0.226997614 triplet loss 0.685847 l2_loss 25.2040062 fraction B 0.167835891 lossA 2.36516333 fraction A 0.0456341393\n",
      "step 4618 loss 1.11278725 fisher_loss 0.226574779 triplet loss 0.886212528 l2_loss 25.2077141 fraction B 0.170616612 lossA 2.51028848 fraction A 0.056464795\n",
      "step 4619 loss 0.915896118 fisher_loss 0.226747498 triplet loss 0.689148605 l2_loss 25.2119904 fraction B 0.146620229 lossA 2.65086579 fraction A 0.0670865551\n",
      "step 4620 loss 1.01220512 fisher_loss 0.227257773 triplet loss 0.784947395 l2_loss 25.2162628 fraction B 0.200901344 lossA 2.69000459 fraction A 0.0690961182\n",
      "step 4621 loss 1.24464345 fisher_loss 0.226996973 triplet loss 1.01764643 l2_loss 25.2191086 fraction B 0.268685281 lossA 2.66443801 fraction A 0.0662702397\n",
      "step 4622 loss 1.09948683 fisher_loss 0.226364359 triplet loss 0.873122513 l2_loss 25.2205448 fraction B 0.230121568 lossA 2.54205728 fraction A 0.0561364703\n",
      "step 4623 loss 1.18027747 fisher_loss 0.225257561 triplet loss 0.955019891 l2_loss 25.2203636 fraction B 0.180633113 lossA 2.44341946 fraction A 0.0487692468\n",
      "step 4624 loss 0.980726957 fisher_loss 0.224438459 triplet loss 0.756288528 l2_loss 25.2209511 fraction B 0.120883666 lossA 2.35986018 fraction A 0.042979233\n",
      "step 4625 loss 1.20763969 fisher_loss 0.223883197 triplet loss 0.983756483 l2_loss 25.2235661 fraction B 0.294729084 lossA 2.29735255 fraction A 0.0390729047\n",
      "step 4626 loss 0.987901449 fisher_loss 0.223548442 triplet loss 0.764353 l2_loss 25.2258511 fraction B 0.236425385 lossA 2.29907775 fraction A 0.0393000953\n",
      "step 4627 loss 1.03637242 fisher_loss 0.223709255 triplet loss 0.812663198 l2_loss 25.2297211 fraction B 0.212277889 lossA 2.29606891 fraction A 0.0392794833\n",
      "step 4628 loss 1.08371079 fisher_loss 0.223875076 triplet loss 0.859835684 l2_loss 25.2344303 fraction B 0.200824738 lossA 2.3695879 fraction A 0.043835137\n",
      "step 4629 loss 0.960652947 fisher_loss 0.224250704 triplet loss 0.736402273 l2_loss 25.2403774 fraction B 0.214205578 lossA 2.42044377 fraction A 0.0478146\n",
      "step 4630 loss 1.1907 fisher_loss 0.224777669 triplet loss 0.965922356 l2_loss 25.2481537 fraction B 0.291678905 lossA 2.42557907 fraction A 0.0487535261\n",
      "step 4631 loss 1.17376781 fisher_loss 0.22508809 triplet loss 0.948679686 l2_loss 25.254755 fraction B 0.213857442 lossA 2.4106555 fraction A 0.0487526581\n",
      "step 4632 loss 0.983539939 fisher_loss 0.225270331 triplet loss 0.758269608 l2_loss 25.2609215 fraction B 0.151720792 lossA 2.37341356 fraction A 0.047613997\n",
      "step 4633 loss 1.42922807 fisher_loss 0.225423872 triplet loss 1.20380425 l2_loss 25.2673416 fraction B 0.154767454 lossA 2.27509904 fraction A 0.042024076\n",
      "step 4634 loss 1.09834909 fisher_loss 0.225358382 triplet loss 0.872990668 l2_loss 25.2725029 fraction B 0.2476843 lossA 2.15451145 fraction A 0.0354224928\n",
      "step 4635 loss 0.973742247 fisher_loss 0.225268364 triplet loss 0.748473883 l2_loss 25.276289 fraction B 0.205175057 lossA 2.1211412 fraction A 0.0336981826\n",
      "step 4636 loss 0.945165396 fisher_loss 0.225344881 triplet loss 0.719820499 l2_loss 25.2802601 fraction B 0.171274215 lossA 2.19296741 fraction A 0.0381402783\n",
      "step 4637 loss 1.13327217 fisher_loss 0.225785613 triplet loss 0.907486618 l2_loss 25.2858906 fraction B 0.245721519 lossA 2.2724843 fraction A 0.0451107919\n",
      "step 4638 loss 0.956805468 fisher_loss 0.226411968 triplet loss 0.730393469 l2_loss 25.29179 fraction B 0.170353711 lossA 2.41716909 fraction A 0.0591263622\n",
      "step 4639 loss 1.48927701 fisher_loss 0.227813452 triplet loss 1.26146352 l2_loss 25.3001175 fraction B 0.182085678 lossA 2.52093959 fraction A 0.0675575957\n",
      "step 4640 loss 1.24006748 fisher_loss 0.228896186 triplet loss 1.01117134 l2_loss 25.3066349 fraction B 0.206030145 lossA 2.48097539 fraction A 0.0642394125\n",
      "step 4641 loss 1.15548754 fisher_loss 0.228720903 triplet loss 0.926766634 l2_loss 25.3105469 fraction B 0.227033064 lossA 2.40933919 fraction A 0.0570428707\n",
      "step 4642 loss 1.17621446 fisher_loss 0.228147909 triplet loss 0.948066533 l2_loss 25.3129368 fraction B 0.20871675 lossA 2.36719847 fraction A 0.050892476\n",
      "step 4643 loss 1.22329736 fisher_loss 0.227668136 triplet loss 0.995629251 l2_loss 25.3144512 fraction B 0.198369563 lossA 2.32170153 fraction A 0.0460416786\n",
      "step 4644 loss 1.21427393 fisher_loss 0.227365062 triplet loss 0.986908853 l2_loss 25.3164062 fraction B 0.213043302 lossA 2.31885743 fraction A 0.0445427857\n",
      "step 4645 loss 0.992557526 fisher_loss 0.227391094 triplet loss 0.765166402 l2_loss 25.3177433 fraction B 0.147630304 lossA 2.33311677 fraction A 0.0446689129\n",
      "step 4646 loss 1.38632703 fisher_loss 0.227533877 triplet loss 1.15879309 l2_loss 25.3210297 fraction B 0.123711944 lossA 2.38842082 fraction A 0.0481480286\n",
      "step 4647 loss 0.767642796 fisher_loss 0.227795258 triplet loss 0.539847553 l2_loss 25.3259163 fraction B 0.0979575142 lossA 2.40638924 fraction A 0.0484513938\n",
      "step 4648 loss 1.22250915 fisher_loss 0.228383318 triplet loss 0.994125843 l2_loss 25.3317509 fraction B 0.242292717 lossA 2.41469979 fraction A 0.0485637188\n",
      "step 4649 loss 1.16483808 fisher_loss 0.228726879 triplet loss 0.936111212 l2_loss 25.3366852 fraction B 0.274821311 lossA 2.3779707 fraction A 0.0461594388\n",
      "step 4650 loss 1.47084498 fisher_loss 0.2283687 triplet loss 1.24247622 l2_loss 25.34062 fraction B 0.151952848 lossA 2.40060878 fraction A 0.0480366088\n",
      "step 4651 loss 1.15814412 fisher_loss 0.228401214 triplet loss 0.929742873 l2_loss 25.3459415 fraction B 0.12054573 lossA 2.39204144 fraction A 0.0476576835\n",
      "step 4652 loss 1.1621027 fisher_loss 0.228279263 triplet loss 0.933823407 l2_loss 25.3508816 fraction B 0.258046746 lossA 2.34443426 fraction A 0.0441753045\n",
      "step 4653 loss 1.02895367 fisher_loss 0.227832615 triplet loss 0.801121056 l2_loss 25.3543606 fraction B 0.159341753 lossA 2.32306719 fraction A 0.0425214507\n",
      "step 4654 loss 0.956694245 fisher_loss 0.227665335 triplet loss 0.729028881 l2_loss 25.3587532 fraction B 0.122745983 lossA 2.35921383 fraction A 0.0459300056\n",
      "step 4655 loss 1.11363387 fisher_loss 0.22779265 triplet loss 0.885841191 l2_loss 25.3651505 fraction B 0.189597666 lossA 2.3611536 fraction A 0.0475076474\n",
      "step 4656 loss 0.954988837 fisher_loss 0.227786317 triplet loss 0.727202535 l2_loss 25.3712578 fraction B 0.134128258 lossA 2.33020091 fraction A 0.0469115302\n",
      "step 4657 loss 1.11494696 fisher_loss 0.22787632 triplet loss 0.887070596 l2_loss 25.3773422 fraction B 0.258520037 lossA 2.30584598 fraction A 0.0469450727\n",
      "step 4658 loss 1.04071248 fisher_loss 0.228063136 triplet loss 0.812649369 l2_loss 25.3827343 fraction B 0.232510701 lossA 2.27688503 fraction A 0.0458507501\n",
      "step 4659 loss 0.895860493 fisher_loss 0.228344917 triplet loss 0.667515576 l2_loss 25.3873425 fraction B 0.146906793 lossA 2.21012068 fraction A 0.0409366712\n",
      "step 4660 loss 1.01750207 fisher_loss 0.228326216 triplet loss 0.789175808 l2_loss 25.3907166 fraction B 0.159755692 lossA 2.19730163 fraction A 0.0406198315\n",
      "step 4661 loss 1.13148201 fisher_loss 0.228571549 triplet loss 0.902910471 l2_loss 25.394083 fraction B 0.173806608 lossA 2.23527122 fraction A 0.0432891883\n",
      "step 4662 loss 0.781668425 fisher_loss 0.228705093 triplet loss 0.552963316 l2_loss 25.3975372 fraction B 0.10479141 lossA 2.24167299 fraction A 0.0444567539\n",
      "step 4663 loss 1.46260929 fisher_loss 0.228584468 triplet loss 1.23402476 l2_loss 25.4035263 fraction B 0.222179517 lossA 2.28214169 fraction A 0.0490853637\n",
      "step 4664 loss 1.22160876 fisher_loss 0.228501543 triplet loss 0.993107259 l2_loss 25.4089012 fraction B 0.233460292 lossA 2.29427481 fraction A 0.05074092\n",
      "step 4665 loss 1.10379696 fisher_loss 0.228095174 triplet loss 0.875701785 l2_loss 25.412859 fraction B 0.133947253 lossA 2.36370087 fraction A 0.0564194769\n",
      "step 4666 loss 1.17638409 fisher_loss 0.227706149 triplet loss 0.948677957 l2_loss 25.4168148 fraction B 0.190580204 lossA 2.41351247 fraction A 0.0602661483\n",
      "step 4667 loss 1.09376609 fisher_loss 0.227368683 triplet loss 0.866397381 l2_loss 25.4202843 fraction B 0.151001349 lossA 2.42690301 fraction A 0.0607075207\n",
      "step 4668 loss 0.94178 fisher_loss 0.226880476 triplet loss 0.71489948 l2_loss 25.4224205 fraction B 0.113419116 lossA 2.3915956 fraction A 0.0558370911\n",
      "step 4669 loss 1.08220184 fisher_loss 0.226172537 triplet loss 0.856029332 l2_loss 25.4242153 fraction B 0.134437174 lossA 2.29748654 fraction A 0.0453940518\n",
      "step 4670 loss 0.995896161 fisher_loss 0.225130394 triplet loss 0.770765781 l2_loss 25.4261017 fraction B 0.167333394 lossA 2.16195107 fraction A 0.0354786\n",
      "step 4671 loss 1.33083844 fisher_loss 0.2245581 triplet loss 1.10628033 l2_loss 25.4281197 fraction B 0.213233739 lossA 2.06130171 fraction A 0.0305434037\n",
      "step 4672 loss 1.2052834 fisher_loss 0.224358648 triplet loss 0.980924785 l2_loss 25.4307671 fraction B 0.28252 lossA 1.99225497 fraction A 0.0276847091\n",
      "step 4673 loss 1.12689054 fisher_loss 0.224299744 triplet loss 0.902590811 l2_loss 25.4340191 fraction B 0.232390761 lossA 2.00159526 fraction A 0.0276638083\n",
      "step 4674 loss 0.86591363 fisher_loss 0.22428526 triplet loss 0.641628385 l2_loss 25.4374752 fraction B 0.221115291 lossA 2.07325983 fraction A 0.0301977303\n",
      "step 4675 loss 1.0094502 fisher_loss 0.224124253 triplet loss 0.785325944 l2_loss 25.4433098 fraction B 0.215244129 lossA 2.32475591 fraction A 0.0425032675\n",
      "step 4676 loss 1.13421011 fisher_loss 0.224539638 triplet loss 0.909670472 l2_loss 25.4524632 fraction B 0.207139254 lossA 2.51868844 fraction A 0.0576975979\n",
      "step 4677 loss 0.985491812 fisher_loss 0.225562319 triplet loss 0.759929478 l2_loss 25.4623013 fraction B 0.128315762 lossA 2.64087772 fraction A 0.0684658438\n",
      "step 4678 loss 0.931577206 fisher_loss 0.226620808 triplet loss 0.704956412 l2_loss 25.4705791 fraction B 0.149680212 lossA 2.68535209 fraction A 0.0719836429\n",
      "step 4679 loss 1.07980525 fisher_loss 0.227488205 triplet loss 0.852317095 l2_loss 25.4778538 fraction B 0.303428113 lossA 2.63962984 fraction A 0.0687281266\n",
      "step 4680 loss 0.975226104 fisher_loss 0.227740452 triplet loss 0.747485638 l2_loss 25.4831963 fraction B 0.264587611 lossA 2.52599955 fraction A 0.0597734824\n",
      "step 4681 loss 1.28262532 fisher_loss 0.227511525 triplet loss 1.05511379 l2_loss 25.4874611 fraction B 0.179757088 lossA 2.38602829 fraction A 0.0493190363\n",
      "step 4682 loss 1.5633384 fisher_loss 0.227026865 triplet loss 1.33631158 l2_loss 25.4905052 fraction B 0.170248896 lossA 2.24305511 fraction A 0.0397321209\n",
      "step 4683 loss 1.28215659 fisher_loss 0.226582631 triplet loss 1.05557394 l2_loss 25.4929981 fraction B 0.237493709 lossA 2.07293582 fraction A 0.0317926519\n",
      "step 4684 loss 1.08456278 fisher_loss 0.226337 triplet loss 0.858225822 l2_loss 25.494772 fraction B 0.230420619 lossA 2.03874707 fraction A 0.0305763297\n",
      "step 4685 loss 1.09409928 fisher_loss 0.226540759 triplet loss 0.867558539 l2_loss 25.4978428 fraction B 0.211461484 lossA 2.12585807 fraction A 0.0340472981\n",
      "step 4686 loss 0.955928326 fisher_loss 0.227060944 triplet loss 0.728867412 l2_loss 25.5028248 fraction B 0.20899789 lossA 2.27720976 fraction A 0.0422421\n",
      "step 4687 loss 0.977679133 fisher_loss 0.22801587 triplet loss 0.749663293 l2_loss 25.5096378 fraction B 0.181006014 lossA 2.44585299 fraction A 0.0559928492\n",
      "step 4688 loss 1.15388 fisher_loss 0.229338527 triplet loss 0.924541473 l2_loss 25.5180988 fraction B 0.202114657 lossA 2.62083626 fraction A 0.0699744448\n",
      "step 4689 loss 1.14362478 fisher_loss 0.230714262 triplet loss 0.912910521 l2_loss 25.526104 fraction B 0.194017202 lossA 2.75257254 fraction A 0.0792942122\n",
      "step 4690 loss 1.1182071 fisher_loss 0.231511846 triplet loss 0.886695206 l2_loss 25.5318241 fraction B 0.209074542 lossA 2.7875843 fraction A 0.0807743445\n",
      "step 4691 loss 1.42984796 fisher_loss 0.231299445 triplet loss 1.19854856 l2_loss 25.5356083 fraction B 0.198812187 lossA 2.6977551 fraction A 0.0733127594\n",
      "step 4692 loss 1.24489546 fisher_loss 0.229548052 triplet loss 1.01534736 l2_loss 25.5360565 fraction B 0.166003779 lossA 2.56284308 fraction A 0.0619124323\n",
      "step 4693 loss 1.42459202 fisher_loss 0.227502197 triplet loss 1.19708979 l2_loss 25.5349274 fraction B 0.103674807 lossA 2.40387917 fraction A 0.0483326204\n",
      "step 4694 loss 1.07089651 fisher_loss 0.225920931 triplet loss 0.844975591 l2_loss 25.5338745 fraction B 0.157726347 lossA 2.31634212 fraction A 0.0404321626\n",
      "step 4695 loss 0.866346776 fisher_loss 0.224957928 triplet loss 0.641388834 l2_loss 25.5342598 fraction B 0.173095927 lossA 2.26404977 fraction A 0.0365167782\n",
      "step 4696 loss 1.32530236 fisher_loss 0.224425793 triplet loss 1.10087657 l2_loss 25.5358734 fraction B 0.182006523 lossA 2.29784966 fraction A 0.0379709378\n",
      "step 4697 loss 1.14311552 fisher_loss 0.224478796 triplet loss 0.918636739 l2_loss 25.5385017 fraction B 0.24460271 lossA 2.38165712 fraction A 0.0436009578\n",
      "step 4698 loss 0.928643584 fisher_loss 0.224792793 triplet loss 0.703850806 l2_loss 25.5423889 fraction B 0.167233318 lossA 2.48196959 fraction A 0.0512313396\n",
      "step 4699 loss 1.1424787 fisher_loss 0.225428477 triplet loss 0.917050242 l2_loss 25.5481014 fraction B 0.190121382 lossA 2.60044622 fraction A 0.0591780543\n",
      "step 4700 loss 1.10201406 fisher_loss 0.226209402 triplet loss 0.875804603 l2_loss 25.5533867 fraction B 0.165053919 lossA 2.72915649 fraction A 0.0678934529\n",
      "step 4701 loss 1.23617589 fisher_loss 0.227067336 triplet loss 1.00910854 l2_loss 25.558485 fraction B 0.149472326 lossA 2.76396585 fraction A 0.0704544\n",
      "step 4702 loss 1.0645659 fisher_loss 0.227302641 triplet loss 0.837263286 l2_loss 25.5628757 fraction B 0.166590676 lossA 2.71260786 fraction A 0.0675466508\n",
      "step 4703 loss 1.0487591 fisher_loss 0.226881936 triplet loss 0.821877182 l2_loss 25.566061 fraction B 0.163194448 lossA 2.59798717 fraction A 0.0597027764\n",
      "step 4704 loss 1.11304867 fisher_loss 0.226124942 triplet loss 0.88692373 l2_loss 25.5691624 fraction B 0.209192231 lossA 2.43501806 fraction A 0.0481378511\n",
      "step 4705 loss 0.99944073 fisher_loss 0.225189343 triplet loss 0.774251401 l2_loss 25.5711327 fraction B 0.145940766 lossA 2.28766346 fraction A 0.0386276208\n",
      "step 4706 loss 0.955683172 fisher_loss 0.224567935 triplet loss 0.731115222 l2_loss 25.5740852 fraction B 0.208377481 lossA 2.17087531 fraction A 0.0332901329\n",
      "step 4707 loss 1.1118598 fisher_loss 0.22410053 triplet loss 0.887759268 l2_loss 25.5783119 fraction B 0.179788783 lossA 2.11484289 fraction A 0.0314004347\n",
      "step 4708 loss 0.98915869 fisher_loss 0.223800853 triplet loss 0.765357852 l2_loss 25.5832272 fraction B 0.246211633 lossA 2.23538256 fraction A 0.0375222489\n",
      "step 4709 loss 0.80966264 fisher_loss 0.224153712 triplet loss 0.585508943 l2_loss 25.590889 fraction B 0.145775035 lossA 2.3667438 fraction A 0.0510158874\n",
      "step 4710 loss 1.13868737 fisher_loss 0.225224197 triplet loss 0.913463175 l2_loss 25.6011887 fraction B 0.201003149 lossA 2.5189 fraction A 0.0648704842\n",
      "step 4711 loss 1.18358099 fisher_loss 0.226500884 triplet loss 0.957080066 l2_loss 25.6097546 fraction B 0.155726656 lossA 2.60225725 fraction A 0.0712999\n",
      "step 4712 loss 0.954788089 fisher_loss 0.227082044 triplet loss 0.727706 l2_loss 25.6167259 fraction B 0.182821274 lossA 2.4689877 fraction A 0.0612449199\n",
      "step 4713 loss 1.41930234 fisher_loss 0.225933105 triplet loss 1.19336927 l2_loss 25.6198711 fraction B 0.152341336 lossA 2.35375237 fraction A 0.0505972244\n",
      "step 4714 loss 1.03197479 fisher_loss 0.224709302 triplet loss 0.80726546 l2_loss 25.620985 fraction B 0.241477981 lossA 2.24503922 fraction A 0.0378007628\n",
      "step 4715 loss 0.93035388 fisher_loss 0.223613888 triplet loss 0.70674 l2_loss 25.6209316 fraction B 0.148297504 lossA 2.0578444 fraction A 0.0293228086\n",
      "step 4716 loss 1.15150368 fisher_loss 0.223097309 triplet loss 0.928406417 l2_loss 25.6217728 fraction B 0.22795631 lossA 1.9557445 fraction A 0.0269711222\n",
      "step 4717 loss 1.0162828 fisher_loss 0.223187968 triplet loss 0.793094814 l2_loss 25.625351 fraction B 0.293192625 lossA 2.16440558 fraction A 0.0331415497\n",
      "step 4718 loss 1.0032711 fisher_loss 0.223886073 triplet loss 0.779385 l2_loss 25.6335049 fraction B 0.182525873 lossA 2.38812494 fraction A 0.0509507954\n",
      "step 4719 loss 1.10411394 fisher_loss 0.225847095 triplet loss 0.878266871 l2_loss 25.6445026 fraction B 0.209588751 lossA 2.62084126 fraction A 0.0703304783\n",
      "step 4720 loss 1.64494801 fisher_loss 0.228100359 triplet loss 1.41684759 l2_loss 25.6549339 fraction B 0.225261346 lossA 2.77386236 fraction A 0.0818826854\n",
      "step 4721 loss 1.10803556 fisher_loss 0.22984767 triplet loss 0.878187954 l2_loss 25.662878 fraction B 0.173904866 lossA 2.79152799 fraction A 0.0837974921\n",
      "step 4722 loss 1.167292 fisher_loss 0.230372086 triplet loss 0.936919868 l2_loss 25.6678486 fraction B 0.130303413 lossA 2.70595622 fraction A 0.0779825523\n",
      "step 4723 loss 0.893931329 fisher_loss 0.229916051 triplet loss 0.664015293 l2_loss 25.6701851 fraction B 0.192344099 lossA 2.47072816 fraction A 0.0607448295\n",
      "step 4724 loss 1.03379893 fisher_loss 0.228342667 triplet loss 0.805456221 l2_loss 25.6712418 fraction B 0.162378386 lossA 2.2772758 fraction A 0.0433196872\n",
      "step 4725 loss 1.03246057 fisher_loss 0.227043539 triplet loss 0.805417 l2_loss 25.6726208 fraction B 0.230939493 lossA 2.07676 fraction A 0.0314114429\n",
      "step 4726 loss 1.12603378 fisher_loss 0.226396814 triplet loss 0.899637 l2_loss 25.6743031 fraction B 0.26493296 lossA 1.96957743 fraction A 0.0279568136\n",
      "step 4727 loss 1.09503639 fisher_loss 0.226310849 triplet loss 0.868725538 l2_loss 25.6787949 fraction B 0.246642426 lossA 2.04823065 fraction A 0.0305544883\n",
      "step 4728 loss 1.01765776 fisher_loss 0.226602629 triplet loss 0.791055083 l2_loss 25.6850319 fraction B 0.162060305 lossA 2.21969843 fraction A 0.0395134464\n",
      "step 4729 loss 1.52001894 fisher_loss 0.227604941 triplet loss 1.29241395 l2_loss 25.6934738 fraction B 0.250704408 lossA 2.36515737 fraction A 0.0535449199\n",
      "step 4730 loss 1.15805185 fisher_loss 0.228546724 triplet loss 0.92950511 l2_loss 25.7010555 fraction B 0.19619149 lossA 2.54287648 fraction A 0.0676522404\n",
      "step 4731 loss 0.991845787 fisher_loss 0.229660109 triplet loss 0.762185693 l2_loss 25.7074413 fraction B 0.129525945 lossA 2.69388485 fraction A 0.0777605847\n",
      "step 4732 loss 0.960087419 fisher_loss 0.230661064 triplet loss 0.729426324 l2_loss 25.7127094 fraction B 0.158002436 lossA 2.71096516 fraction A 0.0778114721\n",
      "step 4733 loss 1.21528316 fisher_loss 0.230723977 triplet loss 0.984559178 l2_loss 25.7164955 fraction B 0.131721884 lossA 2.64010859 fraction A 0.0720475\n",
      "step 4734 loss 1.17672658 fisher_loss 0.230247542 triplet loss 0.946479 l2_loss 25.7186794 fraction B 0.258912146 lossA 2.49503589 fraction A 0.060879942\n",
      "step 4735 loss 1.09999359 fisher_loss 0.229085431 triplet loss 0.870908141 l2_loss 25.7188 fraction B 0.227453977 lossA 2.37525129 fraction A 0.0500356667\n",
      "step 4736 loss 1.03342104 fisher_loss 0.22795257 triplet loss 0.8054685 l2_loss 25.7187214 fraction B 0.152401656 lossA 2.26505518 fraction A 0.0399563052\n",
      "step 4737 loss 0.982427239 fisher_loss 0.227156252 triplet loss 0.755271 l2_loss 25.7190456 fraction B 0.135046139 lossA 2.21565866 fraction A 0.0367297307\n",
      "step 4738 loss 1.31266415 fisher_loss 0.226812467 triplet loss 1.08585167 l2_loss 25.7206078 fraction B 0.241641462 lossA 2.19340682 fraction A 0.0357507765\n",
      "step 4739 loss 1.19268155 fisher_loss 0.226744473 triplet loss 0.965937138 l2_loss 25.7214851 fraction B 0.202281937 lossA 2.27833867 fraction A 0.0402057\n",
      "step 4740 loss 1.06166387 fisher_loss 0.227078021 triplet loss 0.834585786 l2_loss 25.7238083 fraction B 0.189215168 lossA 2.40959167 fraction A 0.0487346202\n",
      "step 4741 loss 1.36190689 fisher_loss 0.227970317 triplet loss 1.13393652 l2_loss 25.7269382 fraction B 0.232852757 lossA 2.51591325 fraction A 0.0563281961\n",
      "step 4742 loss 1.36467981 fisher_loss 0.228674591 triplet loss 1.13600516 l2_loss 25.7297211 fraction B 0.197790653 lossA 2.57274961 fraction A 0.0598915331\n",
      "step 4743 loss 0.774167717 fisher_loss 0.228965938 triplet loss 0.545201778 l2_loss 25.7315617 fraction B 0.0739220902 lossA 2.63062096 fraction A 0.065505\n",
      "step 4744 loss 0.962844908 fisher_loss 0.229668185 triplet loss 0.733176708 l2_loss 25.7383575 fraction B 0.161984861 lossA 2.66950226 fraction A 0.069396615\n",
      "step 4745 loss 1.0703764 fisher_loss 0.230361357 triplet loss 0.840015054 l2_loss 25.7445755 fraction B 0.215556681 lossA 2.58080482 fraction A 0.0640081316\n",
      "step 4746 loss 1.31867135 fisher_loss 0.229967952 triplet loss 1.08870339 l2_loss 25.7486877 fraction B 0.18860805 lossA 2.43296838 fraction A 0.0536096841\n",
      "step 4747 loss 1.16056371 fisher_loss 0.228962988 triplet loss 0.93160069 l2_loss 25.7518444 fraction B 0.261314064 lossA 2.30649185 fraction A 0.043641638\n",
      "step 4748 loss 1.13095653 fisher_loss 0.228008151 triplet loss 0.90294838 l2_loss 25.7538834 fraction B 0.185979337 lossA 2.17879868 fraction A 0.0356458761\n",
      "step 4749 loss 1.12238097 fisher_loss 0.227111101 triplet loss 0.895269811 l2_loss 25.7561073 fraction B 0.307916135 lossA 2.03156447 fraction A 0.0300185736\n",
      "step 4750 loss 1.11714661 fisher_loss 0.226400182 triplet loss 0.890746474 l2_loss 25.7584095 fraction B 0.173614413 lossA 2.00596261 fraction A 0.0291053634\n",
      "step 4751 loss 1.02115929 fisher_loss 0.226141617 triplet loss 0.79501766 l2_loss 25.7614899 fraction B 0.227143958 lossA 2.07847357 fraction A 0.0313480198\n",
      "step 4752 loss 1.12770891 fisher_loss 0.226306498 triplet loss 0.901402414 l2_loss 25.76507 fraction B 0.219190702 lossA 2.24394655 fraction A 0.0386008509\n",
      "step 4753 loss 1.07579291 fisher_loss 0.22680369 triplet loss 0.848989248 l2_loss 25.7700329 fraction B 0.161367342 lossA 2.44833875 fraction A 0.052004911\n",
      "step 4754 loss 1.09990156 fisher_loss 0.228020206 triplet loss 0.871881366 l2_loss 25.7769718 fraction B 0.184083134 lossA 2.64771771 fraction A 0.0665117726\n",
      "step 4755 loss 1.17974043 fisher_loss 0.22947894 triplet loss 0.950261533 l2_loss 25.7841015 fraction B 0.215685084 lossA 2.7670989 fraction A 0.0747515634\n",
      "step 4756 loss 1.18761754 fisher_loss 0.230333969 triplet loss 0.957283556 l2_loss 25.7892647 fraction B 0.265542328 lossA 2.79813409 fraction A 0.0766503587\n",
      "step 4757 loss 1.06577992 fisher_loss 0.230186343 triplet loss 0.835593522 l2_loss 25.7929745 fraction B 0.162347436 lossA 2.69798303 fraction A 0.068579942\n",
      "step 4758 loss 1.12838638 fisher_loss 0.228971019 triplet loss 0.899415314 l2_loss 25.794445 fraction B 0.216784865 lossA 2.53814864 fraction A 0.0558882356\n",
      "step 4759 loss 1.25358653 fisher_loss 0.227235377 triplet loss 1.02635109 l2_loss 25.7948837 fraction B 0.168340266 lossA 2.39394522 fraction A 0.0450254157\n",
      "step 4760 loss 1.25577939 fisher_loss 0.225841179 triplet loss 1.02993822 l2_loss 25.795414 fraction B 0.274934 lossA 2.24984288 fraction A 0.0366238542\n",
      "step 4761 loss 1.15108311 fisher_loss 0.224658698 triplet loss 0.926424384 l2_loss 25.7955303 fraction B 0.155484498 lossA 2.16015816 fraction A 0.0326963365\n",
      "step 4762 loss 1.24773455 fisher_loss 0.223959148 triplet loss 1.02377534 l2_loss 25.7976074 fraction B 0.169963807 lossA 2.1007545 fraction A 0.0304796882\n",
      "step 4763 loss 1.03502369 fisher_loss 0.223322883 triplet loss 0.811700761 l2_loss 25.80019 fraction B 0.234844968 lossA 2.16464686 fraction A 0.0323843136\n",
      "step 4764 loss 1.25763822 fisher_loss 0.223196775 triplet loss 1.03444147 l2_loss 25.8054485 fraction B 0.300464332 lossA 2.27719474 fraction A 0.0368892215\n",
      "step 4765 loss 0.791836858 fisher_loss 0.223172143 triplet loss 0.56866473 l2_loss 25.8102379 fraction B 0.115684509 lossA 2.41859865 fraction A 0.0483202301\n",
      "step 4766 loss 0.932443738 fisher_loss 0.223840401 triplet loss 0.708603323 l2_loss 25.8188267 fraction B 0.107067637 lossA 2.57448053 fraction A 0.0621410683\n",
      "step 4767 loss 1.02916694 fisher_loss 0.225130409 triplet loss 0.804036498 l2_loss 25.8285 fraction B 0.222505048 lossA 2.67265773 fraction A 0.0695636049\n",
      "step 4768 loss 1.29733586 fisher_loss 0.22610943 triplet loss 1.07122648 l2_loss 25.8363552 fraction B 0.37332198 lossA 2.69778919 fraction A 0.0713884681\n",
      "step 4769 loss 1.03989947 fisher_loss 0.226380035 triplet loss 0.813519478 l2_loss 25.8415565 fraction B 0.163823858 lossA 2.62568951 fraction A 0.0664034262\n",
      "step 4770 loss 1.11583138 fisher_loss 0.225885063 triplet loss 0.889946342 l2_loss 25.8450317 fraction B 0.234425232 lossA 2.52047443 fraction A 0.0585355721\n",
      "step 4771 loss 1.00784814 fisher_loss 0.225153521 triplet loss 0.782694638 l2_loss 25.8479309 fraction B 0.152822405 lossA 2.38842249 fraction A 0.0469751731\n",
      "step 4772 loss 0.977895558 fisher_loss 0.224213257 triplet loss 0.753682315 l2_loss 25.8501339 fraction B 0.147078946 lossA 2.27667475 fraction A 0.0387620069\n",
      "step 4773 loss 1.34347761 fisher_loss 0.223876327 triplet loss 1.11960125 l2_loss 25.8538876 fraction B 0.311996639 lossA 2.20666718 fraction A 0.0348180421\n",
      "step 4774 loss 1.45009351 fisher_loss 0.22378847 triplet loss 1.22630501 l2_loss 25.857605 fraction B 0.310233951 lossA 2.20780396 fraction A 0.034401048\n",
      "step 4775 loss 1.18597126 fisher_loss 0.223817572 triplet loss 0.962153673 l2_loss 25.8624191 fraction B 0.20386292 lossA 2.28772902 fraction A 0.0379212052\n",
      "step 4776 loss 1.3936702 fisher_loss 0.224025726 triplet loss 1.16964447 l2_loss 25.8679371 fraction B 0.221142486 lossA 2.37359405 fraction A 0.0443267897\n",
      "step 4777 loss 1.35146534 fisher_loss 0.224735111 triplet loss 1.1267302 l2_loss 25.8738384 fraction B 0.20041953 lossA 2.42421174 fraction A 0.0491429195\n",
      "step 4778 loss 1.06078839 fisher_loss 0.22539103 triplet loss 0.835397422 l2_loss 25.8790054 fraction B 0.170706585 lossA 2.48506618 fraction A 0.0545788221\n",
      "step 4779 loss 1.11741817 fisher_loss 0.22633034 triplet loss 0.89108783 l2_loss 25.8846283 fraction B 0.172266543 lossA 2.52922106 fraction A 0.0580717772\n",
      "step 4780 loss 1.1358707 fisher_loss 0.227035061 triplet loss 0.90883559 l2_loss 25.8893852 fraction B 0.22823891 lossA 2.57000613 fraction A 0.0609224662\n",
      "step 4781 loss 1.22756338 fisher_loss 0.227599412 triplet loss 0.999964 l2_loss 25.893589 fraction B 0.258473307 lossA 2.56527448 fraction A 0.060535606\n",
      "step 4782 loss 1.08949184 fisher_loss 0.227554157 triplet loss 0.861937642 l2_loss 25.8962231 fraction B 0.241714746 lossA 2.50672221 fraction A 0.055972781\n",
      "step 4783 loss 1.25139809 fisher_loss 0.227058977 triplet loss 1.02433908 l2_loss 25.8977928 fraction B 0.170240343 lossA 2.46973825 fraction A 0.0536642\n",
      "step 4784 loss 1.09803057 fisher_loss 0.226478174 triplet loss 0.871552408 l2_loss 25.9006157 fraction B 0.330909103 lossA 2.36988688 fraction A 0.0467546508\n",
      "step 4785 loss 1.21318 fisher_loss 0.225390986 triplet loss 0.987789 l2_loss 25.9014034 fraction B 0.244982883 lossA 2.27112985 fraction A 0.0406964421\n",
      "step 4786 loss 1.17783058 fisher_loss 0.224431664 triplet loss 0.953398883 l2_loss 25.9023476 fraction B 0.163476333 lossA 2.17921662 fraction A 0.0362146087\n",
      "step 4787 loss 0.897627771 fisher_loss 0.223644957 triplet loss 0.673982799 l2_loss 25.9033813 fraction B 0.112484679 lossA 2.17638016 fraction A 0.0365694463\n",
      "step 4788 loss 1.00687325 fisher_loss 0.223245367 triplet loss 0.783627927 l2_loss 25.9071522 fraction B 0.19264327 lossA 2.25236368 fraction A 0.042116154\n",
      "step 4789 loss 1.02654886 fisher_loss 0.22342658 triplet loss 0.803122282 l2_loss 25.9117947 fraction B 0.250830621 lossA 2.291224 fraction A 0.0461326689\n",
      "step 4790 loss 0.912202835 fisher_loss 0.2238141 triplet loss 0.688388765 l2_loss 25.916811 fraction B 0.106659047 lossA 2.41817331 fraction A 0.0584410653\n",
      "step 4791 loss 1.01091588 fisher_loss 0.225169882 triplet loss 0.785746038 l2_loss 25.9240379 fraction B 0.221076623 lossA 2.55558801 fraction A 0.06848149\n",
      "step 4792 loss 1.08234191 fisher_loss 0.226668119 triplet loss 0.85567373 l2_loss 25.9305706 fraction B 0.194320738 lossA 2.57176518 fraction A 0.0691361055\n",
      "step 4793 loss 1.00746489 fisher_loss 0.226897 triplet loss 0.780567944 l2_loss 25.9350967 fraction B 0.174596086 lossA 2.52486 fraction A 0.0651422665\n",
      "step 4794 loss 1.14008403 fisher_loss 0.226438075 triplet loss 0.913645923 l2_loss 25.9384022 fraction B 0.181594282 lossA 2.44986868 fraction A 0.059260495\n",
      "step 4795 loss 1.16217256 fisher_loss 0.226020336 triplet loss 0.936152279 l2_loss 25.9411869 fraction B 0.224843293 lossA 2.34984112 fraction A 0.0499981381\n",
      "step 4796 loss 1.03627014 fisher_loss 0.225225613 triplet loss 0.811044514 l2_loss 25.9429436 fraction B 0.196852982 lossA 2.26297188 fraction A 0.0417893529\n",
      "step 4797 loss 1.29004872 fisher_loss 0.224776506 triplet loss 1.06527221 l2_loss 25.9449635 fraction B 0.280745953 lossA 2.16341186 fraction A 0.0357416868\n",
      "step 4798 loss 0.9627738 fisher_loss 0.224657103 triplet loss 0.738116682 l2_loss 25.9460506 fraction B 0.141905308 lossA 2.16993642 fraction A 0.0360491797\n",
      "step 4799 loss 0.954008043 fisher_loss 0.225140885 triplet loss 0.728867173 l2_loss 25.9502659 fraction B 0.127151534 lossA 2.28534198 fraction A 0.0422496349\n",
      "step 4800 loss 1.13128209 fisher_loss 0.226164535 triplet loss 0.905117571 l2_loss 25.957222 fraction B 0.232206702 lossA 2.4207902 fraction A 0.0518158302\n",
      "step 4801 loss 1.18271124 fisher_loss 0.227534801 triplet loss 0.955176473 l2_loss 25.9643517 fraction B 0.140517965 lossA 2.49712276 fraction A 0.0576143935\n",
      "step 4802 loss 1.10218716 fisher_loss 0.22854495 triplet loss 0.873642147 l2_loss 25.9708672 fraction B 0.214945659 lossA 2.51127219 fraction A 0.058984153\n",
      "step 4803 loss 1.12809026 fisher_loss 0.229073375 triplet loss 0.899016917 l2_loss 25.9762897 fraction B 0.202041954 lossA 2.52091432 fraction A 0.0597003475\n",
      "step 4804 loss 1.32204151 fisher_loss 0.229386926 triplet loss 1.09265459 l2_loss 25.9812107 fraction B 0.266951859 lossA 2.49936247 fraction A 0.0586887561\n",
      "step 4805 loss 1.22932553 fisher_loss 0.229288936 triplet loss 1.0000366 l2_loss 25.9853611 fraction B 0.178157493 lossA 2.43208623 fraction A 0.0538522303\n",
      "step 4806 loss 1.01793385 fisher_loss 0.228750855 triplet loss 0.789183 l2_loss 25.9881248 fraction B 0.171659917 lossA 2.34736705 fraction A 0.0474005714\n",
      "step 4807 loss 1.21274352 fisher_loss 0.227916241 triplet loss 0.98482734 l2_loss 25.990675 fraction B 0.257957548 lossA 2.26208973 fraction A 0.0412188098\n",
      "step 4808 loss 0.971553147 fisher_loss 0.227263272 triplet loss 0.744289875 l2_loss 25.9929752 fraction B 0.184226 lossA 2.22738719 fraction A 0.0392941162\n",
      "step 4809 loss 1.04604983 fisher_loss 0.22705777 triplet loss 0.818992078 l2_loss 25.9969883 fraction B 0.124643989 lossA 2.18321133 fraction A 0.036926385\n",
      "step 4810 loss 1.20312822 fisher_loss 0.227029592 triplet loss 0.976098597 l2_loss 26.0008678 fraction B 0.19129546 lossA 2.20566583 fraction A 0.038014695\n",
      "step 4811 loss 0.803715706 fisher_loss 0.227397084 triplet loss 0.576318622 l2_loss 26.0052452 fraction B 0.0978340209 lossA 2.36190462 fraction A 0.0486421064\n",
      "step 4812 loss 1.01535499 fisher_loss 0.228487194 triplet loss 0.786867797 l2_loss 26.0140877 fraction B 0.144201756 lossA 2.51902 fraction A 0.0615803786\n",
      "step 4813 loss 1.17938244 fisher_loss 0.229724675 triplet loss 0.949657798 l2_loss 26.0234432 fraction B 0.15327 lossA 2.68067 fraction A 0.0725980178\n",
      "step 4814 loss 1.51193726 fisher_loss 0.230814129 triplet loss 1.28112316 l2_loss 26.0306664 fraction B 0.279469788 lossA 2.74154091 fraction A 0.076172\n",
      "step 4815 loss 0.933329523 fisher_loss 0.231121555 triplet loss 0.702208 l2_loss 26.035017 fraction B 0.154008523 lossA 2.61997914 fraction A 0.0673082694\n",
      "step 4816 loss 1.27151155 fisher_loss 0.229945093 triplet loss 1.04156649 l2_loss 26.0374241 fraction B 0.137294173 lossA 2.47128725 fraction A 0.0555553138\n",
      "step 4817 loss 1.15994048 fisher_loss 0.228594273 triplet loss 0.931346238 l2_loss 26.0394707 fraction B 0.22993803 lossA 2.35319829 fraction A 0.0457504131\n",
      "step 4818 loss 0.986890852 fisher_loss 0.227605626 triplet loss 0.759285212 l2_loss 26.0418987 fraction B 0.174881086 lossA 2.23339105 fraction A 0.0375365913\n",
      "step 4819 loss 0.998378396 fisher_loss 0.226984113 triplet loss 0.771394312 l2_loss 26.0449753 fraction B 0.192249984 lossA 2.16709375 fraction A 0.0347897895\n",
      "step 4820 loss 1.33542705 fisher_loss 0.226635307 triplet loss 1.10879171 l2_loss 26.0488052 fraction B 0.247477531 lossA 2.09222627 fraction A 0.0320931934\n",
      "step 4821 loss 1.04287934 fisher_loss 0.226404086 triplet loss 0.816475213 l2_loss 26.0522251 fraction B 0.159404099 lossA 2.23866892 fraction A 0.0386153758\n",
      "step 4822 loss 1.45825458 fisher_loss 0.22687906 triplet loss 1.23137546 l2_loss 26.0593777 fraction B 0.186205745 lossA 2.36792684 fraction A 0.0475952327\n",
      "step 4823 loss 1.16125011 fisher_loss 0.227480575 triplet loss 0.933769584 l2_loss 26.0664368 fraction B 0.218339145 lossA 2.46636677 fraction A 0.0560089685\n",
      "step 4824 loss 1.03822696 fisher_loss 0.228132829 triplet loss 0.810094118 l2_loss 26.0727768 fraction B 0.193769217 lossA 2.51410794 fraction A 0.0598717183\n",
      "step 4825 loss 1.10733366 fisher_loss 0.228459254 triplet loss 0.878874421 l2_loss 26.078722 fraction B 0.279478729 lossA 2.50388741 fraction A 0.0589602105\n",
      "step 4826 loss 1.19788074 fisher_loss 0.228216559 triplet loss 0.969664216 l2_loss 26.0833321 fraction B 0.178336427 lossA 2.4472065 fraction A 0.054236155\n",
      "step 4827 loss 1.07223916 fisher_loss 0.227692917 triplet loss 0.844546199 l2_loss 26.0873032 fraction B 0.210464388 lossA 2.34447217 fraction A 0.045975972\n",
      "step 4828 loss 1.08410048 fisher_loss 0.226740167 triplet loss 0.857360363 l2_loss 26.0900764 fraction B 0.197547972 lossA 2.23571849 fraction A 0.0384473763\n",
      "step 4829 loss 1.13335085 fisher_loss 0.225928724 triplet loss 0.907422185 l2_loss 26.0931778 fraction B 0.196875528 lossA 2.11104774 fraction A 0.0328537226\n",
      "step 4830 loss 0.951947749 fisher_loss 0.225163475 triplet loss 0.726784289 l2_loss 26.0967751 fraction B 0.188069373 lossA 2.13765883 fraction A 0.0340759493\n",
      "step 4831 loss 1.11483085 fisher_loss 0.225000396 triplet loss 0.88983047 l2_loss 26.102541 fraction B 0.217967868 lossA 2.22599196 fraction A 0.0386828966\n",
      "step 4832 loss 0.951186121 fisher_loss 0.225034848 triplet loss 0.726151288 l2_loss 26.1080284 fraction B 0.152101576 lossA 2.33191037 fraction A 0.0471382104\n",
      "step 4833 loss 1.08421147 fisher_loss 0.225342631 triplet loss 0.858868837 l2_loss 26.1140842 fraction B 0.199120224 lossA 2.38369513 fraction A 0.0517499037\n",
      "step 4834 loss 0.993445873 fisher_loss 0.225273132 triplet loss 0.768172741 l2_loss 26.1179695 fraction B 0.172446728 lossA 2.45486474 fraction A 0.0576266721\n",
      "step 4835 loss 1.35495794 fisher_loss 0.225498036 triplet loss 1.12945986 l2_loss 26.123003 fraction B 0.195259348 lossA 2.46264911 fraction A 0.0581718944\n",
      "step 4836 loss 1.30164504 fisher_loss 0.225214303 triplet loss 1.07643068 l2_loss 26.1270542 fraction B 0.106561825 lossA 2.41242409 fraction A 0.0537536703\n",
      "step 4837 loss 1.07548249 fisher_loss 0.224744588 triplet loss 0.850737929 l2_loss 26.1304035 fraction B 0.175785914 lossA 2.34314752 fraction A 0.0475500636\n",
      "step 4838 loss 1.04393768 fisher_loss 0.224097058 triplet loss 0.81984067 l2_loss 26.1334553 fraction B 0.196870804 lossA 2.28162408 fraction A 0.0430624\n",
      "step 4839 loss 0.951397061 fisher_loss 0.223534957 triplet loss 0.72786212 l2_loss 26.1373234 fraction B 0.177447185 lossA 2.16598296 fraction A 0.0354605615\n",
      "step 4840 loss 1.04213285 fisher_loss 0.222782075 triplet loss 0.819350839 l2_loss 26.139185 fraction B 0.169669479 lossA 2.10382557 fraction A 0.0324458927\n",
      "step 4841 loss 1.02143 fisher_loss 0.222490549 triplet loss 0.798939466 l2_loss 26.1421661 fraction B 0.175049886 lossA 2.17729783 fraction A 0.0359825306\n",
      "step 4842 loss 1.01502967 fisher_loss 0.222673982 triplet loss 0.792355716 l2_loss 26.1464653 fraction B 0.192651957 lossA 2.32248211 fraction A 0.0470580496\n",
      "step 4843 loss 1.12771606 fisher_loss 0.223629609 triplet loss 0.904086411 l2_loss 26.1525078 fraction B 0.178731903 lossA 2.41398454 fraction A 0.0541107878\n",
      "step 4844 loss 1.09955359 fisher_loss 0.22448799 triplet loss 0.875065625 l2_loss 26.1578083 fraction B 0.151814148 lossA 2.503407 fraction A 0.0612215921\n",
      "step 4845 loss 1.09764528 fisher_loss 0.225460753 triplet loss 0.872184575 l2_loss 26.1634769 fraction B 0.171953589 lossA 2.5741775 fraction A 0.0654854774\n",
      "step 4846 loss 1.38038528 fisher_loss 0.226246491 triplet loss 1.1541388 l2_loss 26.1675358 fraction B 0.168937564 lossA 2.59390903 fraction A 0.0654783398\n",
      "step 4847 loss 1.0648365 fisher_loss 0.22650218 triplet loss 0.838334322 l2_loss 26.1698608 fraction B 0.226651579 lossA 2.54283857 fraction A 0.0590918064\n",
      "step 4848 loss 1.0858475 fisher_loss 0.226377308 triplet loss 0.859470189 l2_loss 26.171854 fraction B 0.15479289 lossA 2.46059108 fraction A 0.0525479242\n",
      "step 4849 loss 1.2502619 fisher_loss 0.225850374 triplet loss 1.02441156 l2_loss 26.1729374 fraction B 0.236271247 lossA 2.38720846 fraction A 0.0472043566\n",
      "step 4850 loss 1.23204589 fisher_loss 0.224815294 triplet loss 1.00723064 l2_loss 26.1746979 fraction B 0.128624186 lossA 2.32523012 fraction A 0.0435764\n",
      "step 4851 loss 0.966338515 fisher_loss 0.223644763 triplet loss 0.742693722 l2_loss 26.1785355 fraction B 0.198384762 lossA 2.28353357 fraction A 0.0412971601\n",
      "step 4852 loss 0.953277886 fisher_loss 0.222529575 triplet loss 0.730748296 l2_loss 26.183197 fraction B 0.195128039 lossA 2.28209305 fraction A 0.0415558964\n",
      "step 4853 loss 0.880367339 fisher_loss 0.221874133 triplet loss 0.658493221 l2_loss 26.1891041 fraction B 0.123851895 lossA 2.29480219 fraction A 0.0437520444\n",
      "step 4854 loss 0.980491161 fisher_loss 0.221789628 triplet loss 0.758701563 l2_loss 26.1960163 fraction B 0.22470428 lossA 2.35825062 fraction A 0.0506787263\n",
      "step 4855 loss 0.940809786 fisher_loss 0.22227478 triplet loss 0.718535 l2_loss 26.2026749 fraction B 0.170040056 lossA 2.4117496 fraction A 0.0562646613\n",
      "step 4856 loss 1.20334733 fisher_loss 0.222768083 triplet loss 0.980579257 l2_loss 26.2094269 fraction B 0.253117353 lossA 2.43401241 fraction A 0.0584183186\n",
      "step 4857 loss 1.1797601 fisher_loss 0.223001748 triplet loss 0.95675832 l2_loss 26.2152386 fraction B 0.26841557 lossA 2.42246413 fraction A 0.0573106296\n",
      "step 4858 loss 1.05095184 fisher_loss 0.223053649 triplet loss 0.827898145 l2_loss 26.219698 fraction B 0.122541428 lossA 2.41265345 fraction A 0.0546777472\n",
      "step 4859 loss 1.099401 fisher_loss 0.223058164 triplet loss 0.876342773 l2_loss 26.2226181 fraction B 0.173252732 lossA 2.4256103 fraction A 0.0533709414\n",
      "step 4860 loss 1.07226658 fisher_loss 0.223347291 triplet loss 0.848919272 l2_loss 26.2252636 fraction B 0.150746882 lossA 2.39538813 fraction A 0.049344644\n",
      "step 4861 loss 1.09083343 fisher_loss 0.223510966 triplet loss 0.867322445 l2_loss 26.2278481 fraction B 0.182285056 lossA 2.33399034 fraction A 0.0439686924\n",
      "step 4862 loss 1.1349299 fisher_loss 0.223656297 triplet loss 0.911273599 l2_loss 26.2296467 fraction B 0.296673775 lossA 2.29840922 fraction A 0.0413667075\n",
      "step 4863 loss 1.1711005 fisher_loss 0.223926246 triplet loss 0.947174251 l2_loss 26.2321262 fraction B 0.208959028 lossA 2.27239609 fraction A 0.0395724513\n",
      "step 4864 loss 0.937558413 fisher_loss 0.224283487 triplet loss 0.713274956 l2_loss 26.2343693 fraction B 0.239963546 lossA 2.34293318 fraction A 0.0435764\n",
      "step 4865 loss 0.922087431 fisher_loss 0.224672571 triplet loss 0.697414875 l2_loss 26.2398987 fraction B 0.13158 lossA 2.37372088 fraction A 0.046297919\n",
      "step 4866 loss 1.43010962 fisher_loss 0.22470139 triplet loss 1.20540822 l2_loss 26.2480621 fraction B 0.26583153 lossA 2.37520123 fraction A 0.0477691032\n",
      "step 4867 loss 0.951414645 fisher_loss 0.224469796 triplet loss 0.726944864 l2_loss 26.2539387 fraction B 0.115454935 lossA 2.39944339 fraction A 0.0520752929\n",
      "step 4868 loss 1.12433529 fisher_loss 0.224647403 triplet loss 0.899687946 l2_loss 26.2619495 fraction B 0.156492174 lossA 2.38452554 fraction A 0.0529126897\n",
      "step 4869 loss 1.09624135 fisher_loss 0.224594593 triplet loss 0.871646762 l2_loss 26.2690678 fraction B 0.247734457 lossA 2.39112115 fraction A 0.0554097705\n",
      "step 4870 loss 1.28023767 fisher_loss 0.224739581 triplet loss 1.05549812 l2_loss 26.2758 fraction B 0.269736558 lossA 2.3448627 fraction A 0.0523609072\n",
      "step 4871 loss 0.984607458 fisher_loss 0.224484891 triplet loss 0.760122538 l2_loss 26.280426 fraction B 0.185828447 lossA 2.27085209 fraction A 0.0452269949\n",
      "step 4872 loss 1.00063205 fisher_loss 0.2240877 triplet loss 0.776544392 l2_loss 26.2847443 fraction B 0.169060767 lossA 2.23303 fraction A 0.0400312133\n",
      "step 4873 loss 1.21124947 fisher_loss 0.223763421 triplet loss 0.987486 l2_loss 26.2887783 fraction B 0.188498139 lossA 2.2273066 fraction A 0.0380862355\n",
      "step 4874 loss 0.929586709 fisher_loss 0.223446772 triplet loss 0.706139922 l2_loss 26.2927322 fraction B 0.243857086 lossA 2.27115893 fraction A 0.0416381955\n",
      "step 4875 loss 1.38687241 fisher_loss 0.223762542 triplet loss 1.1631099 l2_loss 26.2981167 fraction B 0.175758332 lossA 2.34975934 fraction A 0.0483151227\n",
      "step 4876 loss 1.23576307 fisher_loss 0.224362746 triplet loss 1.01140034 l2_loss 26.3029137 fraction B 0.315109879 lossA 2.4129374 fraction A 0.0521329939\n",
      "step 4877 loss 1.10902083 fisher_loss 0.225048199 triplet loss 0.883972585 l2_loss 26.3061 fraction B 0.180073127 lossA 2.48499155 fraction A 0.0560952872\n",
      "step 4878 loss 1.21561754 fisher_loss 0.225931704 triplet loss 0.989685833 l2_loss 26.3096581 fraction B 0.0955580845 lossA 2.54707527 fraction A 0.058385428\n",
      "step 4879 loss 1.18533266 fisher_loss 0.22686018 triplet loss 0.958472431 l2_loss 26.3130054 fraction B 0.242754236 lossA 2.55549669 fraction A 0.0578565039\n",
      "step 4880 loss 0.997227132 fisher_loss 0.22720021 triplet loss 0.770026922 l2_loss 26.3158493 fraction B 0.157221943 lossA 2.50445533 fraction A 0.053788662\n",
      "step 4881 loss 1.23520017 fisher_loss 0.226929933 triplet loss 1.00827026 l2_loss 26.3187733 fraction B 0.273150653 lossA 2.42368197 fraction A 0.0479121469\n",
      "step 4882 loss 1.09743917 fisher_loss 0.226368532 triplet loss 0.871070623 l2_loss 26.3212395 fraction B 0.208360657 lossA 2.3376987 fraction A 0.0425248556\n",
      "step 4883 loss 1.18437552 fisher_loss 0.225683689 triplet loss 0.958691895 l2_loss 26.3246841 fraction B 0.25094381 lossA 2.25577474 fraction A 0.038155783\n",
      "step 4884 loss 0.991096735 fisher_loss 0.224865735 triplet loss 0.766231 l2_loss 26.3287487 fraction B 0.141513661 lossA 2.26525974 fraction A 0.0392353274\n",
      "step 4885 loss 1.07883728 fisher_loss 0.22456193 triplet loss 0.854275346 l2_loss 26.3351421 fraction B 0.2345431 lossA 2.26135206 fraction A 0.0396738\n",
      "step 4886 loss 1.05611598 fisher_loss 0.224234819 triplet loss 0.831881166 l2_loss 26.341259 fraction B 0.17036356 lossA 2.31150627 fraction A 0.0436829291\n",
      "step 4887 loss 0.914433599 fisher_loss 0.224134162 triplet loss 0.690299451 l2_loss 26.3479595 fraction B 0.176447958 lossA 2.42445087 fraction A 0.0545245595\n",
      "step 4888 loss 1.17173302 fisher_loss 0.224886268 triplet loss 0.946846783 l2_loss 26.3567486 fraction B 0.163057193 lossA 2.49910808 fraction A 0.061628662\n",
      "step 4889 loss 1.01082587 fisher_loss 0.225561112 triplet loss 0.78526479 l2_loss 26.3642273 fraction B 0.191725507 lossA 2.50101972 fraction A 0.0627033561\n",
      "step 4890 loss 1.26964355 fisher_loss 0.225709587 triplet loss 1.04393399 l2_loss 26.37113 fraction B 0.15971829 lossA 2.44729376 fraction A 0.0587792434\n",
      "step 4891 loss 0.957848549 fisher_loss 0.225584716 triplet loss 0.732263803 l2_loss 26.3773537 fraction B 0.13315855 lossA 2.38678241 fraction A 0.0543873124\n",
      "step 4892 loss 1.23255372 fisher_loss 0.225306347 triplet loss 1.00724733 l2_loss 26.3824711 fraction B 0.0857584 lossA 2.3444953 fraction A 0.0497723259\n",
      "step 4893 loss 1.14388025 fisher_loss 0.224953726 triplet loss 0.918926477 l2_loss 26.3866749 fraction B 0.215226188 lossA 2.29257345 fraction A 0.0445511527\n",
      "step 4894 loss 1.1475749 fisher_loss 0.224528089 triplet loss 0.923046768 l2_loss 26.3899307 fraction B 0.205856979 lossA 2.25359774 fraction A 0.0408255048\n",
      "step 4895 loss 1.00961614 fisher_loss 0.224033952 triplet loss 0.785582125 l2_loss 26.392067 fraction B 0.2083738 lossA 2.26244092 fraction A 0.0410474762\n",
      "step 4896 loss 1.05156732 fisher_loss 0.223866865 triplet loss 0.827700436 l2_loss 26.3958149 fraction B 0.203348458 lossA 2.27276826 fraction A 0.0412608273\n",
      "step 4897 loss 0.955267429 fisher_loss 0.223879129 triplet loss 0.731388271 l2_loss 26.3997 fraction B 0.15103659 lossA 2.31048751 fraction A 0.0439326502\n",
      "step 4898 loss 1.01253605 fisher_loss 0.224065557 triplet loss 0.788470447 l2_loss 26.404789 fraction B 0.127215952 lossA 2.38527441 fraction A 0.0497284606\n",
      "step 4899 loss 1.03896117 fisher_loss 0.224420175 triplet loss 0.814541 l2_loss 26.4114017 fraction B 0.175014079 lossA 2.49730897 fraction A 0.059746895\n",
      "step 4900 loss 0.861890674 fisher_loss 0.225171432 triplet loss 0.636719227 l2_loss 26.4192734 fraction B 0.0983041674 lossA 2.58286357 fraction A 0.0661148801\n",
      "step 4901 loss 1.34507751 fisher_loss 0.225621402 triplet loss 1.11945605 l2_loss 26.4261932 fraction B 0.150457948 lossA 2.55702305 fraction A 0.0644904748\n",
      "step 4902 loss 0.999846339 fisher_loss 0.225244507 triplet loss 0.774601817 l2_loss 26.4312859 fraction B 0.182456881 lossA 2.47227979 fraction A 0.0577810183\n",
      "step 4903 loss 0.911802649 fisher_loss 0.224491447 triplet loss 0.687311232 l2_loss 26.4342594 fraction B 0.122664548 lossA 2.3370645 fraction A 0.0468463302\n",
      "step 4904 loss 1.0718739 fisher_loss 0.223781064 triplet loss 0.848092854 l2_loss 26.4380932 fraction B 0.146223471 lossA 2.23723912 fraction A 0.0395602435\n",
      "step 4905 loss 1.38181496 fisher_loss 0.223487735 triplet loss 1.15832722 l2_loss 26.4429398 fraction B 0.311740816 lossA 2.17731833 fraction A 0.0365808196\n",
      "step 4906 loss 1.06904721 fisher_loss 0.223529816 triplet loss 0.845517397 l2_loss 26.447403 fraction B 0.22622703 lossA 2.19752645 fraction A 0.0382735059\n",
      "step 4907 loss 0.890726566 fisher_loss 0.224110156 triplet loss 0.66661644 l2_loss 26.4519348 fraction B 0.165267348 lossA 2.23956728 fraction A 0.0415545925\n",
      "step 4908 loss 0.876879632 fisher_loss 0.224815175 triplet loss 0.652064443 l2_loss 26.4594612 fraction B 0.116433948 lossA 2.28774881 fraction A 0.0454880521\n",
      "step 4909 loss 1.22950661 fisher_loss 0.225565553 triplet loss 1.00394106 l2_loss 26.4686661 fraction B 0.221606418 lossA 2.38258886 fraction A 0.0539684333\n",
      "step 4910 loss 0.992366076 fisher_loss 0.226606816 triplet loss 0.76575923 l2_loss 26.478981 fraction B 0.154665411 lossA 2.42258167 fraction A 0.0584181361\n",
      "step 4911 loss 1.10888147 fisher_loss 0.227135643 triplet loss 0.881745815 l2_loss 26.4898167 fraction B 0.212997064 lossA 2.4411664 fraction A 0.0611769669\n",
      "step 4912 loss 1.05657554 fisher_loss 0.227235615 triplet loss 0.82934 l2_loss 26.5001774 fraction B 0.132127658 lossA 2.40958166 fraction A 0.059520103\n",
      "step 4913 loss 1.14050639 fisher_loss 0.226859108 triplet loss 0.913647294 l2_loss 26.5089836 fraction B 0.186908886 lossA 2.36666894 fraction A 0.0557420775\n",
      "step 4914 loss 1.47303009 fisher_loss 0.226351202 triplet loss 1.24667883 l2_loss 26.516283 fraction B 0.211563677 lossA 2.31604338 fraction A 0.051437445\n",
      "step 4915 loss 1.34682977 fisher_loss 0.225716725 triplet loss 1.12111306 l2_loss 26.522356 fraction B 0.198460028 lossA 2.24212337 fraction A 0.0440368243\n",
      "step 4916 loss 1.04886937 fisher_loss 0.224878356 triplet loss 0.82399106 l2_loss 26.5253716 fraction B 0.152291805 lossA 2.22382736 fraction A 0.0420509912\n",
      "step 4917 loss 1.15504098 fisher_loss 0.224617332 triplet loss 0.930423677 l2_loss 26.5303345 fraction B 0.254571944 lossA 2.20179319 fraction A 0.03981594\n",
      "step 4918 loss 1.02391756 fisher_loss 0.224406585 triplet loss 0.799510956 l2_loss 26.5346737 fraction B 0.216124892 lossA 2.23159313 fraction A 0.0414753743\n",
      "step 4919 loss 1.00822616 fisher_loss 0.224467814 triplet loss 0.783758342 l2_loss 26.5396271 fraction B 0.18176119 lossA 2.30457044 fraction A 0.0480994545\n",
      "step 4920 loss 0.941688061 fisher_loss 0.224944383 triplet loss 0.716743648 l2_loss 26.5463028 fraction B 0.264714271 lossA 2.34698248 fraction A 0.0526943393\n",
      "step 4921 loss 1.34017384 fisher_loss 0.22539 triplet loss 1.11478388 l2_loss 26.5525951 fraction B 0.226822987 lossA 2.41791296 fraction A 0.0584152751\n",
      "step 4922 loss 1.16134286 fisher_loss 0.225909486 triplet loss 0.935433388 l2_loss 26.5574818 fraction B 0.24541077 lossA 2.48186636 fraction A 0.0624142624\n",
      "step 4923 loss 1.0570879 fisher_loss 0.226195306 triplet loss 0.830892563 l2_loss 26.5611095 fraction B 0.159362063 lossA 2.53131294 fraction A 0.065143168\n",
      "step 4924 loss 1.13819468 fisher_loss 0.226368815 triplet loss 0.911825836 l2_loss 26.5646057 fraction B 0.159073278 lossA 2.54569483 fraction A 0.0651792809\n",
      "step 4925 loss 0.938823044 fisher_loss 0.226241514 triplet loss 0.712581515 l2_loss 26.5667152 fraction B 0.176163793 lossA 2.51263952 fraction A 0.0613544583\n",
      "step 4926 loss 1.12501693 fisher_loss 0.225865066 triplet loss 0.899151802 l2_loss 26.5683765 fraction B 0.173364088 lossA 2.45245242 fraction A 0.0556410886\n",
      "step 4927 loss 1.10601342 fisher_loss 0.225476399 triplet loss 0.880537033 l2_loss 26.5698967 fraction B 0.157208011 lossA 2.36612177 fraction A 0.0479552522\n",
      "step 4928 loss 1.20649898 fisher_loss 0.224927932 triplet loss 0.981571078 l2_loss 26.5710697 fraction B 0.237243071 lossA 2.29442549 fraction A 0.042217724\n",
      "step 4929 loss 0.996500254 fisher_loss 0.224756122 triplet loss 0.771744132 l2_loss 26.5720901 fraction B 0.227807492 lossA 2.2721045 fraction A 0.0408482514\n",
      "step 4930 loss 0.991335154 fisher_loss 0.224827707 triplet loss 0.766507447 l2_loss 26.5747261 fraction B 0.120994098 lossA 2.26986909 fraction A 0.0413976051\n",
      "step 4931 loss 1.04850519 fisher_loss 0.225140512 triplet loss 0.823364675 l2_loss 26.579 fraction B 0.16219528 lossA 2.31178474 fraction A 0.0450131372\n",
      "step 4932 loss 1.44168735 fisher_loss 0.225735128 triplet loss 1.21595216 l2_loss 26.5838127 fraction B 0.241957188 lossA 2.32883811 fraction A 0.0467677973\n",
      "step 4933 loss 1.07562494 fisher_loss 0.226120159 triplet loss 0.849504828 l2_loss 26.5879116 fraction B 0.198859245 lossA 2.32727718 fraction A 0.0471684225\n",
      "step 4934 loss 1.19929838 fisher_loss 0.226241603 triplet loss 0.973056734 l2_loss 26.5917435 fraction B 0.146747023 lossA 2.33653569 fraction A 0.0491075292\n",
      "step 4935 loss 1.09902394 fisher_loss 0.22644572 triplet loss 0.872578263 l2_loss 26.5964279 fraction B 0.196713671 lossA 2.36149883 fraction A 0.0526327938\n",
      "step 4936 loss 0.926073611 fisher_loss 0.226722017 triplet loss 0.699351609 l2_loss 26.6015434 fraction B 0.123533018 lossA 2.39086795 fraction A 0.0556208044\n",
      "step 4937 loss 1.00829566 fisher_loss 0.227064237 triplet loss 0.781231463 l2_loss 26.6069336 fraction B 0.0950873792 lossA 2.43099 fraction A 0.0586869828\n",
      "step 4938 loss 1.26008391 fisher_loss 0.22738418 triplet loss 1.0326997 l2_loss 26.6122952 fraction B 0.151352808 lossA 2.47113204 fraction A 0.0604692847\n",
      "step 4939 loss 1.01232982 fisher_loss 0.227067813 triplet loss 0.785262048 l2_loss 26.6162453 fraction B 0.188298464 lossA 2.49955583 fraction A 0.0607059263\n",
      "step 4940 loss 0.904639304 fisher_loss 0.226731345 triplet loss 0.677907944 l2_loss 26.6197815 fraction B 0.185243025 lossA 2.49866581 fraction A 0.0582963228\n",
      "step 4941 loss 1.32578456 fisher_loss 0.226318017 triplet loss 1.09946656 l2_loss 26.6225395 fraction B 0.205263808 lossA 2.48298311 fraction A 0.0546672791\n",
      "step 4942 loss 1.01175594 fisher_loss 0.226143852 triplet loss 0.785612106 l2_loss 26.6245403 fraction B 0.233402699 lossA 2.45432186 fraction A 0.0507582724\n",
      "step 4943 loss 1.00524831 fisher_loss 0.226273686 triplet loss 0.778974652 l2_loss 26.6264496 fraction B 0.163705587 lossA 2.41101027 fraction A 0.0471618287\n",
      "step 4944 loss 1.00276208 fisher_loss 0.226295337 triplet loss 0.776466787 l2_loss 26.6301098 fraction B 0.169403255 lossA 2.32715535 fraction A 0.0417648666\n",
      "step 4945 loss 0.937282503 fisher_loss 0.226113439 triplet loss 0.711169064 l2_loss 26.6340351 fraction B 0.142112419 lossA 2.31474447 fraction A 0.0413740613\n",
      "step 4946 loss 1.22662747 fisher_loss 0.22589007 triplet loss 1.00073743 l2_loss 26.6417885 fraction B 0.218107775 lossA 2.32986307 fraction A 0.0427153148\n",
      "step 4947 loss 1.00026155 fisher_loss 0.225721166 triplet loss 0.774540424 l2_loss 26.6498165 fraction B 0.217087969 lossA 2.39001775 fraction A 0.0485054\n",
      "step 4948 loss 0.922108114 fisher_loss 0.225825235 triplet loss 0.696282864 l2_loss 26.6597691 fraction B 0.170072928 lossA 2.46152925 fraction A 0.0560616\n",
      "step 4949 loss 0.947738588 fisher_loss 0.226665244 triplet loss 0.721073329 l2_loss 26.6703911 fraction B 0.142022863 lossA 2.54309726 fraction A 0.0634093359\n",
      "step 4950 loss 0.816850305 fisher_loss 0.227711827 triplet loss 0.589138508 l2_loss 26.6805115 fraction B 0.0573706962 lossA 2.61834645 fraction A 0.0705987066\n",
      "Saved checkpoint for step .tf2/EWC/Pad-IMS/embedding/fisher/ckpt-1\n"
     ]
    }
   ],
   "source": [
    "conv_net.unfreeze()\n",
    "train_emb_EWC = Train_Embeddings_EWC(embedding, fisher_matrix_TL , learning_rate, int(num_batches*training_iters),\n",
    "                                     batch_size, display_step, \"batch_all\", margin, squared,lamb, filepath, restore=False)\n",
    "\n",
    "idx = np.random.choice(X_test_Pad.shape[0], 500)\n",
    "train_emb_EWC.fit(X_train_IMS, y_train_IMS, X_test_Pad[idx,:], y_test_Pad[idx], save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgFmK6aGv6BO"
   },
   "source": [
    "We observe that at the end of embeddings training, the fraction of positive triplets is very low for both tasks (Paderborn and IMS). This means that the network has learnt to produce good embeddings for both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXPO22vB_KYX"
   },
   "source": [
    "## Classifier EWC\n",
    "\n",
    "Now try to train the classifier using EWC (without training the embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1585909050651,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "E125s4ANcd59",
    "outputId": "5c73d7c3-1616-4351-cc13-095f63b3b1dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n"
     ]
    }
   ],
   "source": [
    "training_iters = 10000\n",
    "learning_rate = 0.00001\n",
    "print(int(num_batches*training_iters))\n",
    "filepath =\"Weights/EWC/Pad-IMS/fisher/classifier/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 198012,
     "status": "ok",
     "timestamp": 1585909254830,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "72hIrbH8AymN",
    "outputId": "180097fe-8e01-42b6-f27f-84051a2fe9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16 loss 0.775520205 fisher_loss 0.00113950553 l2_loss 0.000137006646 accuracy B 0.795399189 accuracy A 0.983347237\n",
      "step 32 loss 0.436623156 fisher_loss 0.00433566282 l2_loss 0.000538415 accuracy B 0.802977 accuracy A 0.985845149\n",
      "step 48 loss 0.556738317 fisher_loss 0.00872853771 l2_loss 0.0011557024 accuracy B 0.808660328 accuracy A 0.984179854\n",
      "step 64 loss 0.572201669 fisher_loss 0.0133826341 l2_loss 0.00193439797 accuracy B 0.811096072 accuracy A 0.981681943\n",
      "step 80 loss 0.553531349 fisher_loss 0.0176187046 l2_loss 0.00278328103 accuracy B 0.814614356 accuracy A 0.979184031\n",
      "step 96 loss 0.44560954 fisher_loss 0.0210323911 l2_loss 0.00363906147 accuracy B 0.820568323 accuracy A 0.974188149\n",
      "step 112 loss 0.388815552 fisher_loss 0.0238844454 l2_loss 0.00452722609 accuracy B 0.825439811 accuracy A 0.973355532\n",
      "step 128 loss 0.502016604 fisher_loss 0.0252305511 l2_loss 0.00527583621 accuracy B 0.829228699 accuracy A 0.969192326\n",
      "step 144 loss 0.70135355 fisher_loss 0.0280521214 l2_loss 0.00621526921 accuracy B 0.834912062 accuracy A 0.96502912\n",
      "step 160 loss 0.474875867 fisher_loss 0.0293149278 l2_loss 0.00703558838 accuracy B 0.83978349 accuracy A 0.96502912\n",
      "step 176 loss 0.676531136 fisher_loss 0.0318369418 l2_loss 0.00799163803 accuracy B 0.843301773 accuracy A 0.963363886\n",
      "step 192 loss 0.518312871 fisher_loss 0.0326935649 l2_loss 0.00878468622 accuracy B 0.844384313 accuracy A 0.961698592\n",
      "step 208 loss 0.445006371 fisher_loss 0.0321441442 l2_loss 0.00946753565 accuracy B 0.845737457 accuracy A 0.95920068\n",
      "step 224 loss 0.763721 fisher_loss 0.0338452 l2_loss 0.0104528144 accuracy B 0.849797 accuracy A 0.957535386\n",
      "step 240 loss 0.690030456 fisher_loss 0.0340041779 l2_loss 0.0111542009 accuracy B 0.850067675 accuracy A 0.955870092\n",
      "step 256 loss 0.491140068 fisher_loss 0.0348987505 l2_loss 0.0119001698 accuracy B 0.85087955 accuracy A 0.95337218\n",
      "step 272 loss 0.692781091 fisher_loss 0.0363539159 l2_loss 0.0128209861 accuracy B 0.852232754 accuracy A 0.950041652\n",
      "step 288 loss 0.427020222 fisher_loss 0.036707975 l2_loss 0.0136456462 accuracy B 0.853856564 accuracy A 0.946711063\n",
      "step 304 loss 0.653144956 fisher_loss 0.0358872786 l2_loss 0.0142789641 accuracy B 0.852774 accuracy A 0.945878446\n",
      "step 320 loss 0.37948674 fisher_loss 0.0372146815 l2_loss 0.0152584407 accuracy B 0.858186722 accuracy A 0.944213152\n",
      "step 336 loss 0.419659883 fisher_loss 0.0379988588 l2_loss 0.0161759071 accuracy B 0.8614344 accuracy A 0.94171524\n",
      "step 352 loss 0.393076152 fisher_loss 0.0384889133 l2_loss 0.0170479901 accuracy B 0.8614344 accuracy A 0.940882623\n",
      "step 368 loss 0.560656548 fisher_loss 0.039591413 l2_loss 0.0179761946 accuracy B 0.8614344 accuracy A 0.940049946\n",
      "step 384 loss 0.560760081 fisher_loss 0.0393442661 l2_loss 0.0186368376 accuracy B 0.861705 accuracy A 0.939217329\n",
      "step 400 loss 0.498914748 fisher_loss 0.0392280705 l2_loss 0.0192982815 accuracy B 0.8614344 accuracy A 0.938384652\n",
      "step 416 loss 0.543314278 fisher_loss 0.0395087749 l2_loss 0.0200695898 accuracy B 0.861705 accuracy A 0.938384652\n",
      "step 432 loss 0.370262116 fisher_loss 0.0396294184 l2_loss 0.0207761489 accuracy B 0.8614344 accuracy A 0.937552035\n",
      "step 448 loss 0.612694681 fisher_loss 0.0404996127 l2_loss 0.0216421 accuracy B 0.86197567 accuracy A 0.935054123\n",
      "step 464 loss 0.587550819 fisher_loss 0.042675335 l2_loss 0.0227549952 accuracy B 0.863328815 accuracy A 0.9300583\n",
      "step 480 loss 0.368246526 fisher_loss 0.0433267653 l2_loss 0.0236965586 accuracy B 0.864411354 accuracy A 0.927560389\n",
      "step 496 loss 0.472844332 fisher_loss 0.0429711454 l2_loss 0.0243221 accuracy B 0.864952624 accuracy A 0.927560389\n",
      "step 512 loss 0.503567696 fisher_loss 0.0434879214 l2_loss 0.0251389928 accuracy B 0.864952624 accuracy A 0.925895095\n",
      "step 528 loss 0.482095063 fisher_loss 0.0427764542 l2_loss 0.0257238299 accuracy B 0.864682 accuracy A 0.925062478\n",
      "step 544 loss 0.343383878 fisher_loss 0.0431030802 l2_loss 0.026526019 accuracy B 0.865223289 accuracy A 0.925062478\n",
      "step 560 loss 0.437605798 fisher_loss 0.0431144312 l2_loss 0.0272608921 accuracy B 0.865493894 accuracy A 0.925062478\n",
      "step 576 loss 0.348819256 fisher_loss 0.0443026908 l2_loss 0.0281686671 accuracy B 0.866576433 accuracy A 0.923397183\n",
      "step 592 loss 0.362318635 fisher_loss 0.0438263379 l2_loss 0.0287714377 accuracy B 0.866576433 accuracy A 0.922564507\n",
      "step 608 loss 0.495786756 fisher_loss 0.044078324 l2_loss 0.0294937715 accuracy B 0.867659 accuracy A 0.920899272\n",
      "step 624 loss 0.383167058 fisher_loss 0.0451237336 l2_loss 0.0302851219 accuracy B 0.866305828 accuracy A 0.919234\n",
      "step 640 loss 0.379586 fisher_loss 0.0448207073 l2_loss 0.0309689529 accuracy B 0.867388368 accuracy A 0.919234\n",
      "step 656 loss 0.870590925 fisher_loss 0.0441091917 l2_loss 0.0314838476 accuracy B 0.865493894 accuracy A 0.919234\n",
      "step 672 loss 0.395696878 fisher_loss 0.04364454 l2_loss 0.0319223516 accuracy B 0.863870084 accuracy A 0.919234\n",
      "step 688 loss 0.435617447 fisher_loss 0.0441506021 l2_loss 0.0325549357 accuracy B 0.864411354 accuracy A 0.919234\n",
      "step 704 loss 0.433722436 fisher_loss 0.0458683483 l2_loss 0.0336375 accuracy B 0.866847098 accuracy A 0.915903389\n",
      "step 720 loss 0.559113204 fisher_loss 0.0456526056 l2_loss 0.0343009457 accuracy B 0.867117703 accuracy A 0.916736066\n",
      "step 736 loss 0.608597934 fisher_loss 0.0446673483 l2_loss 0.0347410068 accuracy B 0.865493894 accuracy A 0.916736066\n",
      "step 752 loss 0.593741715 fisher_loss 0.0457724743 l2_loss 0.0356172 accuracy B 0.867659 accuracy A 0.915070772\n",
      "step 768 loss 0.566947043 fisher_loss 0.0462015718 l2_loss 0.0364023745 accuracy B 0.867659 accuracy A 0.915070772\n",
      "step 784 loss 0.611555696 fisher_loss 0.0458593331 l2_loss 0.0369745381 accuracy B 0.868470907 accuracy A 0.915070772\n",
      "step 800 loss 0.415697396 fisher_loss 0.0471736118 l2_loss 0.0377646908 accuracy B 0.867117703 accuracy A 0.912572861\n",
      "step 816 loss 0.537510037 fisher_loss 0.0484417975 l2_loss 0.0386160128 accuracy B 0.866035163 accuracy A 0.911740243\n",
      "step 832 loss 0.481016606 fisher_loss 0.0480556302 l2_loss 0.0391588286 accuracy B 0.866035163 accuracy A 0.911740243\n",
      "step 848 loss 0.484298 fisher_loss 0.0480341129 l2_loss 0.0398279652 accuracy B 0.866847098 accuracy A 0.911740243\n",
      "step 864 loss 0.401686221 fisher_loss 0.0462255776 l2_loss 0.0400496125 accuracy B 0.868200243 accuracy A 0.912572861\n",
      "step 880 loss 0.442496181 fisher_loss 0.0457520448 l2_loss 0.0404551476 accuracy B 0.867929637 accuracy A 0.912572861\n",
      "step 896 loss 0.460098147 fisher_loss 0.0482896455 l2_loss 0.0416979492 accuracy B 0.867117703 accuracy A 0.910074949\n",
      "step 912 loss 0.643733203 fisher_loss 0.0479737185 l2_loss 0.0423509516 accuracy B 0.867659 accuracy A 0.910907567\n",
      "step 928 loss 0.426967859 fisher_loss 0.0475644022 l2_loss 0.0427304469 accuracy B 0.868470907 accuracy A 0.910907567\n",
      "step 944 loss 0.558304727 fisher_loss 0.0480195209 l2_loss 0.0432939194 accuracy B 0.867117703 accuracy A 0.910074949\n",
      "step 960 loss 0.525912106 fisher_loss 0.0485657789 l2_loss 0.0439820774 accuracy B 0.866847098 accuracy A 0.909242272\n",
      "step 976 loss 0.335121155 fisher_loss 0.049078472 l2_loss 0.0447878577 accuracy B 0.867388368 accuracy A 0.909242272\n",
      "step 992 loss 0.392457426 fisher_loss 0.0497857593 l2_loss 0.0455768108 accuracy B 0.868741572 accuracy A 0.908409655\n",
      "step 1008 loss 0.487755299 fisher_loss 0.0485343337 l2_loss 0.0458310954 accuracy B 0.867929637 accuracy A 0.909242272\n",
      "step 1024 loss 0.447748125 fisher_loss 0.048239518 l2_loss 0.0462714434 accuracy B 0.867388368 accuracy A 0.909242272\n",
      "step 1040 loss 0.857193589 fisher_loss 0.0470955819 l2_loss 0.0465719514 accuracy B 0.868470907 accuracy A 0.910907567\n",
      "step 1056 loss 0.542612 fisher_loss 0.0469523817 l2_loss 0.0469836928 accuracy B 0.868470907 accuracy A 0.910907567\n",
      "step 1072 loss 0.595428884 fisher_loss 0.0462884344 l2_loss 0.0472826399 accuracy B 0.867659 accuracy A 0.910907567\n",
      "step 1088 loss 0.665004849 fisher_loss 0.0465480164 l2_loss 0.0478908 accuracy B 0.867929637 accuracy A 0.910907567\n",
      "step 1104 loss 0.437757581 fisher_loss 0.0464027 l2_loss 0.0483905412 accuracy B 0.868200243 accuracy A 0.910907567\n",
      "step 1120 loss 0.567609906 fisher_loss 0.0485242568 l2_loss 0.0495091937 accuracy B 0.868470907 accuracy A 0.908409655\n",
      "step 1136 loss 0.699077368 fisher_loss 0.0500027426 l2_loss 0.0504741557 accuracy B 0.867659 accuracy A 0.904246449\n",
      "step 1152 loss 0.528746903 fisher_loss 0.0497613028 l2_loss 0.0510066599 accuracy B 0.868470907 accuracy A 0.904246449\n",
      "step 1168 loss 0.671987534 fisher_loss 0.0483724885 l2_loss 0.0512831658 accuracy B 0.869824111 accuracy A 0.908409655\n",
      "step 1184 loss 0.503712714 fisher_loss 0.0497080646 l2_loss 0.0520116873 accuracy B 0.868741572 accuracy A 0.904246449\n",
      "step 1200 loss 0.454479218 fisher_loss 0.0509026572 l2_loss 0.0528605804 accuracy B 0.868470907 accuracy A 0.903413832\n",
      "step 1216 loss 0.317117125 fisher_loss 0.0490768775 l2_loss 0.0530034639 accuracy B 0.868741572 accuracy A 0.903413832\n",
      "step 1232 loss 0.740171373 fisher_loss 0.0471894741 l2_loss 0.0529269204 accuracy B 0.868470907 accuracy A 0.908409655\n",
      "step 1248 loss 0.572750807 fisher_loss 0.0482966602 l2_loss 0.0535558052 accuracy B 0.868741572 accuracy A 0.904246449\n",
      "step 1264 loss 0.466032565 fisher_loss 0.0500841588 l2_loss 0.0543379821 accuracy B 0.866847098 accuracy A 0.903413832\n",
      "step 1280 loss 0.52623713 fisher_loss 0.0489579923 l2_loss 0.0546294041 accuracy B 0.868200243 accuracy A 0.903413832\n",
      "step 1296 loss 0.397742271 fisher_loss 0.0485681146 l2_loss 0.0550780445 accuracy B 0.868741572 accuracy A 0.903413832\n",
      "step 1312 loss 0.471056163 fisher_loss 0.0498102196 l2_loss 0.0559439622 accuracy B 0.867929637 accuracy A 0.903413832\n",
      "step 1328 loss 0.411598086 fisher_loss 0.0505056903 l2_loss 0.0567059182 accuracy B 0.868200243 accuracy A 0.903413832\n",
      "step 1344 loss 0.534359634 fisher_loss 0.0502384491 l2_loss 0.0571922436 accuracy B 0.868470907 accuracy A 0.903413832\n",
      "step 1360 loss 0.753853798 fisher_loss 0.0491974391 l2_loss 0.0572641 accuracy B 0.869553447 accuracy A 0.903413832\n",
      "step 1376 loss 0.498543739 fisher_loss 0.0504168943 l2_loss 0.0578867495 accuracy B 0.867929637 accuracy A 0.900915921\n",
      "step 1392 loss 0.37335065 fisher_loss 0.0498533919 l2_loss 0.0581908785 accuracy B 0.868470907 accuracy A 0.902581155\n",
      "step 1408 loss 0.705231607 fisher_loss 0.0490845479 l2_loss 0.0584129281 accuracy B 0.868470907 accuracy A 0.903413832\n",
      "step 1424 loss 0.350065649 fisher_loss 0.0486043431 l2_loss 0.0585578457 accuracy B 0.868200243 accuracy A 0.903413832\n",
      "step 1440 loss 0.493477583 fisher_loss 0.0471797734 l2_loss 0.0585564449 accuracy B 0.867929637 accuracy A 0.904246449\n",
      "step 1456 loss 0.303296328 fisher_loss 0.0469535068 l2_loss 0.0589914732 accuracy B 0.867388368 accuracy A 0.905911744\n",
      "step 1472 loss 0.460048527 fisher_loss 0.0488453433 l2_loss 0.0599310398 accuracy B 0.868470907 accuracy A 0.903413832\n",
      "step 1488 loss 0.409917563 fisher_loss 0.0493375883 l2_loss 0.0604112335 accuracy B 0.867388368 accuracy A 0.900915921\n",
      "step 1504 loss 0.579186797 fisher_loss 0.0504575595 l2_loss 0.0610461943 accuracy B 0.866576433 accuracy A 0.899250627\n",
      "step 1520 loss 0.491266489 fisher_loss 0.049409382 l2_loss 0.0612305664 accuracy B 0.867659 accuracy A 0.900915921\n",
      "step 1536 loss 0.513713717 fisher_loss 0.0501486138 l2_loss 0.0619796067 accuracy B 0.868470907 accuracy A 0.900083244\n",
      "step 1552 loss 0.599681318 fisher_loss 0.0503554381 l2_loss 0.0627591163 accuracy B 0.869824111 accuracy A 0.901748538\n",
      "step 1568 loss 0.461595803 fisher_loss 0.0499058738 l2_loss 0.0632103235 accuracy B 0.870906651 accuracy A 0.901748538\n",
      "step 1584 loss 0.531289637 fisher_loss 0.0502687581 l2_loss 0.0636254773 accuracy B 0.868741572 accuracy A 0.900083244\n",
      "step 1600 loss 0.437080026 fisher_loss 0.0516942255 l2_loss 0.0643881261 accuracy B 0.869553447 accuracy A 0.898418\n",
      "step 1616 loss 0.491402954 fisher_loss 0.0506186 l2_loss 0.0645173565 accuracy B 0.868200243 accuracy A 0.900083244\n",
      "step 1632 loss 0.433177024 fisher_loss 0.0484560765 l2_loss 0.064293921 accuracy B 0.869012177 accuracy A 0.903413832\n",
      "step 1648 loss 0.368725389 fisher_loss 0.0490208529 l2_loss 0.0649267063 accuracy B 0.869282842 accuracy A 0.903413832\n",
      "step 1664 loss 0.599183917 fisher_loss 0.0482281744 l2_loss 0.0650265142 accuracy B 0.868470907 accuracy A 0.903413832\n",
      "step 1680 loss 0.385937691 fisher_loss 0.0483636372 l2_loss 0.0653704256 accuracy B 0.868470907 accuracy A 0.903413832\n",
      "step 1696 loss 0.368707269 fisher_loss 0.0497857295 l2_loss 0.0662825257 accuracy B 0.870906651 accuracy A 0.901748538\n",
      "step 1712 loss 0.392454654 fisher_loss 0.0520783439 l2_loss 0.0674522221 accuracy B 0.870094717 accuracy A 0.897585332\n",
      "step 1728 loss 0.627513647 fisher_loss 0.0512774065 l2_loss 0.0678300187 accuracy B 0.870365381 accuracy A 0.899250627\n",
      "step 1744 loss 0.427877933 fisher_loss 0.0520742722 l2_loss 0.0684350654 accuracy B 0.870365381 accuracy A 0.897585332\n",
      "step 1760 loss 0.490879565 fisher_loss 0.0516487956 l2_loss 0.0686547384 accuracy B 0.869012177 accuracy A 0.897585332\n",
      "step 1776 loss 0.438411802 fisher_loss 0.0507397577 l2_loss 0.0687788948 accuracy B 0.869282842 accuracy A 0.899250627\n",
      "step 1792 loss 0.466175646 fisher_loss 0.0503178127 l2_loss 0.0690092072 accuracy B 0.869553447 accuracy A 0.899250627\n",
      "step 1808 loss 0.378880441 fisher_loss 0.0497507 l2_loss 0.0692983568 accuracy B 0.869824111 accuracy A 0.899250627\n",
      "step 1824 loss 0.484721 fisher_loss 0.0504746065 l2_loss 0.0697675273 accuracy B 0.868741572 accuracy A 0.897585332\n",
      "step 1840 loss 0.333971739 fisher_loss 0.0504898168 l2_loss 0.0700737 accuracy B 0.868200243 accuracy A 0.897585332\n",
      "step 1856 loss 0.535288095 fisher_loss 0.0509135537 l2_loss 0.0706736743 accuracy B 0.868470907 accuracy A 0.897585332\n",
      "step 1872 loss 0.586917102 fisher_loss 0.0511957929 l2_loss 0.0711842701 accuracy B 0.868470907 accuracy A 0.896752715\n",
      "step 1888 loss 0.628529549 fisher_loss 0.0519237742 l2_loss 0.0717053488 accuracy B 0.869012177 accuracy A 0.895087421\n",
      "step 1904 loss 0.361902207 fisher_loss 0.0528664961 l2_loss 0.0722556859 accuracy B 0.870365381 accuracy A 0.894254804\n",
      "step 1920 loss 0.43503654 fisher_loss 0.0511828773 l2_loss 0.0721393749 accuracy B 0.868200243 accuracy A 0.897585332\n",
      "step 1936 loss 0.364795744 fisher_loss 0.0512918979 l2_loss 0.0724771321 accuracy B 0.868200243 accuracy A 0.896752715\n",
      "step 1952 loss 0.523352504 fisher_loss 0.050004553 l2_loss 0.0722997859 accuracy B 0.867929637 accuracy A 0.898418\n",
      "step 1968 loss 0.590620339 fisher_loss 0.0500038899 l2_loss 0.0724728107 accuracy B 0.867117703 accuracy A 0.897585332\n",
      "step 1984 loss 0.453540981 fisher_loss 0.0506971292 l2_loss 0.0731605589 accuracy B 0.867929637 accuracy A 0.897585332\n",
      "step 2000 loss 0.303435445 fisher_loss 0.0511445962 l2_loss 0.0739958361 accuracy B 0.869824111 accuracy A 0.897585332\n",
      "step 2016 loss 0.374022752 fisher_loss 0.0511312895 l2_loss 0.0743798837 accuracy B 0.869553447 accuracy A 0.897585332\n",
      "step 2032 loss 0.401675761 fisher_loss 0.0512372777 l2_loss 0.0747362822 accuracy B 0.869824111 accuracy A 0.897585332\n",
      "step 2048 loss 0.458548397 fisher_loss 0.0513391905 l2_loss 0.075372085 accuracy B 0.870636 accuracy A 0.896752715\n",
      "step 2064 loss 0.416668475 fisher_loss 0.0502856299 l2_loss 0.0754464 accuracy B 0.869824111 accuracy A 0.900083244\n",
      "step 2080 loss 0.418771416 fisher_loss 0.0520513281 l2_loss 0.0762658194 accuracy B 0.870094717 accuracy A 0.895087421\n",
      "step 2096 loss 0.529207885 fisher_loss 0.0525882207 l2_loss 0.0767381862 accuracy B 0.870094717 accuracy A 0.894254804\n",
      "step 2112 loss 0.475147 fisher_loss 0.0530168265 l2_loss 0.0770880505 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 2128 loss 0.413674265 fisher_loss 0.0515154637 l2_loss 0.0768399313 accuracy B 0.868470907 accuracy A 0.895087421\n",
      "step 2144 loss 0.478522688 fisher_loss 0.0497327149 l2_loss 0.0765649 accuracy B 0.869553447 accuracy A 0.899250627\n",
      "step 2160 loss 0.640675068 fisher_loss 0.050359752 l2_loss 0.0771353915 accuracy B 0.870094717 accuracy A 0.898418\n",
      "step 2176 loss 0.574082911 fisher_loss 0.0492531285 l2_loss 0.0770492107 accuracy B 0.868741572 accuracy A 0.900083244\n",
      "step 2192 loss 0.433684558 fisher_loss 0.0505105741 l2_loss 0.077505894 accuracy B 0.868741572 accuracy A 0.897585332\n",
      "step 2208 loss 0.595936418 fisher_loss 0.0513909273 l2_loss 0.0781200305 accuracy B 0.868741572 accuracy A 0.895087421\n",
      "step 2224 loss 0.735949636 fisher_loss 0.050863158 l2_loss 0.0784891 accuracy B 0.869824111 accuracy A 0.897585332\n",
      "step 2240 loss 0.43833077 fisher_loss 0.0508059971 l2_loss 0.078791894 accuracy B 0.869824111 accuracy A 0.896752715\n",
      "step 2256 loss 0.39692086 fisher_loss 0.0517812409 l2_loss 0.0793820173 accuracy B 0.869282842 accuracy A 0.895087421\n",
      "step 2272 loss 0.607802629 fisher_loss 0.0517784469 l2_loss 0.0796792805 accuracy B 0.869282842 accuracy A 0.895920038\n",
      "step 2288 loss 0.833057165 fisher_loss 0.050215371 l2_loss 0.0795730725 accuracy B 0.871177256 accuracy A 0.898418\n",
      "step 2304 loss 0.706977844 fisher_loss 0.0514170267 l2_loss 0.0803195685 accuracy B 0.870094717 accuracy A 0.896752715\n",
      "step 2320 loss 0.575192451 fisher_loss 0.0511060245 l2_loss 0.0804168954 accuracy B 0.870094717 accuracy A 0.896752715\n",
      "step 2336 loss 0.335691333 fisher_loss 0.0503028072 l2_loss 0.0803252533 accuracy B 0.870365381 accuracy A 0.897585332\n",
      "step 2352 loss 0.390209407 fisher_loss 0.051312793 l2_loss 0.0808212832 accuracy B 0.869012177 accuracy A 0.895087421\n",
      "step 2368 loss 0.506561041 fisher_loss 0.0524371974 l2_loss 0.0815850943 accuracy B 0.869553447 accuracy A 0.893422127\n",
      "step 2384 loss 0.489854336 fisher_loss 0.0518439636 l2_loss 0.0818056241 accuracy B 0.868470907 accuracy A 0.895087421\n",
      "step 2400 loss 0.610294938 fisher_loss 0.050487414 l2_loss 0.0817540213 accuracy B 0.870094717 accuracy A 0.896752715\n",
      "step 2416 loss 0.560896814 fisher_loss 0.0524843894 l2_loss 0.0828105286 accuracy B 0.870094717 accuracy A 0.894254804\n",
      "step 2432 loss 0.436710179 fisher_loss 0.0511844195 l2_loss 0.0828116238 accuracy B 0.871177256 accuracy A 0.895920038\n",
      "step 2448 loss 0.383157611 fisher_loss 0.0500050597 l2_loss 0.0826215521 accuracy B 0.869282842 accuracy A 0.897585332\n",
      "step 2464 loss 0.542867243 fisher_loss 0.048772309 l2_loss 0.0822273418 accuracy B 0.868470907 accuracy A 0.899250627\n",
      "step 2480 loss 0.462340325 fisher_loss 0.0507180355 l2_loss 0.0831380934 accuracy B 0.869824111 accuracy A 0.895087421\n",
      "step 2496 loss 0.554410219 fisher_loss 0.0505573228 l2_loss 0.0832584575 accuracy B 0.869553447 accuracy A 0.895087421\n",
      "step 2512 loss 0.355934 fisher_loss 0.0505731367 l2_loss 0.0835948735 accuracy B 0.870365381 accuracy A 0.896752715\n",
      "step 2528 loss 0.51056695 fisher_loss 0.0517711788 l2_loss 0.0843826309 accuracy B 0.869282842 accuracy A 0.894254804\n",
      "step 2544 loss 0.459406555 fisher_loss 0.0517260395 l2_loss 0.084721677 accuracy B 0.869012177 accuracy A 0.894254804\n",
      "step 2560 loss 0.350191683 fisher_loss 0.0514600649 l2_loss 0.0849861801 accuracy B 0.870094717 accuracy A 0.894254804\n",
      "step 2576 loss 0.505188882 fisher_loss 0.0510329455 l2_loss 0.0852640048 accuracy B 0.871718526 accuracy A 0.895087421\n",
      "step 2592 loss 0.502873719 fisher_loss 0.0522204526 l2_loss 0.0861217752 accuracy B 0.871447921 accuracy A 0.894254804\n",
      "step 2608 loss 0.665501297 fisher_loss 0.0534821302 l2_loss 0.0868619084 accuracy B 0.871447921 accuracy A 0.891756892\n",
      "step 2624 loss 0.531162739 fisher_loss 0.0553656481 l2_loss 0.0874705687 accuracy B 0.871989191 accuracy A 0.889258921\n",
      "step 2640 loss 0.459764898 fisher_loss 0.0527440868 l2_loss 0.0870775804 accuracy B 0.870365381 accuracy A 0.893422127\n",
      "step 2656 loss 0.492802382 fisher_loss 0.0501056425 l2_loss 0.0863031223 accuracy B 0.869012177 accuracy A 0.895920038\n",
      "step 2672 loss 0.344352037 fisher_loss 0.0516289137 l2_loss 0.0870054215 accuracy B 0.869824111 accuracy A 0.894254804\n",
      "step 2688 loss 0.417472214 fisher_loss 0.0529904701 l2_loss 0.0877174959 accuracy B 0.869282842 accuracy A 0.892589509\n",
      "step 2704 loss 0.507388949 fisher_loss 0.0525906 l2_loss 0.0878999457 accuracy B 0.869553447 accuracy A 0.892589509\n",
      "step 2720 loss 0.557110667 fisher_loss 0.0533233732 l2_loss 0.0883779824 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 2736 loss 0.498625934 fisher_loss 0.0531263277 l2_loss 0.0884982646 accuracy B 0.870094717 accuracy A 0.892589509\n",
      "step 2752 loss 0.55762 fisher_loss 0.0515620112 l2_loss 0.0883680359 accuracy B 0.869282842 accuracy A 0.894254804\n",
      "step 2768 loss 0.448507309 fisher_loss 0.0514874831 l2_loss 0.0884887502 accuracy B 0.868470907 accuracy A 0.894254804\n",
      "step 2784 loss 0.534327 fisher_loss 0.0533605926 l2_loss 0.0893523768 accuracy B 0.869824111 accuracy A 0.891756892\n",
      "step 2800 loss 0.415186852 fisher_loss 0.0538682677 l2_loss 0.0899311081 accuracy B 0.870906651 accuracy A 0.891756892\n",
      "step 2816 loss 0.519319534 fisher_loss 0.0518900976 l2_loss 0.0897319689 accuracy B 0.869824111 accuracy A 0.894254804\n",
      "step 2832 loss 0.380496204 fisher_loss 0.0521534607 l2_loss 0.0899913 accuracy B 0.869553447 accuracy A 0.894254804\n",
      "step 2848 loss 0.648433745 fisher_loss 0.0518560782 l2_loss 0.090018 accuracy B 0.869553447 accuracy A 0.894254804\n",
      "step 2864 loss 0.514402866 fisher_loss 0.0511382 l2_loss 0.0900355279 accuracy B 0.869282842 accuracy A 0.895920038\n",
      "step 2880 loss 0.387906432 fisher_loss 0.0517936461 l2_loss 0.0905468464 accuracy B 0.869282842 accuracy A 0.894254804\n",
      "step 2896 loss 0.40513131 fisher_loss 0.0516968146 l2_loss 0.0906111151 accuracy B 0.868741572 accuracy A 0.894254804\n",
      "step 2912 loss 0.402043551 fisher_loss 0.0508638695 l2_loss 0.0903613269 accuracy B 0.869012177 accuracy A 0.895087421\n",
      "step 2928 loss 0.412323475 fisher_loss 0.0504520945 l2_loss 0.0903020576 accuracy B 0.869282842 accuracy A 0.895087421\n",
      "step 2944 loss 0.525805473 fisher_loss 0.0493822172 l2_loss 0.0902615786 accuracy B 0.869012177 accuracy A 0.898418\n",
      "step 2960 loss 0.525330424 fisher_loss 0.0505094044 l2_loss 0.0909755751 accuracy B 0.870365381 accuracy A 0.895087421\n",
      "step 2976 loss 0.488188565 fisher_loss 0.0508214235 l2_loss 0.0911196247 accuracy B 0.869824111 accuracy A 0.895087421\n",
      "step 2992 loss 0.356405616 fisher_loss 0.0509674214 l2_loss 0.0913658813 accuracy B 0.870094717 accuracy A 0.895087421\n",
      "step 3008 loss 0.421069711 fisher_loss 0.0507148877 l2_loss 0.0916692764 accuracy B 0.870636 accuracy A 0.895920038\n",
      "step 3024 loss 0.49689278 fisher_loss 0.0499131382 l2_loss 0.0914633572 accuracy B 0.869282842 accuracy A 0.896752715\n",
      "step 3040 loss 0.490715951 fisher_loss 0.0502768755 l2_loss 0.0917316303 accuracy B 0.870094717 accuracy A 0.895920038\n",
      "step 3056 loss 0.430480391 fisher_loss 0.0509810224 l2_loss 0.0923438594 accuracy B 0.870906651 accuracy A 0.895087421\n",
      "step 3072 loss 0.315749258 fisher_loss 0.0516405292 l2_loss 0.0930164382 accuracy B 0.871718526 accuracy A 0.894254804\n",
      "step 3088 loss 0.388327748 fisher_loss 0.0556268655 l2_loss 0.0944315344 accuracy B 0.871989191 accuracy A 0.889258921\n",
      "step 3104 loss 0.888757885 fisher_loss 0.0538969375 l2_loss 0.0942513645 accuracy B 0.871177256 accuracy A 0.892589509\n",
      "step 3120 loss 0.491949409 fisher_loss 0.0527232178 l2_loss 0.0942223668 accuracy B 0.869824111 accuracy A 0.892589509\n",
      "step 3136 loss 0.385163456 fisher_loss 0.0525766797 l2_loss 0.0940863639 accuracy B 0.869553447 accuracy A 0.893422127\n",
      "step 3152 loss 0.529100835 fisher_loss 0.0535874367 l2_loss 0.0945344865 accuracy B 0.870906651 accuracy A 0.890924215\n",
      "step 3168 loss 0.457134545 fisher_loss 0.0544703528 l2_loss 0.0953261852 accuracy B 0.871447921 accuracy A 0.890091598\n",
      "step 3184 loss 0.609195471 fisher_loss 0.0526454449 l2_loss 0.09516792 accuracy B 0.870094717 accuracy A 0.893422127\n",
      "step 3200 loss 0.427017897 fisher_loss 0.0505215488 l2_loss 0.0944799334 accuracy B 0.869553447 accuracy A 0.895087421\n",
      "step 3216 loss 0.443666667 fisher_loss 0.0498314314 l2_loss 0.094431974 accuracy B 0.869553447 accuracy A 0.897585332\n",
      "step 3232 loss 0.411468118 fisher_loss 0.0507836714 l2_loss 0.0951193795 accuracy B 0.871447921 accuracy A 0.895087421\n",
      "step 3248 loss 0.455538899 fisher_loss 0.0524549447 l2_loss 0.0960603878 accuracy B 0.871447921 accuracy A 0.893422127\n",
      "step 3264 loss 0.549529076 fisher_loss 0.0522234328 l2_loss 0.0960173681 accuracy B 0.870906651 accuracy A 0.893422127\n",
      "step 3280 loss 0.448589861 fisher_loss 0.0513363257 l2_loss 0.0958952 accuracy B 0.871989191 accuracy A 0.895087421\n",
      "step 3296 loss 0.440204173 fisher_loss 0.0522696674 l2_loss 0.0963383093 accuracy B 0.870094717 accuracy A 0.893422127\n",
      "step 3312 loss 0.604231775 fisher_loss 0.0528009273 l2_loss 0.0967090502 accuracy B 0.869824111 accuracy A 0.893422127\n",
      "step 3328 loss 0.635301352 fisher_loss 0.0518537909 l2_loss 0.0963896513 accuracy B 0.869824111 accuracy A 0.893422127\n",
      "step 3344 loss 0.434496 fisher_loss 0.0510850586 l2_loss 0.0960923061 accuracy B 0.870365381 accuracy A 0.894254804\n",
      "step 3360 loss 0.448600471 fisher_loss 0.0520300716 l2_loss 0.096597679 accuracy B 0.868200243 accuracy A 0.891756892\n",
      "step 3376 loss 0.40405798 fisher_loss 0.053552445 l2_loss 0.0971638411 accuracy B 0.869824111 accuracy A 0.890091598\n",
      "step 3392 loss 0.711116 fisher_loss 0.0519670807 l2_loss 0.0969797224 accuracy B 0.868470907 accuracy A 0.893422127\n",
      "step 3408 loss 0.486163557 fisher_loss 0.0518136472 l2_loss 0.0972187445 accuracy B 0.868470907 accuracy A 0.894254804\n",
      "step 3424 loss 0.503489375 fisher_loss 0.0509149916 l2_loss 0.0971290097 accuracy B 0.870636 accuracy A 0.894254804\n",
      "step 3440 loss 0.555036604 fisher_loss 0.0516123511 l2_loss 0.097510159 accuracy B 0.869012177 accuracy A 0.893422127\n",
      "step 3456 loss 0.351670325 fisher_loss 0.0527170189 l2_loss 0.097919859 accuracy B 0.869012177 accuracy A 0.893422127\n",
      "step 3472 loss 0.60387665 fisher_loss 0.0514791235 l2_loss 0.0977894291 accuracy B 0.869012177 accuracy A 0.894254804\n",
      "step 3488 loss 0.392281204 fisher_loss 0.0506370924 l2_loss 0.0976748168 accuracy B 0.870365381 accuracy A 0.895920038\n",
      "step 3504 loss 0.45499298 fisher_loss 0.0533401035 l2_loss 0.0988270715 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 3520 loss 0.468447357 fisher_loss 0.0545174517 l2_loss 0.0996232256 accuracy B 0.872801065 accuracy A 0.892589509\n",
      "step 3536 loss 0.446482658 fisher_loss 0.0521787889 l2_loss 0.0991467088 accuracy B 0.870365381 accuracy A 0.894254804\n",
      "step 3552 loss 0.391477555 fisher_loss 0.0507228635 l2_loss 0.0985422283 accuracy B 0.870906651 accuracy A 0.895087421\n",
      "step 3568 loss 0.484227955 fisher_loss 0.0496533029 l2_loss 0.0982183814 accuracy B 0.869012177 accuracy A 0.896752715\n",
      "step 3584 loss 0.46182844 fisher_loss 0.0518392622 l2_loss 0.0993468612 accuracy B 0.870636 accuracy A 0.893422127\n",
      "step 3600 loss 0.503982425 fisher_loss 0.0543983579 l2_loss 0.100750685 accuracy B 0.871989191 accuracy A 0.890924215\n",
      "step 3616 loss 0.604307175 fisher_loss 0.0526305735 l2_loss 0.100288093 accuracy B 0.870094717 accuracy A 0.892589509\n",
      "step 3632 loss 0.567594945 fisher_loss 0.0504887104 l2_loss 0.0995741785 accuracy B 0.869824111 accuracy A 0.895087421\n",
      "step 3648 loss 0.407371074 fisher_loss 0.0518391207 l2_loss 0.100408018 accuracy B 0.871718526 accuracy A 0.892589509\n",
      "step 3664 loss 0.383147448 fisher_loss 0.0518829 l2_loss 0.100802645 accuracy B 0.872259796 accuracy A 0.893422127\n",
      "step 3680 loss 0.471459836 fisher_loss 0.0509912111 l2_loss 0.100755475 accuracy B 0.870906651 accuracy A 0.895087421\n",
      "step 3696 loss 0.871353865 fisher_loss 0.0511985607 l2_loss 0.100947402 accuracy B 0.870636 accuracy A 0.894254804\n",
      "step 3712 loss 0.461922526 fisher_loss 0.0550338551 l2_loss 0.102324888 accuracy B 0.872259796 accuracy A 0.888426304\n",
      "step 3728 loss 0.619063318 fisher_loss 0.0556877255 l2_loss 0.102518126 accuracy B 0.871177256 accuracy A 0.887593687\n",
      "step 3744 loss 0.434029162 fisher_loss 0.0519791283 l2_loss 0.101113938 accuracy B 0.867929637 accuracy A 0.893422127\n",
      "step 3760 loss 0.388499111 fisher_loss 0.0510712154 l2_loss 0.100809574 accuracy B 0.869282842 accuracy A 0.894254804\n",
      "step 3776 loss 0.479225576 fisher_loss 0.0522549972 l2_loss 0.101313755 accuracy B 0.868741572 accuracy A 0.892589509\n",
      "step 3792 loss 0.407910585 fisher_loss 0.0509157255 l2_loss 0.100604706 accuracy B 0.868741572 accuracy A 0.895087421\n",
      "step 3808 loss 0.448521733 fisher_loss 0.0502978787 l2_loss 0.100252733 accuracy B 0.868200243 accuracy A 0.896752715\n",
      "step 3824 loss 0.487419158 fisher_loss 0.0509156808 l2_loss 0.100939967 accuracy B 0.869282842 accuracy A 0.894254804\n",
      "step 3840 loss 0.605112374 fisher_loss 0.0525495037 l2_loss 0.101984218 accuracy B 0.870365381 accuracy A 0.893422127\n",
      "step 3856 loss 0.470273 fisher_loss 0.0535647571 l2_loss 0.102528185 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 3872 loss 0.439855754 fisher_loss 0.0533610582 l2_loss 0.102883011 accuracy B 0.870636 accuracy A 0.892589509\n",
      "step 3888 loss 0.391829878 fisher_loss 0.0536014475 l2_loss 0.10337349 accuracy B 0.871718526 accuracy A 0.892589509\n",
      "step 3904 loss 0.487725765 fisher_loss 0.0520167165 l2_loss 0.103074 accuracy B 0.870906651 accuracy A 0.894254804\n",
      "step 3920 loss 0.43060109 fisher_loss 0.0510147512 l2_loss 0.102898233 accuracy B 0.871177256 accuracy A 0.895087421\n",
      "step 3936 loss 0.501517 fisher_loss 0.0531989522 l2_loss 0.10403581 accuracy B 0.870906651 accuracy A 0.892589509\n",
      "step 3952 loss 0.61877954 fisher_loss 0.052706603 l2_loss 0.104274265 accuracy B 0.872259796 accuracy A 0.893422127\n",
      "step 3968 loss 0.384461045 fisher_loss 0.0507685132 l2_loss 0.103480048 accuracy B 0.870636 accuracy A 0.895087421\n",
      "step 3984 loss 0.429896 fisher_loss 0.0530909263 l2_loss 0.104323104 accuracy B 0.870636 accuracy A 0.890924215\n",
      "step 4000 loss 0.362590671 fisher_loss 0.0532921441 l2_loss 0.104525499 accuracy B 0.870636 accuracy A 0.890924215\n",
      "step 4016 loss 0.623127162 fisher_loss 0.0531109571 l2_loss 0.104381345 accuracy B 0.869553447 accuracy A 0.891756892\n",
      "step 4032 loss 0.431996197 fisher_loss 0.0519690774 l2_loss 0.10397958 accuracy B 0.869012177 accuracy A 0.892589509\n",
      "step 4048 loss 0.530816317 fisher_loss 0.0515914857 l2_loss 0.104207791 accuracy B 0.871718526 accuracy A 0.894254804\n",
      "step 4064 loss 0.481820732 fisher_loss 0.0520222411 l2_loss 0.104525581 accuracy B 0.870906651 accuracy A 0.894254804\n",
      "step 4080 loss 0.543703377 fisher_loss 0.0522959903 l2_loss 0.104541801 accuracy B 0.870094717 accuracy A 0.893422127\n",
      "step 4096 loss 0.527791381 fisher_loss 0.0534227081 l2_loss 0.105188563 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 4112 loss 0.410433352 fisher_loss 0.0529572479 l2_loss 0.105022788 accuracy B 0.869553447 accuracy A 0.892589509\n",
      "step 4128 loss 0.545499742 fisher_loss 0.051270809 l2_loss 0.104601286 accuracy B 0.870365381 accuracy A 0.894254804\n",
      "step 4144 loss 0.706186 fisher_loss 0.0507965051 l2_loss 0.104502171 accuracy B 0.869012177 accuracy A 0.894254804\n",
      "step 4160 loss 0.412067026 fisher_loss 0.0512208156 l2_loss 0.104715146 accuracy B 0.870636 accuracy A 0.894254804\n",
      "step 4176 loss 0.385974467 fisher_loss 0.0515281521 l2_loss 0.104896612 accuracy B 0.869824111 accuracy A 0.893422127\n",
      "step 4192 loss 0.63564837 fisher_loss 0.0527593903 l2_loss 0.105399586 accuracy B 0.868470907 accuracy A 0.891756892\n",
      "step 4208 loss 0.645093918 fisher_loss 0.0517994277 l2_loss 0.105389856 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 4224 loss 0.405722648 fisher_loss 0.0524236076 l2_loss 0.105875202 accuracy B 0.870094717 accuracy A 0.891756892\n",
      "step 4240 loss 0.561947584 fisher_loss 0.0529684089 l2_loss 0.106398821 accuracy B 0.870636 accuracy A 0.890924215\n",
      "step 4256 loss 0.470821559 fisher_loss 0.0513352603 l2_loss 0.105828889 accuracy B 0.870636 accuracy A 0.892589509\n",
      "step 4272 loss 0.439010412 fisher_loss 0.0517266169 l2_loss 0.105802245 accuracy B 0.869282842 accuracy A 0.891756892\n",
      "step 4288 loss 0.462182164 fisher_loss 0.0532057509 l2_loss 0.106562234 accuracy B 0.870365381 accuracy A 0.891756892\n",
      "step 4304 loss 0.554034889 fisher_loss 0.05420129 l2_loss 0.10706611 accuracy B 0.871447921 accuracy A 0.889258921\n",
      "step 4320 loss 0.591383457 fisher_loss 0.0556635819 l2_loss 0.107799515 accuracy B 0.870906651 accuracy A 0.887593687\n",
      "step 4336 loss 0.446212173 fisher_loss 0.0542999841 l2_loss 0.107819006 accuracy B 0.871177256 accuracy A 0.890924215\n",
      "step 4352 loss 0.512759268 fisher_loss 0.0524130277 l2_loss 0.107260808 accuracy B 0.870365381 accuracy A 0.894254804\n",
      "step 4368 loss 0.511510551 fisher_loss 0.0513457619 l2_loss 0.10703662 accuracy B 0.871989191 accuracy A 0.895087421\n",
      "step 4384 loss 0.34438774 fisher_loss 0.052916538 l2_loss 0.107909702 accuracy B 0.871447921 accuracy A 0.892589509\n",
      "step 4400 loss 0.948891282 fisher_loss 0.0549824387 l2_loss 0.10873989 accuracy B 0.87253046 accuracy A 0.889258921\n",
      "step 4416 loss 0.422960281 fisher_loss 0.0541403368 l2_loss 0.108231209 accuracy B 0.870636 accuracy A 0.889258921\n",
      "step 4432 loss 0.449258715 fisher_loss 0.0537909679 l2_loss 0.108366616 accuracy B 0.870906651 accuracy A 0.890924215\n",
      "step 4448 loss 0.566259384 fisher_loss 0.0537185445 l2_loss 0.108685568 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 4464 loss 0.417685896 fisher_loss 0.0524826311 l2_loss 0.10799922 accuracy B 0.868741572 accuracy A 0.893422127\n",
      "step 4480 loss 0.392333835 fisher_loss 0.0515674129 l2_loss 0.107777275 accuracy B 0.869553447 accuracy A 0.894254804\n",
      "step 4496 loss 0.650506139 fisher_loss 0.0513917394 l2_loss 0.107999131 accuracy B 0.870906651 accuracy A 0.893422127\n",
      "step 4512 loss 0.532986164 fisher_loss 0.0517866388 l2_loss 0.10816282 accuracy B 0.870906651 accuracy A 0.893422127\n",
      "step 4528 loss 0.458770871 fisher_loss 0.0519599468 l2_loss 0.108345509 accuracy B 0.870094717 accuracy A 0.892589509\n",
      "step 4544 loss 0.538366556 fisher_loss 0.0517013893 l2_loss 0.108580127 accuracy B 0.870636 accuracy A 0.892589509\n",
      "step 4560 loss 0.301927447 fisher_loss 0.0515528359 l2_loss 0.108243734 accuracy B 0.870094717 accuracy A 0.893422127\n",
      "step 4576 loss 0.348281294 fisher_loss 0.0516922735 l2_loss 0.108310811 accuracy B 0.869824111 accuracy A 0.893422127\n",
      "step 4592 loss 0.426471025 fisher_loss 0.0522981 l2_loss 0.108600631 accuracy B 0.869012177 accuracy A 0.893422127\n",
      "step 4608 loss 0.37991336 fisher_loss 0.0522545949 l2_loss 0.108733624 accuracy B 0.869824111 accuracy A 0.893422127\n",
      "step 4624 loss 0.451821238 fisher_loss 0.052472733 l2_loss 0.109164596 accuracy B 0.870094717 accuracy A 0.893422127\n",
      "step 4640 loss 0.407169908 fisher_loss 0.0516256765 l2_loss 0.10888081 accuracy B 0.871177256 accuracy A 0.894254804\n",
      "step 4656 loss 0.423641294 fisher_loss 0.0520979725 l2_loss 0.109245613 accuracy B 0.870636 accuracy A 0.893422127\n",
      "step 4672 loss 0.352628082 fisher_loss 0.0529022776 l2_loss 0.109611742 accuracy B 0.870094717 accuracy A 0.892589509\n",
      "step 4688 loss 0.71889931 fisher_loss 0.0515961871 l2_loss 0.109013543 accuracy B 0.870094717 accuracy A 0.893422127\n",
      "step 4704 loss 0.372481197 fisher_loss 0.0500735976 l2_loss 0.10817004 accuracy B 0.868470907 accuracy A 0.898418\n",
      "step 4720 loss 0.38067919 fisher_loss 0.051015906 l2_loss 0.108659528 accuracy B 0.870365381 accuracy A 0.893422127\n",
      "step 4736 loss 0.434183955 fisher_loss 0.0531095192 l2_loss 0.109655477 accuracy B 0.869553447 accuracy A 0.891756892\n",
      "step 4752 loss 0.543063283 fisher_loss 0.0536129326 l2_loss 0.110169388 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 4768 loss 0.435639054 fisher_loss 0.0533066913 l2_loss 0.110452779 accuracy B 0.870906651 accuracy A 0.892589509\n",
      "step 4784 loss 0.482502729 fisher_loss 0.0528450869 l2_loss 0.110636465 accuracy B 0.871177256 accuracy A 0.892589509\n",
      "step 4800 loss 0.581652701 fisher_loss 0.0529570952 l2_loss 0.110920221 accuracy B 0.870906651 accuracy A 0.892589509\n",
      "step 4816 loss 0.395501852 fisher_loss 0.0523879826 l2_loss 0.110530019 accuracy B 0.869824111 accuracy A 0.891756892\n",
      "step 4832 loss 0.493353605 fisher_loss 0.0512644574 l2_loss 0.1101638 accuracy B 0.870365381 accuracy A 0.894254804\n",
      "step 4848 loss 0.70080173 fisher_loss 0.051804211 l2_loss 0.110199727 accuracy B 0.868741572 accuracy A 0.893422127\n",
      "step 4864 loss 0.416266024 fisher_loss 0.0508599617 l2_loss 0.109942794 accuracy B 0.869824111 accuracy A 0.895920038\n",
      "step 4880 loss 0.499352098 fisher_loss 0.0511356033 l2_loss 0.110352293 accuracy B 0.870636 accuracy A 0.895087421\n",
      "step 4896 loss 0.320101947 fisher_loss 0.0514558069 l2_loss 0.110435076 accuracy B 0.870636 accuracy A 0.893422127\n",
      "step 4912 loss 0.816845596 fisher_loss 0.0526830293 l2_loss 0.111010231 accuracy B 0.869012177 accuracy A 0.892589509\n",
      "step 4928 loss 0.50941819 fisher_loss 0.0525825955 l2_loss 0.111050785 accuracy B 0.870094717 accuracy A 0.893422127\n",
      "step 4944 loss 0.559851825 fisher_loss 0.0543222092 l2_loss 0.111866727 accuracy B 0.870094717 accuracy A 0.890091598\n",
      "step 4960 loss 0.64096725 fisher_loss 0.0547324233 l2_loss 0.111873 accuracy B 0.871177256 accuracy A 0.889258921\n",
      "step 4976 loss 0.491874635 fisher_loss 0.0531012863 l2_loss 0.11126826 accuracy B 0.868200243 accuracy A 0.890924215\n",
      "step 4992 loss 0.5370363 fisher_loss 0.0516581275 l2_loss 0.110877343 accuracy B 0.868741572 accuracy A 0.892589509\n",
      "step 5008 loss 0.587562203 fisher_loss 0.0508481078 l2_loss 0.110612281 accuracy B 0.869553447 accuracy A 0.894254804\n",
      "step 5024 loss 0.561083 fisher_loss 0.0508533865 l2_loss 0.110641859 accuracy B 0.870094717 accuracy A 0.893422127\n",
      "step 5040 loss 0.47418493 fisher_loss 0.0528581105 l2_loss 0.111619592 accuracy B 0.869282842 accuracy A 0.893422127\n",
      "step 5056 loss 0.398664832 fisher_loss 0.0516093671 l2_loss 0.111607939 accuracy B 0.872259796 accuracy A 0.894254804\n",
      "step 5072 loss 0.573424816 fisher_loss 0.0518533699 l2_loss 0.111729212 accuracy B 0.870906651 accuracy A 0.894254804\n",
      "step 5088 loss 0.577296436 fisher_loss 0.0522297025 l2_loss 0.111796983 accuracy B 0.870094717 accuracy A 0.893422127\n",
      "step 5104 loss 0.319167495 fisher_loss 0.0528844967 l2_loss 0.112064265 accuracy B 0.869553447 accuracy A 0.892589509\n",
      "step 5120 loss 0.38263905 fisher_loss 0.054020822 l2_loss 0.112692297 accuracy B 0.870906651 accuracy A 0.889258921\n",
      "step 5136 loss 0.29559347 fisher_loss 0.0521500483 l2_loss 0.112218298 accuracy B 0.870094717 accuracy A 0.891756892\n",
      "step 5152 loss 0.385880828 fisher_loss 0.0510144271 l2_loss 0.111935347 accuracy B 0.869553447 accuracy A 0.894254804\n",
      "step 5168 loss 0.55894959 fisher_loss 0.0524730496 l2_loss 0.112628154 accuracy B 0.870636 accuracy A 0.892589509\n",
      "step 5184 loss 0.724685 fisher_loss 0.0551758781 l2_loss 0.113691144 accuracy B 0.871718526 accuracy A 0.888426304\n",
      "step 5200 loss 0.3411493 fisher_loss 0.0542588234 l2_loss 0.113591798 accuracy B 0.870906651 accuracy A 0.889258921\n",
      "step 5216 loss 0.517613232 fisher_loss 0.0532347672 l2_loss 0.113271333 accuracy B 0.870094717 accuracy A 0.890924215\n",
      "step 5232 loss 0.447260082 fisher_loss 0.05326299 l2_loss 0.113099672 accuracy B 0.869824111 accuracy A 0.890924215\n",
      "step 5248 loss 0.452794671 fisher_loss 0.0534989312 l2_loss 0.11356125 accuracy B 0.870365381 accuracy A 0.891756892\n",
      "step 5264 loss 0.403968871 fisher_loss 0.0530901104 l2_loss 0.113830738 accuracy B 0.870636 accuracy A 0.892589509\n",
      "step 5280 loss 0.439679116 fisher_loss 0.0531474762 l2_loss 0.114268817 accuracy B 0.871447921 accuracy A 0.891756892\n",
      "step 5296 loss 0.438531041 fisher_loss 0.0548204929 l2_loss 0.115006492 accuracy B 0.872259796 accuracy A 0.890091598\n",
      "step 5312 loss 0.369860232 fisher_loss 0.0534135848 l2_loss 0.114428259 accuracy B 0.870636 accuracy A 0.890924215\n",
      "step 5328 loss 0.34705624 fisher_loss 0.0514507852 l2_loss 0.113473885 accuracy B 0.870094717 accuracy A 0.892589509\n",
      "step 5344 loss 0.469108313 fisher_loss 0.0516360961 l2_loss 0.113734081 accuracy B 0.870906651 accuracy A 0.891756892\n",
      "step 5360 loss 0.435565561 fisher_loss 0.0530669354 l2_loss 0.114356205 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 5376 loss 0.602683544 fisher_loss 0.0532613806 l2_loss 0.114467658 accuracy B 0.870636 accuracy A 0.890924215\n",
      "step 5392 loss 0.586951911 fisher_loss 0.0526276939 l2_loss 0.114492238 accuracy B 0.871177256 accuracy A 0.892589509\n",
      "step 5408 loss 0.474718 fisher_loss 0.051839076 l2_loss 0.114248432 accuracy B 0.87253046 accuracy A 0.894254804\n",
      "step 5424 loss 0.393713534 fisher_loss 0.0517945774 l2_loss 0.114295736 accuracy B 0.87253046 accuracy A 0.894254804\n",
      "step 5440 loss 0.467969388 fisher_loss 0.0518982746 l2_loss 0.114233606 accuracy B 0.871177256 accuracy A 0.894254804\n",
      "step 5456 loss 0.503079832 fisher_loss 0.0519795641 l2_loss 0.114057183 accuracy B 0.869282842 accuracy A 0.892589509\n",
      "step 5472 loss 0.656308889 fisher_loss 0.0510191359 l2_loss 0.113730468 accuracy B 0.870094717 accuracy A 0.894254804\n",
      "step 5488 loss 0.357100904 fisher_loss 0.051598385 l2_loss 0.114098474 accuracy B 0.870365381 accuracy A 0.893422127\n",
      "step 5504 loss 0.49525854 fisher_loss 0.0521342568 l2_loss 0.114478096 accuracy B 0.871177256 accuracy A 0.893422127\n",
      "step 5520 loss 0.467312038 fisher_loss 0.0526521727 l2_loss 0.114754647 accuracy B 0.870094717 accuracy A 0.892589509\n",
      "step 5536 loss 0.395258367 fisher_loss 0.0533686355 l2_loss 0.115129843 accuracy B 0.869282842 accuracy A 0.891756892\n",
      "step 5552 loss 0.337823302 fisher_loss 0.0531943627 l2_loss 0.115209498 accuracy B 0.870636 accuracy A 0.892589509\n",
      "step 5568 loss 0.48540625 fisher_loss 0.0521275625 l2_loss 0.114973329 accuracy B 0.871177256 accuracy A 0.893422127\n",
      "step 5584 loss 0.530944705 fisher_loss 0.05329502 l2_loss 0.11550042 accuracy B 0.869824111 accuracy A 0.890924215\n",
      "step 5600 loss 0.503820062 fisher_loss 0.0543770902 l2_loss 0.116034433 accuracy B 0.870906651 accuracy A 0.890924215\n",
      "step 5616 loss 0.317856491 fisher_loss 0.0537935905 l2_loss 0.115831032 accuracy B 0.869824111 accuracy A 0.890924215\n",
      "step 5632 loss 0.511745036 fisher_loss 0.0513149723 l2_loss 0.114984982 accuracy B 0.870365381 accuracy A 0.894254804\n",
      "step 5648 loss 0.568086624 fisher_loss 0.0520436466 l2_loss 0.11554236 accuracy B 0.870906651 accuracy A 0.892589509\n",
      "step 5664 loss 0.550843179 fisher_loss 0.0557199679 l2_loss 0.117073432 accuracy B 0.872259796 accuracy A 0.888426304\n",
      "step 5680 loss 0.410634637 fisher_loss 0.0532430708 l2_loss 0.116126709 accuracy B 0.870636 accuracy A 0.893422127\n",
      "step 5696 loss 0.380475968 fisher_loss 0.0509892143 l2_loss 0.115262382 accuracy B 0.870636 accuracy A 0.895087421\n",
      "step 5712 loss 0.499754071 fisher_loss 0.0514330417 l2_loss 0.115213476 accuracy B 0.870094717 accuracy A 0.894254804\n",
      "step 5728 loss 0.451073855 fisher_loss 0.0519866608 l2_loss 0.11551562 accuracy B 0.869824111 accuracy A 0.892589509\n",
      "step 5744 loss 0.566139 fisher_loss 0.0547000319 l2_loss 0.116704501 accuracy B 0.870094717 accuracy A 0.888426304\n",
      "step 5760 loss 0.425162226 fisher_loss 0.0527860187 l2_loss 0.116079241 accuracy B 0.868741572 accuracy A 0.890924215\n",
      "step 5776 loss 0.381911695 fisher_loss 0.0519308448 l2_loss 0.115946747 accuracy B 0.870636 accuracy A 0.892589509\n",
      "step 5792 loss 0.385363698 fisher_loss 0.0533504821 l2_loss 0.116537146 accuracy B 0.868741572 accuracy A 0.890091598\n",
      "step 5808 loss 0.442120194 fisher_loss 0.0543536469 l2_loss 0.11711318 accuracy B 0.870906651 accuracy A 0.889258921\n",
      "step 5824 loss 0.576274931 fisher_loss 0.053256508 l2_loss 0.116614595 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 5840 loss 0.501600564 fisher_loss 0.0542425252 l2_loss 0.116904519 accuracy B 0.871177256 accuracy A 0.888426304\n",
      "step 5856 loss 0.413062543 fisher_loss 0.0538952 l2_loss 0.116629034 accuracy B 0.868200243 accuracy A 0.890091598\n",
      "step 5872 loss 0.508435428 fisher_loss 0.0512629338 l2_loss 0.115657613 accuracy B 0.868741572 accuracy A 0.894254804\n",
      "step 5888 loss 0.367853373 fisher_loss 0.0502858236 l2_loss 0.115613386 accuracy B 0.869012177 accuracy A 0.896752715\n",
      "step 5904 loss 0.493039757 fisher_loss 0.0514504313 l2_loss 0.116515785 accuracy B 0.871718526 accuracy A 0.894254804\n",
      "step 5920 loss 0.427777 fisher_loss 0.0525219589 l2_loss 0.116852671 accuracy B 0.870636 accuracy A 0.892589509\n",
      "step 5936 loss 0.428639 fisher_loss 0.0525718369 l2_loss 0.116903648 accuracy B 0.870636 accuracy A 0.891756892\n",
      "step 5952 loss 0.527593076 fisher_loss 0.0531208254 l2_loss 0.117262505 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 5968 loss 0.539368451 fisher_loss 0.0537169129 l2_loss 0.117478162 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 5984 loss 0.344205916 fisher_loss 0.0534129776 l2_loss 0.117616162 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 6000 loss 0.325391114 fisher_loss 0.0517275073 l2_loss 0.117193013 accuracy B 0.871447921 accuracy A 0.893422127\n",
      "step 6016 loss 0.365177363 fisher_loss 0.0511381216 l2_loss 0.117051549 accuracy B 0.871177256 accuracy A 0.895087421\n",
      "step 6032 loss 0.577545404 fisher_loss 0.0511812158 l2_loss 0.117074087 accuracy B 0.870906651 accuracy A 0.894254804\n",
      "step 6048 loss 0.406704098 fisher_loss 0.0520733185 l2_loss 0.117376901 accuracy B 0.872259796 accuracy A 0.892589509\n",
      "step 6064 loss 0.480964601 fisher_loss 0.0524762757 l2_loss 0.117423452 accuracy B 0.870906651 accuracy A 0.890924215\n",
      "step 6080 loss 0.557877302 fisher_loss 0.053461317 l2_loss 0.117784619 accuracy B 0.869824111 accuracy A 0.890924215\n",
      "step 6096 loss 0.314076573 fisher_loss 0.0513225794 l2_loss 0.11713583 accuracy B 0.871718526 accuracy A 0.894254804\n",
      "step 6112 loss 0.444125682 fisher_loss 0.0524198636 l2_loss 0.117594868 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 6128 loss 0.606154442 fisher_loss 0.0524391457 l2_loss 0.11766582 accuracy B 0.870636 accuracy A 0.891756892\n",
      "step 6144 loss 0.362727582 fisher_loss 0.052432362 l2_loss 0.117423534 accuracy B 0.868741572 accuracy A 0.893422127\n",
      "step 6160 loss 0.533599913 fisher_loss 0.0523071811 l2_loss 0.117370717 accuracy B 0.869012177 accuracy A 0.893422127\n",
      "step 6176 loss 0.493483752 fisher_loss 0.053052865 l2_loss 0.117938176 accuracy B 0.869282842 accuracy A 0.892589509\n",
      "step 6192 loss 0.376368046 fisher_loss 0.0530231 l2_loss 0.1179787 accuracy B 0.869282842 accuracy A 0.892589509\n",
      "step 6208 loss 0.358633429 fisher_loss 0.0546373464 l2_loss 0.119094044 accuracy B 0.871447921 accuracy A 0.890091598\n",
      "step 6224 loss 0.481873 fisher_loss 0.0550573245 l2_loss 0.119605087 accuracy B 0.87253046 accuracy A 0.890924215\n",
      "step 6240 loss 0.307364672 fisher_loss 0.0546578541 l2_loss 0.119257718 accuracy B 0.870906651 accuracy A 0.889258921\n",
      "step 6256 loss 0.632273316 fisher_loss 0.0524668582 l2_loss 0.118334331 accuracy B 0.869282842 accuracy A 0.892589509\n",
      "step 6272 loss 0.401973128 fisher_loss 0.0510954969 l2_loss 0.117792562 accuracy B 0.870094717 accuracy A 0.894254804\n",
      "step 6288 loss 0.602343619 fisher_loss 0.0499576703 l2_loss 0.117506124 accuracy B 0.869282842 accuracy A 0.895920038\n",
      "step 6304 loss 0.673644066 fisher_loss 0.0504235253 l2_loss 0.117918797 accuracy B 0.869553447 accuracy A 0.895087421\n",
      "step 6320 loss 0.450406969 fisher_loss 0.0509769171 l2_loss 0.117832221 accuracy B 0.869553447 accuracy A 0.894254804\n",
      "step 6336 loss 0.466778278 fisher_loss 0.0535474867 l2_loss 0.118669815 accuracy B 0.869553447 accuracy A 0.890091598\n",
      "step 6352 loss 0.476326525 fisher_loss 0.0539673567 l2_loss 0.118961096 accuracy B 0.870636 accuracy A 0.890091598\n",
      "step 6368 loss 0.40933156 fisher_loss 0.0555005819 l2_loss 0.119723529 accuracy B 0.870906651 accuracy A 0.887593687\n",
      "step 6384 loss 0.490233958 fisher_loss 0.0548223406 l2_loss 0.119952671 accuracy B 0.870906651 accuracy A 0.888426304\n",
      "step 6400 loss 0.517436564 fisher_loss 0.0535017736 l2_loss 0.119818307 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 6416 loss 0.465624094 fisher_loss 0.054514572 l2_loss 0.120429076 accuracy B 0.871447921 accuracy A 0.890924215\n",
      "step 6432 loss 0.420375049 fisher_loss 0.05440538 l2_loss 0.120343208 accuracy B 0.870636 accuracy A 0.890091598\n",
      "step 6448 loss 0.404755116 fisher_loss 0.0529574901 l2_loss 0.119754605 accuracy B 0.869282842 accuracy A 0.890924215\n",
      "step 6464 loss 0.476374179 fisher_loss 0.0523071 l2_loss 0.119443119 accuracy B 0.870094717 accuracy A 0.891756892\n",
      "step 6480 loss 0.53355056 fisher_loss 0.0519039407 l2_loss 0.119536445 accuracy B 0.871447921 accuracy A 0.894254804\n",
      "step 6496 loss 0.45817697 fisher_loss 0.0526539534 l2_loss 0.119745053 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 6512 loss 0.618425846 fisher_loss 0.0521708317 l2_loss 0.119535804 accuracy B 0.870906651 accuracy A 0.893422127\n",
      "step 6528 loss 0.500612855 fisher_loss 0.0528726056 l2_loss 0.119669341 accuracy B 0.869282842 accuracy A 0.890091598\n",
      "step 6544 loss 0.467611283 fisher_loss 0.0547442138 l2_loss 0.120432101 accuracy B 0.870906651 accuracy A 0.889258921\n",
      "step 6560 loss 0.452686667 fisher_loss 0.0542388633 l2_loss 0.120206 accuracy B 0.870365381 accuracy A 0.889258921\n",
      "step 6576 loss 0.476440787 fisher_loss 0.0540037043 l2_loss 0.1203738 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 6592 loss 0.514866948 fisher_loss 0.0533705056 l2_loss 0.120093778 accuracy B 0.869012177 accuracy A 0.890924215\n",
      "step 6608 loss 0.347682238 fisher_loss 0.0536174737 l2_loss 0.120301262 accuracy B 0.870094717 accuracy A 0.890924215\n",
      "step 6624 loss 0.794854641 fisher_loss 0.0528810285 l2_loss 0.12022651 accuracy B 0.870365381 accuracy A 0.891756892\n",
      "step 6640 loss 0.329874963 fisher_loss 0.0516755208 l2_loss 0.119762503 accuracy B 0.870365381 accuracy A 0.893422127\n",
      "step 6656 loss 0.436725855 fisher_loss 0.0511934645 l2_loss 0.119631715 accuracy B 0.869824111 accuracy A 0.894254804\n",
      "step 6672 loss 0.457123846 fisher_loss 0.0522415861 l2_loss 0.119979165 accuracy B 0.870636 accuracy A 0.893422127\n",
      "step 6688 loss 0.550604582 fisher_loss 0.0509841 l2_loss 0.119389839 accuracy B 0.870094717 accuracy A 0.895087421\n",
      "step 6704 loss 0.506816804 fisher_loss 0.0517388396 l2_loss 0.119514741 accuracy B 0.869824111 accuracy A 0.893422127\n",
      "step 6720 loss 0.370141715 fisher_loss 0.0525807329 l2_loss 0.119985357 accuracy B 0.870094717 accuracy A 0.892589509\n",
      "step 6736 loss 0.470316261 fisher_loss 0.0542291142 l2_loss 0.120863073 accuracy B 0.871177256 accuracy A 0.890924215\n",
      "step 6752 loss 0.433282644 fisher_loss 0.0544135496 l2_loss 0.121146627 accuracy B 0.870906651 accuracy A 0.890091598\n",
      "step 6768 loss 0.381715626 fisher_loss 0.0527015105 l2_loss 0.120737769 accuracy B 0.871177256 accuracy A 0.892589509\n",
      "step 6784 loss 0.475898921 fisher_loss 0.0501829833 l2_loss 0.11982163 accuracy B 0.869824111 accuracy A 0.896752715\n",
      "step 6800 loss 0.452104688 fisher_loss 0.052378878 l2_loss 0.120588385 accuracy B 0.871718526 accuracy A 0.891756892\n",
      "step 6816 loss 0.580046952 fisher_loss 0.0530877747 l2_loss 0.120894857 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 6832 loss 0.456190407 fisher_loss 0.0545069836 l2_loss 0.121478826 accuracy B 0.871177256 accuracy A 0.889258921\n",
      "step 6848 loss 0.563058 fisher_loss 0.054643102 l2_loss 0.121661037 accuracy B 0.870094717 accuracy A 0.889258921\n",
      "step 6864 loss 0.510787 fisher_loss 0.0534211509 l2_loss 0.121326149 accuracy B 0.869824111 accuracy A 0.890924215\n",
      "step 6880 loss 0.531827569 fisher_loss 0.0527553447 l2_loss 0.120804094 accuracy B 0.869282842 accuracy A 0.891756892\n",
      "step 6896 loss 0.439413577 fisher_loss 0.0517523699 l2_loss 0.120155066 accuracy B 0.869553447 accuracy A 0.893422127\n",
      "step 6912 loss 0.402742 fisher_loss 0.04957911 l2_loss 0.119473919 accuracy B 0.868470907 accuracy A 0.899250627\n",
      "step 6928 loss 0.612104356 fisher_loss 0.0500061326 l2_loss 0.119690605 accuracy B 0.869282842 accuracy A 0.894254804\n",
      "step 6944 loss 0.353204221 fisher_loss 0.0538054518 l2_loss 0.12094485 accuracy B 0.870636 accuracy A 0.889258921\n",
      "step 6960 loss 0.367912471 fisher_loss 0.0554778427 l2_loss 0.121645369 accuracy B 0.870365381 accuracy A 0.886761\n",
      "step 6976 loss 0.561616957 fisher_loss 0.0554242544 l2_loss 0.121729247 accuracy B 0.870636 accuracy A 0.887593687\n",
      "step 6992 loss 0.664019 fisher_loss 0.0520781465 l2_loss 0.12069989 accuracy B 0.869282842 accuracy A 0.893422127\n",
      "step 7008 loss 0.464020103 fisher_loss 0.0519527867 l2_loss 0.120926939 accuracy B 0.870636 accuracy A 0.893422127\n",
      "step 7024 loss 0.481038153 fisher_loss 0.0509261601 l2_loss 0.12076661 accuracy B 0.870094717 accuracy A 0.894254804\n",
      "step 7040 loss 0.420946211 fisher_loss 0.0515601635 l2_loss 0.120961718 accuracy B 0.870094717 accuracy A 0.894254804\n",
      "step 7056 loss 0.321631104 fisher_loss 0.0538095422 l2_loss 0.121885754 accuracy B 0.869824111 accuracy A 0.889258921\n",
      "step 7072 loss 0.889608145 fisher_loss 0.0545372963 l2_loss 0.12230701 accuracy B 0.870906651 accuracy A 0.889258921\n",
      "step 7088 loss 0.779953122 fisher_loss 0.0535981879 l2_loss 0.122080415 accuracy B 0.870636 accuracy A 0.890924215\n",
      "step 7104 loss 0.333697289 fisher_loss 0.0531880036 l2_loss 0.12197125 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 7120 loss 0.391567141 fisher_loss 0.054527197 l2_loss 0.122586101 accuracy B 0.871177256 accuracy A 0.890091598\n",
      "step 7136 loss 0.497401088 fisher_loss 0.0539236479 l2_loss 0.122347713 accuracy B 0.870094717 accuracy A 0.890924215\n",
      "step 7152 loss 0.57484585 fisher_loss 0.0527662486 l2_loss 0.12190035 accuracy B 0.870365381 accuracy A 0.891756892\n",
      "step 7168 loss 0.401190728 fisher_loss 0.0525833 l2_loss 0.121729322 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 7184 loss 0.4132469 fisher_loss 0.0508069135 l2_loss 0.121343128 accuracy B 0.870365381 accuracy A 0.895920038\n",
      "step 7200 loss 0.578056097 fisher_loss 0.0516407117 l2_loss 0.121862605 accuracy B 0.871718526 accuracy A 0.894254804\n",
      "step 7216 loss 0.442017645 fisher_loss 0.0520299487 l2_loss 0.121682845 accuracy B 0.870365381 accuracy A 0.893422127\n",
      "step 7232 loss 0.370743632 fisher_loss 0.0520004369 l2_loss 0.121755242 accuracy B 0.871177256 accuracy A 0.893422127\n",
      "step 7248 loss 0.417916775 fisher_loss 0.0536343977 l2_loss 0.122444853 accuracy B 0.870636 accuracy A 0.891756892\n",
      "step 7264 loss 0.419227272 fisher_loss 0.0540849157 l2_loss 0.122856885 accuracy B 0.870636 accuracy A 0.890924215\n",
      "step 7280 loss 0.388807923 fisher_loss 0.0523746274 l2_loss 0.122149691 accuracy B 0.870365381 accuracy A 0.891756892\n",
      "step 7296 loss 0.428523242 fisher_loss 0.0527433045 l2_loss 0.122389138 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 7312 loss 0.352655888 fisher_loss 0.0529112257 l2_loss 0.12268725 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 7328 loss 0.409255028 fisher_loss 0.0540832318 l2_loss 0.123209819 accuracy B 0.871177256 accuracy A 0.890091598\n",
      "step 7344 loss 0.524855733 fisher_loss 0.054231979 l2_loss 0.123380795 accuracy B 0.871447921 accuracy A 0.890924215\n",
      "step 7360 loss 0.402542114 fisher_loss 0.0526753515 l2_loss 0.12267033 accuracy B 0.870906651 accuracy A 0.892589509\n",
      "step 7376 loss 0.395216584 fisher_loss 0.0504554324 l2_loss 0.12157771 accuracy B 0.869282842 accuracy A 0.894254804\n",
      "step 7392 loss 0.384482771 fisher_loss 0.0506116375 l2_loss 0.121693105 accuracy B 0.869012177 accuracy A 0.894254804\n",
      "step 7408 loss 0.466419101 fisher_loss 0.0519631058 l2_loss 0.122434318 accuracy B 0.871989191 accuracy A 0.893422127\n",
      "step 7424 loss 0.373469383 fisher_loss 0.0534451045 l2_loss 0.12306127 accuracy B 0.870094717 accuracy A 0.891756892\n",
      "step 7440 loss 0.406548649 fisher_loss 0.0527400598 l2_loss 0.123103611 accuracy B 0.871447921 accuracy A 0.891756892\n",
      "step 7456 loss 0.830946565 fisher_loss 0.0521206819 l2_loss 0.122886397 accuracy B 0.870906651 accuracy A 0.891756892\n",
      "step 7472 loss 0.461255401 fisher_loss 0.0522142686 l2_loss 0.122602522 accuracy B 0.869553447 accuracy A 0.891756892\n",
      "step 7488 loss 0.603010297 fisher_loss 0.0530560911 l2_loss 0.12301027 accuracy B 0.869282842 accuracy A 0.891756892\n",
      "step 7504 loss 0.390793949 fisher_loss 0.0522540882 l2_loss 0.122658327 accuracy B 0.869553447 accuracy A 0.891756892\n",
      "step 7520 loss 0.379620492 fisher_loss 0.0505811684 l2_loss 0.121747032 accuracy B 0.868470907 accuracy A 0.894254804\n",
      "step 7536 loss 0.426326573 fisher_loss 0.0519958138 l2_loss 0.122151345 accuracy B 0.868470907 accuracy A 0.891756892\n",
      "step 7552 loss 0.439085752 fisher_loss 0.0510511212 l2_loss 0.121692769 accuracy B 0.869553447 accuracy A 0.894254804\n",
      "step 7568 loss 0.78525722 fisher_loss 0.0536762662 l2_loss 0.123079672 accuracy B 0.870365381 accuracy A 0.891756892\n",
      "step 7584 loss 0.450919211 fisher_loss 0.0537082627 l2_loss 0.123202607 accuracy B 0.870636 accuracy A 0.891756892\n",
      "step 7600 loss 0.525999308 fisher_loss 0.0533732623 l2_loss 0.123171516 accuracy B 0.870636 accuracy A 0.892589509\n",
      "step 7616 loss 0.272737533 fisher_loss 0.0526399575 l2_loss 0.123134919 accuracy B 0.869824111 accuracy A 0.892589509\n",
      "step 7632 loss 0.415205687 fisher_loss 0.0529643223 l2_loss 0.123279177 accuracy B 0.869282842 accuracy A 0.891756892\n",
      "step 7648 loss 0.385671765 fisher_loss 0.0550307557 l2_loss 0.124389127 accuracy B 0.871177256 accuracy A 0.888426304\n",
      "step 7664 loss 0.341369212 fisher_loss 0.0549103 l2_loss 0.124536008 accuracy B 0.871718526 accuracy A 0.889258921\n",
      "step 7680 loss 0.355575264 fisher_loss 0.0539137125 l2_loss 0.124491535 accuracy B 0.871447921 accuracy A 0.890924215\n",
      "step 7696 loss 0.365625679 fisher_loss 0.0525353365 l2_loss 0.123926781 accuracy B 0.871718526 accuracy A 0.892589509\n",
      "step 7712 loss 0.631251097 fisher_loss 0.052179534 l2_loss 0.123683117 accuracy B 0.871447921 accuracy A 0.893422127\n",
      "step 7728 loss 0.553399503 fisher_loss 0.0521014072 l2_loss 0.12340299 accuracy B 0.870094717 accuracy A 0.892589509\n",
      "step 7744 loss 0.425998449 fisher_loss 0.0523925424 l2_loss 0.123489141 accuracy B 0.869824111 accuracy A 0.891756892\n",
      "step 7760 loss 0.261102647 fisher_loss 0.0520384349 l2_loss 0.123306423 accuracy B 0.870365381 accuracy A 0.892589509\n",
      "step 7776 loss 0.370224506 fisher_loss 0.0530196689 l2_loss 0.123718552 accuracy B 0.869824111 accuracy A 0.890924215\n",
      "step 7792 loss 0.409011602 fisher_loss 0.0533726029 l2_loss 0.124138355 accuracy B 0.870636 accuracy A 0.890924215\n",
      "step 7808 loss 0.649429 fisher_loss 0.0538802631 l2_loss 0.124051921 accuracy B 0.870094717 accuracy A 0.889258921\n",
      "step 7824 loss 0.554140031 fisher_loss 0.0534438081 l2_loss 0.123845227 accuracy B 0.870094717 accuracy A 0.891756892\n",
      "step 7840 loss 0.534147918 fisher_loss 0.0520144477 l2_loss 0.123284712 accuracy B 0.869282842 accuracy A 0.894254804\n",
      "step 7856 loss 0.562144518 fisher_loss 0.0512505881 l2_loss 0.123224795 accuracy B 0.870636 accuracy A 0.894254804\n",
      "step 7872 loss 0.480807483 fisher_loss 0.0519679338 l2_loss 0.123313479 accuracy B 0.869282842 accuracy A 0.892589509\n",
      "step 7888 loss 0.521027923 fisher_loss 0.0537180416 l2_loss 0.123812295 accuracy B 0.869553447 accuracy A 0.889258921\n",
      "step 7904 loss 0.35710603 fisher_loss 0.0522399656 l2_loss 0.123493768 accuracy B 0.869553447 accuracy A 0.891756892\n",
      "step 7920 loss 0.405405104 fisher_loss 0.0531922 l2_loss 0.124152184 accuracy B 0.869553447 accuracy A 0.890924215\n",
      "step 7936 loss 0.523581743 fisher_loss 0.0533152893 l2_loss 0.124472663 accuracy B 0.870636 accuracy A 0.891756892\n",
      "step 7952 loss 0.300930709 fisher_loss 0.0521185659 l2_loss 0.123922594 accuracy B 0.870636 accuracy A 0.891756892\n",
      "step 7968 loss 0.359059602 fisher_loss 0.0504738875 l2_loss 0.123228841 accuracy B 0.869282842 accuracy A 0.895087421\n",
      "step 7984 loss 0.496730477 fisher_loss 0.0524385 l2_loss 0.124183312 accuracy B 0.871177256 accuracy A 0.892589509\n",
      "step 8000 loss 0.391698956 fisher_loss 0.0543929636 l2_loss 0.125078022 accuracy B 0.871177256 accuracy A 0.889258921\n",
      "step 8016 loss 0.588373 fisher_loss 0.0537229553 l2_loss 0.12496715 accuracy B 0.870094717 accuracy A 0.890924215\n",
      "step 8032 loss 0.560184658 fisher_loss 0.0533913709 l2_loss 0.124937356 accuracy B 0.870636 accuracy A 0.890924215\n",
      "step 8048 loss 0.406538188 fisher_loss 0.0531781763 l2_loss 0.124638543 accuracy B 0.870365381 accuracy A 0.891756892\n",
      "step 8064 loss 0.520395219 fisher_loss 0.0525107943 l2_loss 0.124372222 accuracy B 0.869282842 accuracy A 0.891756892\n",
      "step 8080 loss 0.433369666 fisher_loss 0.0526601151 l2_loss 0.12398269 accuracy B 0.867659 accuracy A 0.891756892\n",
      "step 8096 loss 0.380610496 fisher_loss 0.0512990244 l2_loss 0.123547435 accuracy B 0.870094717 accuracy A 0.894254804\n",
      "step 8112 loss 0.460988581 fisher_loss 0.0518797077 l2_loss 0.12375693 accuracy B 0.868741572 accuracy A 0.893422127\n",
      "step 8128 loss 0.460537225 fisher_loss 0.0541253947 l2_loss 0.124927051 accuracy B 0.870906651 accuracy A 0.890091598\n",
      "step 8144 loss 0.414732665 fisher_loss 0.0537512861 l2_loss 0.1251847 accuracy B 0.871718526 accuracy A 0.892589509\n",
      "step 8160 loss 0.331366092 fisher_loss 0.0533420816 l2_loss 0.125212744 accuracy B 0.870636 accuracy A 0.892589509\n",
      "step 8176 loss 0.484946102 fisher_loss 0.0524767376 l2_loss 0.124725744 accuracy B 0.870365381 accuracy A 0.893422127\n",
      "step 8192 loss 0.349695921 fisher_loss 0.0514276586 l2_loss 0.124184325 accuracy B 0.870094717 accuracy A 0.893422127\n",
      "step 8208 loss 0.420239 fisher_loss 0.0523895845 l2_loss 0.124818765 accuracy B 0.871718526 accuracy A 0.891756892\n",
      "step 8224 loss 0.524603 fisher_loss 0.0527618825 l2_loss 0.125325665 accuracy B 0.87253046 accuracy A 0.891756892\n",
      "step 8240 loss 0.499151438 fisher_loss 0.0539144501 l2_loss 0.125578716 accuracy B 0.869824111 accuracy A 0.890091598\n",
      "step 8256 loss 0.687042177 fisher_loss 0.0537467599 l2_loss 0.125483245 accuracy B 0.869553447 accuracy A 0.890924215\n",
      "step 8272 loss 0.581903934 fisher_loss 0.053155 l2_loss 0.125312135 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 8288 loss 0.646914363 fisher_loss 0.0540279076 l2_loss 0.125795245 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 8304 loss 0.406952977 fisher_loss 0.0546135455 l2_loss 0.126111954 accuracy B 0.871177256 accuracy A 0.890091598\n",
      "step 8320 loss 0.477246642 fisher_loss 0.0522718467 l2_loss 0.125123188 accuracy B 0.870094717 accuracy A 0.891756892\n",
      "step 8336 loss 0.368284285 fisher_loss 0.0520454198 l2_loss 0.125052884 accuracy B 0.870636 accuracy A 0.893422127\n",
      "step 8352 loss 0.437895566 fisher_loss 0.052518934 l2_loss 0.125469968 accuracy B 0.871177256 accuracy A 0.892589509\n",
      "step 8368 loss 0.693942428 fisher_loss 0.0537166335 l2_loss 0.126053497 accuracy B 0.870636 accuracy A 0.890924215\n",
      "step 8384 loss 0.478790879 fisher_loss 0.052820608 l2_loss 0.125426397 accuracy B 0.870094717 accuracy A 0.891756892\n",
      "step 8400 loss 0.431078643 fisher_loss 0.0517345443 l2_loss 0.125059336 accuracy B 0.870906651 accuracy A 0.893422127\n",
      "step 8416 loss 0.404464781 fisher_loss 0.051785592 l2_loss 0.125245 accuracy B 0.871447921 accuracy A 0.893422127\n",
      "step 8432 loss 0.37509495 fisher_loss 0.0547173955 l2_loss 0.126452476 accuracy B 0.871177256 accuracy A 0.889258921\n",
      "step 8448 loss 0.467555821 fisher_loss 0.0537458211 l2_loss 0.126119182 accuracy B 0.870365381 accuracy A 0.890924215\n",
      "step 8464 loss 0.649507523 fisher_loss 0.0518473834 l2_loss 0.125577852 accuracy B 0.871177256 accuracy A 0.892589509\n",
      "step 8480 loss 0.43256402 fisher_loss 0.0524464324 l2_loss 0.126034871 accuracy B 0.87253046 accuracy A 0.892589509\n",
      "step 8496 loss 0.460761249 fisher_loss 0.0553265288 l2_loss 0.127443507 accuracy B 0.872259796 accuracy A 0.888426304\n",
      "step 8512 loss 0.28935197 fisher_loss 0.0542783253 l2_loss 0.12683472 accuracy B 0.869824111 accuracy A 0.889258921\n",
      "step 8528 loss 0.691618085 fisher_loss 0.0535998419 l2_loss 0.126637489 accuracy B 0.870906651 accuracy A 0.889258921\n",
      "step 8544 loss 0.488402396 fisher_loss 0.0533309765 l2_loss 0.126421317 accuracy B 0.870636 accuracy A 0.890091598\n",
      "step 8560 loss 0.552195251 fisher_loss 0.0534278564 l2_loss 0.126535192 accuracy B 0.870365381 accuracy A 0.890091598\n",
      "Saved checkpoint for step .tf2/EWC/Pad-IMS/classifier/fisher/ckpt-1\n"
     ]
    }
   ],
   "source": [
    "conv_net.freeze()\n",
    "train_clas_EWC = Train_Classifier_EWC(conv_net,fisher_matrix_CL, learning_rate, int(num_batches*training_iters),\n",
    "                                     batch_size, display_step,1.3e3, filepath, restore=False)\n",
    "\n",
    "train_clas_EWC.fit(X_train_IMS, y_train_IMS, X_train_Pad, y_train_Pad, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RzokhySTmpD"
   },
   "source": [
    "Finally, test the accuracy for both tasks\n",
    "\n",
    "### Task A (Paderborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from Weights/EWC/Pad-IMS/fisher/classifier/v2/ckpt-1\n"
     ]
    }
   ],
   "source": [
    "# Comment this line to use the model you just trained \n",
    "#embedding = Embedding()\n",
    "#conv_net = Classification(embedding)\n",
    "\n",
    "#pretrain = Pretraining(conv_net, learning_rate, training_iters, batch_size, \n",
    "#                       display_step,\"Weights/EWC/Pad-IMS/fisher/classifier/\", restore=True)\n",
    "#embedding = conv_net.embedding\n",
    "\n",
    "embedding, conv_net = load_model(\"Weights/EWC/Pad-IMS/fisher/classifier/v2/\")\n",
    "#embedding, _= load_model(\"Weights/EWC/Pad-IMS/fisher/embedding/\", net =\"emb\")\n",
    "\n",
    "#conv_net.embedding = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1585909317840,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "WZBt-Ge9BDBn",
    "outputId": "63ca020c-c340-4844-fa77-32520ddbbe6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer classification_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.out.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.out.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.out.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.out.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Test Accuracy: 0.737500\n"
     ]
    }
   ],
   "source": [
    "# Test model on test set.\n",
    "pred= conv_net(X_test_Pad)\n",
    "y_pred = np.argmax(pred.numpy(), axis=1)\n",
    "print(\"Test Accuracy: %f\" % accuracy_score(y_pred, y_test_Pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = Embedding()\n",
    "#conv_net = Classification(embedding)\n",
    "\n",
    "classifier = Pretraining(conv_net, 0.0001, 80, batch_size, 100, filepath, restore=False)\n",
    "\n",
    "X_train = np.concatenate((X_train_Pad, X_train_IMS), axis=0)\n",
    "y_train = np.concatenate((y_train_Pad, y_train_IMS), axis=0) \n",
    "\n",
    "classifier.fit(X_train, y_train, save =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.828333\n",
      "Test Accuracy: 0.881744\n"
     ]
    }
   ],
   "source": [
    "# Test model on test set.\n",
    "pred= conv_net(X_test_Pad)\n",
    "y_pred = np.argmax(pred.numpy(), axis=1)\n",
    "print(\"Test Accuracy: %f\" % accuracy_score(y_pred, y_test_Pad))\n",
    "\n",
    "pred= conv_net(X_test_IMS)\n",
    "y_pred = np.argmax(pred.numpy(), axis=1)\n",
    "print(\"Test Accuracy: %f\" % accuracy_score(y_pred, y_test_IMS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Ww38twqTw7B"
   },
   "source": [
    "\n",
    "### Task B (IMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1585909319779,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "9n03hTwvB3gA",
    "outputId": "082bd339-f0ed-4bfe-8a83-ed9b68538da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.901362\n"
     ]
    }
   ],
   "source": [
    "# Test model on test set.\n",
    "pred= conv_net(X_test_IMS)\n",
    "y_pred = np.argmax(pred.numpy(), axis=1)\n",
    "print(\"Test Accuracy: %f\" % accuracy_score(y_pred, y_test_IMS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeetnxbDTzb8"
   },
   "source": [
    "We observe that using the EWC the model has learnt fairly well to perform taks B and it has prevented it from completely forgetting task A. However, since the datasets were pretty different, the accuracy on task A has decreased a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lehAk4BLz0JI"
   },
   "source": [
    "Finally, we visualize the embeddings for both datasets.\n",
    "\n",
    "### Task A (Paderborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sPs6EW62zitw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer embedding_21 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.conv4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embeddings.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).net.embeddings.beta\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.conv4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.fc2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embeddings.gamma\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).net.embeddings.beta\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "emb_Pad = np.nan_to_num(embedding(X_test_Pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3474,
     "status": "ok",
     "timestamp": 1585909331367,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "3T9TKf7lzqMm",
    "outputId": "216b4558-7edd-470e-cc27-96a8d0809c0b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = emb_Pad\n",
    "\n",
    "embed = umap.UMAP(n_neighbors=10,\n",
    "                      min_dist=0.5,\n",
    "                      metric='correlation').fit_transform(features)\n",
    "\n",
    "color = pd.DataFrame(y_test_Pad,columns=['color'])\n",
    "color.replace({0:'red', 1:'blue', 2:'green', 3:'orange'},inplace=True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.scatter(embed[:,0], embed[:,1], \n",
    "            c=color.values.flatten(),\n",
    "            cmap=\"Spectral\", \n",
    "            s=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "plt.title(\"Extracted features UMAP\", fontsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-rIHyAEEz863"
   },
   "source": [
    "### Task B (IMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbvqPphBz_UB"
   },
   "outputs": [],
   "source": [
    "emb_IMS = np.nan_to_num(embedding(X_test_IMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4832,
     "status": "ok",
     "timestamp": 1585909339622,
     "user": {
      "displayName": "Laia Domingo",
      "photoUrl": "",
      "userId": "05713674999451466092"
     },
     "user_tz": -120
    },
    "id": "hhQH7jKFz_Rd",
    "outputId": "7988095a-cfd5-4fd5-f37c-1b9e8aef5150"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = emb_IMS\n",
    "\n",
    "embed = umap.UMAP(n_neighbors=10,\n",
    "                      min_dist=0.5,\n",
    "                      metric='correlation').fit_transform(features)\n",
    "\n",
    "color = pd.DataFrame(y_test_IMS,columns=['color'])\n",
    "color.replace({0:'red', 1:'blue', 2:'green', 3:'orange'},inplace=True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.scatter(embed[:,0], embed[:,1], \n",
    "            c=color.values.flatten(),\n",
    "            cmap=\"Spectral\", \n",
    "            s=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "plt.title(\"Extracted features UMAP\", fontsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "651l7Y5J0P4V"
   },
   "source": [
    "Finally, we show the code to load the weights of the model (for predictions or retrainig)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPwkI2T5oFdHg4JQtLYQtPL",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CÃ²pia de EWC-concept-drift.ipynb",
   "provenance": [
    {
     "file_id": "1GreEeWVtPRGldRR3V1521-LG1XIZfD5T",
     "timestamp": 1584960200837
    },
    {
     "file_id": "1fuu4M1efq7vfdDLG09En1niJUZ59I-oB",
     "timestamp": 1582024464889
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
